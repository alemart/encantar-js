{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"demo/","text":"","title":""},{"location":"demos/","text":"Demo gallery Please read the guidelines before you play with the demos. Feel free to remix them. Basic demos WebAR with A-Frame Create an augmented scene with A-Frame . This is the easiest demo to edit! Launch demo WebAR with three.js Create an augmented scene with three.js . Launch demo WebAR with WebGL only Create an augmented scene without additional libraries. Launch demo Hello, world! A basic template to help you get started. Launch demo Interactive demos Touch interaction Have virtual elements respond to touch input. Launch demo Guidelines For a good experience: Don't move the camera nor the target image too quickly. This produces motion blur. The target image should appear clearly in the video. The physical environment should be properly illuminated. If you're scanning the image on a screen, make sure to adjust the brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable. If you print the image, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. Avoid low-quality cameras. Cameras of common smartphones are okay. Try locally Try the demos on your own machine: Run on a console: git clone git@github.com:alemart/martins-js.git cd martins-js npm start Open https://localhost:8000/demos/ Pick a demo and have fun!","title":"Demo gallery"},{"location":"demos/#demo-gallery","text":"Please read the guidelines before you play with the demos. Feel free to remix them.","title":"Demo gallery"},{"location":"demos/#basic-demos","text":"","title":"Basic demos"},{"location":"demos/#webar-with-a-frame","text":"Create an augmented scene with A-Frame . This is the easiest demo to edit! Launch demo","title":"WebAR with A-Frame"},{"location":"demos/#webar-with-threejs","text":"Create an augmented scene with three.js . Launch demo","title":"WebAR with three.js"},{"location":"demos/#webar-with-webgl-only","text":"Create an augmented scene without additional libraries. Launch demo","title":"WebAR with WebGL only"},{"location":"demos/#hello-world","text":"A basic template to help you get started. Launch demo","title":"Hello, world!"},{"location":"demos/#interactive-demos","text":"","title":"Interactive demos"},{"location":"demos/#touch-interaction","text":"Have virtual elements respond to touch input. Launch demo","title":"Touch interaction"},{"location":"demos/#guidelines","text":"For a good experience: Don't move the camera nor the target image too quickly. This produces motion blur. The target image should appear clearly in the video. The physical environment should be properly illuminated. If you're scanning the image on a screen, make sure to adjust the brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable. If you print the image, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. Avoid low-quality cameras. Cameras of common smartphones are okay.","title":"Guidelines"},{"location":"demos/#try-locally","text":"Try the demos on your own machine: Run on a console: git clone git@github.com:alemart/martins-js.git cd martins-js npm start Open https://localhost:8000/demos/ Pick a demo and have fun!","title":"Try locally"},{"location":"download/","text":"","title":"Download"},{"location":"faq/","text":"Questions & Answers What is MARTINS.js? MARTINS.js is a GPU-accelerated Augmented Reality engine for the web. It's a standalone WebAR technology for creating AR experiences that run in web browsers. Users don't need specialized hardware nor dedicated software - only a modern web browser. MARTINS is a recursive acronym for MARTINS Augmented Reality Technology for Internet Software. It also happens to be my name. See, AR is part of my name. Can you believe it? What is WebAR? Refer to the concepts . Is this WebXR? No, MARTINS.js is not WebXR. The WebXR API allows you to access functionalities of VR and AR-capable devices in web browsers. It relies on other technologies, such as Google's ARCore or Apple's ARKit, to run the show. Those technologies are great, but they are supported on specific devices, which may or may not match your users' devices. On the other hand, MARTINS.js is fully standalone and is built from scratch using standard web technologies such as WebGL2 and WebAssembly, which are widely available. My intention is to give it broad compatibility. Why do my models appear \"laid down\" in AR? MARTINS.js uses a right-handed coordinate system with the Z-axis pointing \"up\". The same convention is used in Blender . When exporting your own models, make sure that the Z-axis points \"up\" and that the ground plane is the XY-plane. If your models appear \"laid down\" in AR, this is probably the issue. Fix with code Fixing the orientation of the model is the preferred solution. However, you can also fix the issue with code: add a node (entity) to the scene graph and make it rotate its children by 90 degrees around the x-axis. How can I contact you? Get in touch! I love WebAR! I know!","title":"Questions & Answers"},{"location":"faq/#questions-answers","text":"","title":"Questions &amp; Answers"},{"location":"faq/#what-is-martinsjs","text":"MARTINS.js is a GPU-accelerated Augmented Reality engine for the web. It's a standalone WebAR technology for creating AR experiences that run in web browsers. Users don't need specialized hardware nor dedicated software - only a modern web browser. MARTINS is a recursive acronym for MARTINS Augmented Reality Technology for Internet Software. It also happens to be my name. See, AR is part of my name. Can you believe it?","title":"What is MARTINS.js?"},{"location":"faq/#what-is-webar","text":"Refer to the concepts .","title":"What is WebAR?"},{"location":"faq/#is-this-webxr","text":"No, MARTINS.js is not WebXR. The WebXR API allows you to access functionalities of VR and AR-capable devices in web browsers. It relies on other technologies, such as Google's ARCore or Apple's ARKit, to run the show. Those technologies are great, but they are supported on specific devices, which may or may not match your users' devices. On the other hand, MARTINS.js is fully standalone and is built from scratch using standard web technologies such as WebGL2 and WebAssembly, which are widely available. My intention is to give it broad compatibility.","title":"Is this WebXR?"},{"location":"faq/#why-do-my-models-appear-laid-down-in-ar","text":"MARTINS.js uses a right-handed coordinate system with the Z-axis pointing \"up\". The same convention is used in Blender . When exporting your own models, make sure that the Z-axis points \"up\" and that the ground plane is the XY-plane. If your models appear \"laid down\" in AR, this is probably the issue. Fix with code Fixing the orientation of the model is the preferred solution. However, you can also fix the issue with code: add a node (entity) to the scene graph and make it rotate its children by 90 degrees around the x-axis.","title":"Why do my models appear \"laid down\" in AR?"},{"location":"faq/#how-can-i-contact-you","text":"Get in touch!","title":"How can I contact you?"},{"location":"faq/#i-love-webar","text":"I know!","title":"I love WebAR!"},{"location":"license/","text":"GNU LESSER GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. This version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below. 0. Additional Definitions. As used herein, \"this License\" refers to version 3 of the GNU Lesser General Public License, and the \"GNU GPL\" refers to version 3 of the GNU General Public License. \"The Library\" refers to a covered work governed by this License, other than an Application or a Combined Work as defined below. An \"Application\" is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library. A \"Combined Work\" is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the \"Linked Version\". The \"Minimal Corresponding Source\" for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version. The \"Corresponding Application Code\" for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work. 1. Exception to Section 3 of the GNU GPL. You may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL. 2. Conveying Modified Versions. If you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version: a) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or b) under the GNU GPL, with none of the additional permissions of this License applicable to that copy. 3. Object Code Incorporating Material from Library Header Files. The object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following: a) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the object code with a copy of the GNU GPL and this license document. 4. Combined Works. You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following: a) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the Combined Work with a copy of the GNU GPL and this license document. c) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document. d) Do one of the following: 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. e) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.) 5. Combined Libraries. You may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License. b) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work. 6. Revised Versions of the GNU Lesser General Public License. The Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation. If the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.","title":"License"},{"location":"license/#gnu-lesser-general-public-license","text":"Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. This version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below.","title":"GNU LESSER GENERAL PUBLIC LICENSE"},{"location":"license/#0-additional-definitions","text":"As used herein, \"this License\" refers to version 3 of the GNU Lesser General Public License, and the \"GNU GPL\" refers to version 3 of the GNU General Public License. \"The Library\" refers to a covered work governed by this License, other than an Application or a Combined Work as defined below. An \"Application\" is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library. A \"Combined Work\" is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the \"Linked Version\". The \"Minimal Corresponding Source\" for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version. The \"Corresponding Application Code\" for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work.","title":"0. Additional Definitions."},{"location":"license/#1-exception-to-section-3-of-the-gnu-gpl","text":"You may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL.","title":"1. Exception to Section 3 of the GNU GPL."},{"location":"license/#2-conveying-modified-versions","text":"If you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version: a) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or b) under the GNU GPL, with none of the additional permissions of this License applicable to that copy.","title":"2. Conveying Modified Versions."},{"location":"license/#3-object-code-incorporating-material-from-library-header-files","text":"The object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following: a) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the object code with a copy of the GNU GPL and this license document.","title":"3. Object Code Incorporating Material from Library Header Files."},{"location":"license/#4-combined-works","text":"You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following: a) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the Combined Work with a copy of the GNU GPL and this license document. c) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document. d) Do one of the following: 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. e) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.)","title":"4. Combined Works."},{"location":"license/#5-combined-libraries","text":"You may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License. b) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.","title":"5. Combined Libraries."},{"location":"license/#6-revised-versions-of-the-gnu-lesser-general-public-license","text":"The Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation. If the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.","title":"6. Revised Versions of the GNU Lesser General Public License."},{"location":"support-my-work/","text":"","title":"Support my work"},{"location":"api/ar-event-listener/","text":"AREventListener A function that is linked to an AREventTarget . It is called as soon as that AREventTarget receives an AREvent of a specific type . The event is passed as an argument.","title":"AREventListener"},{"location":"api/ar-event-listener/#areventlistener","text":"A function that is linked to an AREventTarget . It is called as soon as that AREventTarget receives an AREvent of a specific type . The event is passed as an argument.","title":"AREventListener"},{"location":"api/ar-event-target/","text":"AREventTarget An AREventTarget is an object that is able to receive AREvents . You may add event listeners to it in order to listen to \"relevant changes\" in its state. Methods addEventListener target.addEventListener(type: AREventType, listener: AREventListener): void Add an event listener to target . Arguments type: AREventType . The type of event you intend to listen to. listener: AREventListener . The event listener you intend to add. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); }); removeEventListener target.removeEventListener(type: AREventType, listener: AREventListener): void Remove an event listener from target . Arguments type: AREventType . The type of event you are listening to. listener: AREventListener . The event listener you intend to remove.","title":"AREventTarget"},{"location":"api/ar-event-target/#areventtarget","text":"An AREventTarget is an object that is able to receive AREvents . You may add event listeners to it in order to listen to \"relevant changes\" in its state.","title":"AREventTarget"},{"location":"api/ar-event-target/#methods","text":"","title":"Methods"},{"location":"api/ar-event-target/#addeventlistener","text":"target.addEventListener(type: AREventType, listener: AREventListener): void Add an event listener to target . Arguments type: AREventType . The type of event you intend to listen to. listener: AREventListener . The event listener you intend to add. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); });","title":"addEventListener"},{"location":"api/ar-event-target/#removeeventlistener","text":"target.removeEventListener(type: AREventType, listener: AREventListener): void Remove an event listener from target . Arguments type: AREventType . The type of event you are listening to. listener: AREventListener . The event listener you intend to remove.","title":"removeEventListener"},{"location":"api/ar-event-type/","text":"AREventType An AREventType is a string representing the type of an AREvent . The documentation of the different AREventTargets (e.g., Session ) specify which event types are valid for those targets.","title":"AREventType"},{"location":"api/ar-event-type/#areventtype","text":"An AREventType is a string representing the type of an AREvent . The documentation of the different AREventTargets (e.g., Session ) specify which event types are valid for those targets.","title":"AREventType"},{"location":"api/ar-event/","text":"AREvent An AREvent is an Event sent to an AREventTarget . AREvents are used to notify AREventListeners about \"relevant changes\" in the state of AREventTargets . Properties type event.type: AREventType An AREventType representing the type of the event.","title":"AREvent"},{"location":"api/ar-event/#arevent","text":"An AREvent is an Event sent to an AREventTarget . AREvents are used to notify AREventListeners about \"relevant changes\" in the state of AREventTargets .","title":"AREvent"},{"location":"api/ar-event/#properties","text":"","title":"Properties"},{"location":"api/ar-event/#type","text":"event.type: AREventType An AREventType representing the type of the event.","title":"type"},{"location":"api/camera-source/","text":"CameraSource A source of data linked to a webcam. Instantiation Martins.Source.Camera Martins.Source.Camera(settings: object): CameraSource Create a new webcam-based source of data with the specified settings . Arguments settings: object, optional . An object with the following keys (all are optional): resolution: Resolution . The desired resolution of the video. The higher the resolution, the longer it takes for the video to be uploaded to the GPU, which impacts performance. The lower the resolution, the less accurate the tracking will be. Suggested values: \"md+\" , \"md\" , \"sm+\" . aspectRatio: number . A hint specifying the preferred aspect ratio of the video. constraints: MediaTrackConstraints . Additional video constraints that will be passed to navigator.mediaDevices.getUserMedia() . Returns A new webcam-based source of data. Example const webcam = Martins . Source . Camera ({ resolution : 'md+' , constraints : { facingMode : 'environment' // will prefer the rear camera on mobile devices //facingMode: 'user' // will prefer the front camera on mobile devices } }); Properties resolution source.resolution: Resolution, read-only The resolution of this source of data.","title":"CameraSource"},{"location":"api/camera-source/#camerasource","text":"A source of data linked to a webcam.","title":"CameraSource"},{"location":"api/camera-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/camera-source/#martinssourcecamera","text":"Martins.Source.Camera(settings: object): CameraSource Create a new webcam-based source of data with the specified settings . Arguments settings: object, optional . An object with the following keys (all are optional): resolution: Resolution . The desired resolution of the video. The higher the resolution, the longer it takes for the video to be uploaded to the GPU, which impacts performance. The lower the resolution, the less accurate the tracking will be. Suggested values: \"md+\" , \"md\" , \"sm+\" . aspectRatio: number . A hint specifying the preferred aspect ratio of the video. constraints: MediaTrackConstraints . Additional video constraints that will be passed to navigator.mediaDevices.getUserMedia() . Returns A new webcam-based source of data. Example const webcam = Martins . Source . Camera ({ resolution : 'md+' , constraints : { facingMode : 'environment' // will prefer the rear camera on mobile devices //facingMode: 'user' // will prefer the front camera on mobile devices } });","title":"Martins.Source.Camera"},{"location":"api/camera-source/#properties","text":"","title":"Properties"},{"location":"api/camera-source/#resolution","text":"source.resolution: Resolution, read-only The resolution of this source of data.","title":"resolution"},{"location":"api/canvas-source/","text":"CanvasSource A source of data linked to a <canvas> element. Instantiation Martins.Source.Canvas Martins.Source.Canvas(canvas: HTMLCanvasElement): CanvasSource Create a new source of data linked to the provided canvas . Arguments canvas: HTMLCanvasElement . A <canvas> element. Returns A new source of data.","title":"CanvasSource"},{"location":"api/canvas-source/#canvassource","text":"A source of data linked to a <canvas> element.","title":"CanvasSource"},{"location":"api/canvas-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/canvas-source/#martinssourcecanvas","text":"Martins.Source.Canvas(canvas: HTMLCanvasElement): CanvasSource Create a new source of data linked to the provided canvas . Arguments canvas: HTMLCanvasElement . A <canvas> element. Returns A new source of data.","title":"Martins.Source.Canvas"},{"location":"api/frame/","text":"Frame A Frame holds data for augmenting the physical scene with the virtual scene. Properties session frame.session: Session, read-only A reference to the session . results frame.results: Iterable<TrackerResult>, read-only Use this property to iterate through the results generated by the trackers . Example function animate ( time , frame ) { for ( const result of frame . results ) { // ... } session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate );","title":"Frame"},{"location":"api/frame/#frame","text":"A Frame holds data for augmenting the physical scene with the virtual scene.","title":"Frame"},{"location":"api/frame/#properties","text":"","title":"Properties"},{"location":"api/frame/#session","text":"frame.session: Session, read-only A reference to the session .","title":"session"},{"location":"api/frame/#results","text":"frame.results: Iterable<TrackerResult>, read-only Use this property to iterate through the results generated by the trackers . Example function animate ( time , frame ) { for ( const result of frame . results ) { // ... } session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate );","title":"results"},{"location":"api/gizmos/","text":"Gizmos Gizmos provide visual cues about the state of the trackers . They are particularly useful during development. Properties visible gizmos.visible: boolean Whether or not the gizmos are visible.","title":"Gizmos"},{"location":"api/gizmos/#gizmos","text":"Gizmos provide visual cues about the state of the trackers . They are particularly useful during development.","title":"Gizmos"},{"location":"api/gizmos/#properties","text":"","title":"Properties"},{"location":"api/gizmos/#visible","text":"gizmos.visible: boolean Whether or not the gizmos are visible.","title":"visible"},{"location":"api/hud/","text":"HUD A HUD (Heads Up Display) is an overlay used to display 2D elements that do not correlate with the physical scene. It's part of a viewport and occupies its entire space. It appears in front of the augmented scene. Properties container hud.container: HTMLDivElement, read-only The container of the HUD. visible hud.visible: boolean Whether or not the HUD is visible.","title":"HUD"},{"location":"api/hud/#hud","text":"A HUD (Heads Up Display) is an overlay used to display 2D elements that do not correlate with the physical scene. It's part of a viewport and occupies its entire space. It appears in front of the augmented scene.","title":"HUD"},{"location":"api/hud/#properties","text":"","title":"Properties"},{"location":"api/hud/#container","text":"hud.container: HTMLDivElement, read-only The container of the HUD.","title":"container"},{"location":"api/hud/#visible","text":"hud.visible: boolean Whether or not the HUD is visible.","title":"visible"},{"location":"api/image-tracker-result/","text":"ImageTrackerResult A result generated by an Image Tracker . Properties tracker result.tracker: ImageTracker, read-only A reference to the Image Tracker that generated this result. trackables result.trackables: TrackableImage[], read-only An array of zero or one TrackableImage object(s). viewer result.viewer: Viewer | undefined, read-only A viewer associated with the trackable. If there is no trackable, this property will be undefined .","title":"ImageTrackerResult"},{"location":"api/image-tracker-result/#imagetrackerresult","text":"A result generated by an Image Tracker .","title":"ImageTrackerResult"},{"location":"api/image-tracker-result/#properties","text":"","title":"Properties"},{"location":"api/image-tracker-result/#tracker","text":"result.tracker: ImageTracker, read-only A reference to the Image Tracker that generated this result.","title":"tracker"},{"location":"api/image-tracker-result/#trackables","text":"result.trackables: TrackableImage[], read-only An array of zero or one TrackableImage object(s).","title":"trackables"},{"location":"api/image-tracker-result/#viewer","text":"result.viewer: Viewer | undefined, read-only A viewer associated with the trackable. If there is no trackable, this property will be undefined .","title":"viewer"},{"location":"api/image-tracker/","text":"ImageTracker A tracker that tracks images in a video. Images are tracked using templates known as reference images . Properties state tracker.state: string, read-only The current state of the tracker. database tracker.database: ReferenceImageDatabase, read-only A database of reference images . resolution tracker.resolution: Resolution The resolution adopted by the computer vision algorithms implemented in the tracker. Higher resolutions improve the tracking quality, but are computationally more expensive. Events An ImageTracker is an AREventTarget . You can listen to the following events: targetfound A target has been found. Properties referenceImage: ReferenceImage . The reference image that is linked to the target. Example tracker . addEventListener ( 'targetfound' , event => { console . log ( 'Found target: ' + event . referenceImage . name ); }); targetlost A target has been lost. Properties referenceImage: ReferenceImage . The reference image that is linked to the target.","title":"ImageTracker"},{"location":"api/image-tracker/#imagetracker","text":"A tracker that tracks images in a video. Images are tracked using templates known as reference images .","title":"ImageTracker"},{"location":"api/image-tracker/#properties","text":"","title":"Properties"},{"location":"api/image-tracker/#state","text":"tracker.state: string, read-only The current state of the tracker.","title":"state"},{"location":"api/image-tracker/#database","text":"tracker.database: ReferenceImageDatabase, read-only A database of reference images .","title":"database"},{"location":"api/image-tracker/#resolution","text":"tracker.resolution: Resolution The resolution adopted by the computer vision algorithms implemented in the tracker. Higher resolutions improve the tracking quality, but are computationally more expensive.","title":"resolution"},{"location":"api/image-tracker/#events","text":"An ImageTracker is an AREventTarget . You can listen to the following events:","title":"Events"},{"location":"api/image-tracker/#targetfound","text":"A target has been found. Properties referenceImage: ReferenceImage . The reference image that is linked to the target. Example tracker . addEventListener ( 'targetfound' , event => { console . log ( 'Found target: ' + event . referenceImage . name ); });","title":"targetfound"},{"location":"api/image-tracker/#targetlost","text":"A target has been lost. Properties referenceImage: ReferenceImage . The reference image that is linked to the target.","title":"targetlost"},{"location":"api/martins/","text":"Martins The Martins namespace is the entry point of the features and components of MARTINS.js. I have documented the instantiation of the components of the engine in their respective pages. Properties Settings Martins.Settings: Settings, read-only The settings of the engine. version Martins.version: string, read-only The version of MARTINS.js. Methods isSupported Martins.isSupported(): boolean Checks if the user agent is capable of running the engine. Returns Returns true if the user agent is compatible with the engine, or false otherwise.","title":"Martins"},{"location":"api/martins/#martins","text":"The Martins namespace is the entry point of the features and components of MARTINS.js. I have documented the instantiation of the components of the engine in their respective pages.","title":"Martins"},{"location":"api/martins/#properties","text":"","title":"Properties"},{"location":"api/martins/#settings","text":"Martins.Settings: Settings, read-only The settings of the engine.","title":"Settings"},{"location":"api/martins/#version","text":"Martins.version: string, read-only The version of MARTINS.js.","title":"version"},{"location":"api/martins/#methods","text":"","title":"Methods"},{"location":"api/martins/#issupported","text":"Martins.isSupported(): boolean Checks if the user agent is capable of running the engine. Returns Returns true if the user agent is compatible with the engine, or false otherwise.","title":"isSupported"},{"location":"api/perspective-view/","text":"PerspectiveView A View that models a perspective projection. Properties aspect view.aspect: number, read-only Aspect ratio of the viewing frustum. fovy view.fovy: number, read-only Vertical field-of-view of the viewing frustum, measured in radians. near view.near: number, read-only Distance of the near clipping plane of the viewing frustum to the Z = 0 plane in viewer space. far view.far: number, read-only Distance of the far clipping plane of the viewing frustum to the Z = 0 plane in viewer space.","title":"PerspectiveView"},{"location":"api/perspective-view/#perspectiveview","text":"A View that models a perspective projection.","title":"PerspectiveView"},{"location":"api/perspective-view/#properties","text":"","title":"Properties"},{"location":"api/perspective-view/#aspect","text":"view.aspect: number, read-only Aspect ratio of the viewing frustum.","title":"aspect"},{"location":"api/perspective-view/#fovy","text":"view.fovy: number, read-only Vertical field-of-view of the viewing frustum, measured in radians.","title":"fovy"},{"location":"api/perspective-view/#near","text":"view.near: number, read-only Distance of the near clipping plane of the viewing frustum to the Z = 0 plane in viewer space.","title":"near"},{"location":"api/perspective-view/#far","text":"view.far: number, read-only Distance of the far clipping plane of the viewing frustum to the Z = 0 plane in viewer space.","title":"far"},{"location":"api/pose/","text":"Pose A pose represents a position and an orientation in 3D space. Properties transform pose.transform: RigidTransform, read-only The underlying rigid transform .","title":"Pose"},{"location":"api/pose/#pose","text":"A pose represents a position and an orientation in 3D space.","title":"Pose"},{"location":"api/pose/#properties","text":"","title":"Properties"},{"location":"api/pose/#transform","text":"pose.transform: RigidTransform, read-only The underlying rigid transform .","title":"transform"},{"location":"api/reference-image-database/","text":"ReferenceImageDatabase A database of reference images that belongs to an Image Tracker . Properties count database.count: number, read-only The number of reference images stored in this database. capacity database.capacity: number The maximum number of reference images that can be stored in this database. Note: this property is writable since version 0.2.1 (experimental). Methods add database.add(referenceImages: ReferenceImage[]): SpeedyPromise<void> Add reference image(s) to the database. Arguments referenceImages: ReferenceImage[] . The reference image(s) you want to add. Returns A promise that resolves as soon as the provided reference images are loaded and added to the database. Example const referenceImages = [{ name : 'my-first-image' , image : document . getElementById ( 'my-first-image' ) }, { name : 'my-second-image' , image : document . getElementById ( 'my-second-image' ) }]; tracker . database . add ( referenceImages ). then (() => { console . log ( 'The images have been added to the database' ); }); @@iterator database[Symbol.iterator](): Iterator<ReferenceImage> This is used to iterate over the reference images stored in the database. Returns An iterator. Example for ( const referenceImage of tracker . database ) { console . log ( referenceImage . name ); }","title":"ReferenceImageDatabase"},{"location":"api/reference-image-database/#referenceimagedatabase","text":"A database of reference images that belongs to an Image Tracker .","title":"ReferenceImageDatabase"},{"location":"api/reference-image-database/#properties","text":"","title":"Properties"},{"location":"api/reference-image-database/#count","text":"database.count: number, read-only The number of reference images stored in this database.","title":"count"},{"location":"api/reference-image-database/#capacity","text":"database.capacity: number The maximum number of reference images that can be stored in this database. Note: this property is writable since version 0.2.1 (experimental).","title":"capacity"},{"location":"api/reference-image-database/#methods","text":"","title":"Methods"},{"location":"api/reference-image-database/#add","text":"database.add(referenceImages: ReferenceImage[]): SpeedyPromise<void> Add reference image(s) to the database. Arguments referenceImages: ReferenceImage[] . The reference image(s) you want to add. Returns A promise that resolves as soon as the provided reference images are loaded and added to the database. Example const referenceImages = [{ name : 'my-first-image' , image : document . getElementById ( 'my-first-image' ) }, { name : 'my-second-image' , image : document . getElementById ( 'my-second-image' ) }]; tracker . database . add ( referenceImages ). then (() => { console . log ( 'The images have been added to the database' ); });","title":"add"},{"location":"api/reference-image-database/#iterator","text":"database[Symbol.iterator](): Iterator<ReferenceImage> This is used to iterate over the reference images stored in the database. Returns An iterator. Example for ( const referenceImage of tracker . database ) { console . log ( referenceImage . name ); }","title":"@@iterator"},{"location":"api/reference-image/","text":"ReferenceImage An interface specifying an image template that is fed to an Image Tracker . Properties name referenceImage.name: string, read-only A name used to identify this reference image in a database . image referenceImage.image: HTMLImageElement | HTMLCanvasElement | ImageBitmap, read-only Image template with pixel data.","title":"ReferenceImage"},{"location":"api/reference-image/#referenceimage","text":"An interface specifying an image template that is fed to an Image Tracker .","title":"ReferenceImage"},{"location":"api/reference-image/#properties","text":"","title":"Properties"},{"location":"api/reference-image/#name","text":"referenceImage.name: string, read-only A name used to identify this reference image in a database .","title":"name"},{"location":"api/reference-image/#image","text":"referenceImage.image: HTMLImageElement | HTMLCanvasElement | ImageBitmap, read-only Image template with pixel data.","title":"image"},{"location":"api/resolution/","text":"Resolution A Resolution is a setting defined by a string. It is mapped to a size measured in pixels according to special rules. You may use it to change the resolution in pixels of a video captured by a webcam, or to adjust the resolution in pixels of the videos that are processed by a tracker for example. The table below shows examples of how resolution strings are converted to pixels: Resolution string 16:9 landscape 16:10 landscape 4:3 landscape \"xs\" 212x120 192x120 160x120 \"xs+\" 284x160 256x160 212x160 \"sm\" 356x200 320x200 266x200 \"sm+\" 426x240 384x240 320x240 \"md\" 568x320 512x320 426x320 \"md+\" 640x360 576x360 480x360 \"lg\" 852x480 768x480 640x480 \"lg+\" 1066x600 960x600 800x600","title":"Resolution"},{"location":"api/resolution/#resolution","text":"A Resolution is a setting defined by a string. It is mapped to a size measured in pixels according to special rules. You may use it to change the resolution in pixels of a video captured by a webcam, or to adjust the resolution in pixels of the videos that are processed by a tracker for example. The table below shows examples of how resolution strings are converted to pixels: Resolution string 16:9 landscape 16:10 landscape 4:3 landscape \"xs\" 212x120 192x120 160x120 \"xs+\" 284x160 256x160 212x160 \"sm\" 356x200 320x200 266x200 \"sm+\" 426x240 384x240 320x240 \"md\" 568x320 512x320 426x320 \"md+\" 640x360 576x360 480x360 \"lg\" 852x480 768x480 640x480 \"lg+\" 1066x600 960x600 800x600","title":"Resolution"},{"location":"api/rigid-transform/","text":"RigidTransform A RigidTransform represents a rotation and a translation in 3D space. Properties matrix transform.matrix: SpeedyMatrix, read-only A 4x4 matrix encoding the transform. inverse transform.inverse: RigidTransform, read-only The inverse transform.","title":"RigidTransform"},{"location":"api/rigid-transform/#rigidtransform","text":"A RigidTransform represents a rotation and a translation in 3D space.","title":"RigidTransform"},{"location":"api/rigid-transform/#properties","text":"","title":"Properties"},{"location":"api/rigid-transform/#matrix","text":"transform.matrix: SpeedyMatrix, read-only A 4x4 matrix encoding the transform.","title":"matrix"},{"location":"api/rigid-transform/#inverse","text":"transform.inverse: RigidTransform, read-only The inverse transform.","title":"inverse"},{"location":"api/session/","text":"Session A central component of a WebAR experience. Read the concepts for more information. Instantiation Martins.startSession Martins.startSession(options: object): SpeedyPromise<Session> Start a new session. Arguments options: object . Options object with the following keys: trackers: Tracker[] . The trackers to be attached to the session. sources: Source[] . The sources of data to be linked to the session. viewport: Viewport . The viewport to be linked to the session. mode: string, optional . Either \"immersive\" or \"inline\" . Defaults to \"immersive\" . gizmos: boolean, optional . Whether or not to display the gizmos . Defaults to false . stats: boolean, optional . Whether or not to display the stats panel. It's useful during development. Defaults to false . Returns A promise that resolves to a new Session object. Properties mode session.mode: string, read-only Session mode: either \"immersive\" or \"inline\" . time session.time: Time, read-only A reference to the Time utilities of this session. viewport session.viewport: Viewport, read-only A reference to the Viewport linked to this session. gizmos session.gizmos: Gizmos, read-only A reference to the Gizmos object. Methods requestAnimationFrame session.requestAnimationFrame(callback: function): SessionRequestAnimationFrameHandle Schedules a call to the callback function, which is intended to update and render the virtual scene. Your callback function must itself call session.requestAnimationFrame() again in order to continue to update and render the virtual scene. Note session.requestAnimationFrame() is analogous to window.requestAnimationFrame() , but they are not the same! The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser. Arguments callback: function . A function that receives two parameters: time: DOMHighResTimeStamp . Elapsed time, in milliseconds, since an arbitrary reference. This parameter is kept to mimic web standards, but its usage is discouraged. Prefer using frame.session.time.elapsed and frame.session.time.delta instead. These are especially useful for creating animations. See also: Time . frame: Frame . A Frame holding the data you need to create the augmented scene. Returns A handle. Example function animate ( time , frame ) { // update and render the virtual scene // ... // repeat the call session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate ); cancelAnimationFrame session.cancelAnimationFrame(handle: SessionRequestAnimationFrameHandle): void Cancels an animation frame request. Arguments handle: SessionRequestAnimationFrameHandle . A handle returned by session.requestAnimationFrame() . end session.end(): SpeedyPromise<void> Ends the session. Returns A promise that resolves as soon as the session is terminated. Events A session is an AREventTarget . You can listen to the following events: end The session has ended. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); });","title":"Session"},{"location":"api/session/#session","text":"A central component of a WebAR experience. Read the concepts for more information.","title":"Session"},{"location":"api/session/#instantiation","text":"","title":"Instantiation"},{"location":"api/session/#martinsstartsession","text":"Martins.startSession(options: object): SpeedyPromise<Session> Start a new session. Arguments options: object . Options object with the following keys: trackers: Tracker[] . The trackers to be attached to the session. sources: Source[] . The sources of data to be linked to the session. viewport: Viewport . The viewport to be linked to the session. mode: string, optional . Either \"immersive\" or \"inline\" . Defaults to \"immersive\" . gizmos: boolean, optional . Whether or not to display the gizmos . Defaults to false . stats: boolean, optional . Whether or not to display the stats panel. It's useful during development. Defaults to false . Returns A promise that resolves to a new Session object.","title":"Martins.startSession"},{"location":"api/session/#properties","text":"","title":"Properties"},{"location":"api/session/#mode","text":"session.mode: string, read-only Session mode: either \"immersive\" or \"inline\" .","title":"mode"},{"location":"api/session/#time","text":"session.time: Time, read-only A reference to the Time utilities of this session.","title":"time"},{"location":"api/session/#viewport","text":"session.viewport: Viewport, read-only A reference to the Viewport linked to this session.","title":"viewport"},{"location":"api/session/#gizmos","text":"session.gizmos: Gizmos, read-only A reference to the Gizmos object.","title":"gizmos"},{"location":"api/session/#methods","text":"","title":"Methods"},{"location":"api/session/#requestanimationframe","text":"session.requestAnimationFrame(callback: function): SessionRequestAnimationFrameHandle Schedules a call to the callback function, which is intended to update and render the virtual scene. Your callback function must itself call session.requestAnimationFrame() again in order to continue to update and render the virtual scene. Note session.requestAnimationFrame() is analogous to window.requestAnimationFrame() , but they are not the same! The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser. Arguments callback: function . A function that receives two parameters: time: DOMHighResTimeStamp . Elapsed time, in milliseconds, since an arbitrary reference. This parameter is kept to mimic web standards, but its usage is discouraged. Prefer using frame.session.time.elapsed and frame.session.time.delta instead. These are especially useful for creating animations. See also: Time . frame: Frame . A Frame holding the data you need to create the augmented scene. Returns A handle. Example function animate ( time , frame ) { // update and render the virtual scene // ... // repeat the call session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate );","title":"requestAnimationFrame"},{"location":"api/session/#cancelanimationframe","text":"session.cancelAnimationFrame(handle: SessionRequestAnimationFrameHandle): void Cancels an animation frame request. Arguments handle: SessionRequestAnimationFrameHandle . A handle returned by session.requestAnimationFrame() .","title":"cancelAnimationFrame"},{"location":"api/session/#end","text":"session.end(): SpeedyPromise<void> Ends the session. Returns A promise that resolves as soon as the session is terminated.","title":"end"},{"location":"api/session/#events","text":"A session is an AREventTarget . You can listen to the following events:","title":"Events"},{"location":"api/session/#end_1","text":"The session has ended. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); });","title":"end"},{"location":"api/settings/","text":"Settings Engine settings. Properties powerPreference Martins.Settings.powerPreference: string Power profile. One of the following: \"default\" , \"low-power\" , \"high-performance\" . Profile Description \"default\" Default settings. \"low-power\" Reduce performance in order to reduce power consumption. \"high-performance\" High performance mode.","title":"Settings"},{"location":"api/settings/#settings","text":"Engine settings.","title":"Settings"},{"location":"api/settings/#properties","text":"","title":"Properties"},{"location":"api/settings/#powerpreference","text":"Martins.Settings.powerPreference: string Power profile. One of the following: \"default\" , \"low-power\" , \"high-performance\" . Profile Description \"default\" Default settings. \"low-power\" Reduce performance in order to reduce power consumption. \"high-performance\" High performance mode.","title":"powerPreference"},{"location":"api/source/","text":"Source An abstraction representing a source of data that is meant to be linked to a session . A video is a typical source of data. Sources of data feed the trackers . Refer to the concepts for more information.","title":"Source"},{"location":"api/source/#source","text":"An abstraction representing a source of data that is meant to be linked to a session . A video is a typical source of data. Sources of data feed the trackers . Refer to the concepts for more information.","title":"Source"},{"location":"api/speedy-matrix/","text":"SpeedyMatrix Speedy includes its own fast implementation of matrices. They are used extensively in MARTINS.js. Properties rows matrix.rows: number, read-only Number of rows of the matrix. columns matrix.columns: number, read-only Number of columns of the matrix. Methods read matrix.read(): number[] Read the entries of the matrix in column-major format . Returns The entries of the matrix in column-major format. Example /* Suppose that you are given this matrix: [ 1 4 7 ] matrix = [ 2 5 8 ] [ 3 6 9 ] Its entries in column-major format are: [1,2,3, 4,5,6, 7,8,9] */ const entries = matrix . read (); toString matrix.toString(): string Convert to string. Returns A human-readable representation of the matrix. Example console . log ( matrix . toString ());","title":"SpeedyMatrix"},{"location":"api/speedy-matrix/#speedymatrix","text":"Speedy includes its own fast implementation of matrices. They are used extensively in MARTINS.js.","title":"SpeedyMatrix"},{"location":"api/speedy-matrix/#properties","text":"","title":"Properties"},{"location":"api/speedy-matrix/#rows","text":"matrix.rows: number, read-only Number of rows of the matrix.","title":"rows"},{"location":"api/speedy-matrix/#columns","text":"matrix.columns: number, read-only Number of columns of the matrix.","title":"columns"},{"location":"api/speedy-matrix/#methods","text":"","title":"Methods"},{"location":"api/speedy-matrix/#read","text":"matrix.read(): number[] Read the entries of the matrix in column-major format . Returns The entries of the matrix in column-major format. Example /* Suppose that you are given this matrix: [ 1 4 7 ] matrix = [ 2 5 8 ] [ 3 6 9 ] Its entries in column-major format are: [1,2,3, 4,5,6, 7,8,9] */ const entries = matrix . read ();","title":"read"},{"location":"api/speedy-matrix/#tostring","text":"matrix.toString(): string Convert to string. Returns A human-readable representation of the matrix. Example console . log ( matrix . toString ());","title":"toString"},{"location":"api/speedy-promise/","text":"SpeedyPromise Speedy includes its own implementation of promises. A SpeedyPromise works just like a standard promise. SpeedyPromises are designed for real-time applications. There are some subtle differences behind the scenes, but you need not be concerned with those. Instantiation Speedy.Promise new Speedy.Promise(executor: function): SpeedyPromise Creates a new SpeedyPromise . This works just like the constructor of a standard promise. Arguments executor: function . A function that takes two functions as arguments: resolve: function . To be called when the promise is resolved. reject: function . To be called when the promise is rejected. Returns A new SpeedyPromise . Example function sleep ( ms ) { return new Speedy . Promise (( resolve , reject ) => { if ( ms >= 0 ) setTimeout ( resolve , ms ); else reject ( new Error ( 'Invalid time' )); }); } sleep ( 2000 ). then (() => { console . log ( '2 seconds have passed' ); }). catch ( error => { console . error ( error . message ); }). finally (() => { console . log ( 'Done!' ); });","title":"SpeedyPromise"},{"location":"api/speedy-promise/#speedypromise","text":"Speedy includes its own implementation of promises. A SpeedyPromise works just like a standard promise. SpeedyPromises are designed for real-time applications. There are some subtle differences behind the scenes, but you need not be concerned with those.","title":"SpeedyPromise"},{"location":"api/speedy-promise/#instantiation","text":"","title":"Instantiation"},{"location":"api/speedy-promise/#speedypromise_1","text":"new Speedy.Promise(executor: function): SpeedyPromise Creates a new SpeedyPromise . This works just like the constructor of a standard promise. Arguments executor: function . A function that takes two functions as arguments: resolve: function . To be called when the promise is resolved. reject: function . To be called when the promise is rejected. Returns A new SpeedyPromise . Example function sleep ( ms ) { return new Speedy . Promise (( resolve , reject ) => { if ( ms >= 0 ) setTimeout ( resolve , ms ); else reject ( new Error ( 'Invalid time' )); }); } sleep ( 2000 ). then (() => { console . log ( '2 seconds have passed' ); }). catch ( error => { console . error ( error . message ); }). finally (() => { console . log ( 'Done!' ); });","title":"Speedy.Promise"},{"location":"api/speedy-size/","text":"SpeedySize Represents the size of a rectangle. Properties width size.width: number, read-only Width of the rectangle. height size.height: number, read-only Height of the rectangle.","title":"SpeedySize"},{"location":"api/speedy-size/#speedysize","text":"Represents the size of a rectangle.","title":"SpeedySize"},{"location":"api/speedy-size/#properties","text":"","title":"Properties"},{"location":"api/speedy-size/#width","text":"size.width: number, read-only Width of the rectangle.","title":"width"},{"location":"api/speedy-size/#height","text":"size.height: number, read-only Height of the rectangle.","title":"height"},{"location":"api/speedy/","text":"Speedy MARTINS.js is built using Speedy Vision , a GPU-accelerated Computer Vision library which is another project of mine. Many features provided by Speedy Vision are very useful (e.g., matrices). I have decided to include some of them in parts of the MARTINS.js API for convenience. In this documentation, I provide a quick reference of some of the classes used in Speedy Vision. This reference is minimal. For a more complete reference, please visit the website of the project . Properties Speedy Speedy: Speedy, read-only The Speedy namespace is provided in global scope. It's also available as Martins.Speedy .","title":"Speedy"},{"location":"api/speedy/#speedy","text":"MARTINS.js is built using Speedy Vision , a GPU-accelerated Computer Vision library which is another project of mine. Many features provided by Speedy Vision are very useful (e.g., matrices). I have decided to include some of them in parts of the MARTINS.js API for convenience. In this documentation, I provide a quick reference of some of the classes used in Speedy Vision. This reference is minimal. For a more complete reference, please visit the website of the project .","title":"Speedy"},{"location":"api/speedy/#properties","text":"","title":"Properties"},{"location":"api/speedy/#speedy_1","text":"Speedy: Speedy, read-only The Speedy namespace is provided in global scope. It's also available as Martins.Speedy .","title":"Speedy"},{"location":"api/time/","text":"Time Time-related utilities. They are useful for animating virtual scenes. Properties elapsed time.elapsed: number, read-only Elapsed time since the start of the session, measured at the beginning of the current animation frame and given in seconds. delta time.delta: number, read-only Elapsed time between the current and the previous animation frame, given in seconds. Use this value to produce animations that are independent of the framerate. scale time.scale: number Time scale. Use it to accelerate, slowdown or pause the passage of time. Defaults to 1. unscaled time.unscaled: number, read-only Time scale independent seconds since the start of the session, measured at the beginning of the current animation frame.","title":"Time"},{"location":"api/time/#time","text":"Time-related utilities. They are useful for animating virtual scenes.","title":"Time"},{"location":"api/time/#properties","text":"","title":"Properties"},{"location":"api/time/#elapsed","text":"time.elapsed: number, read-only Elapsed time since the start of the session, measured at the beginning of the current animation frame and given in seconds.","title":"elapsed"},{"location":"api/time/#delta","text":"time.delta: number, read-only Elapsed time between the current and the previous animation frame, given in seconds. Use this value to produce animations that are independent of the framerate.","title":"delta"},{"location":"api/time/#scale","text":"time.scale: number Time scale. Use it to accelerate, slowdown or pause the passage of time. Defaults to 1.","title":"scale"},{"location":"api/time/#unscaled","text":"time.unscaled: number, read-only Time scale independent seconds since the start of the session, measured at the beginning of the current animation frame.","title":"unscaled"},{"location":"api/trackable-image/","text":"TrackableImage A trackable that represents an image target tracked by an Image Tracker . Properties pose trackable.pose: Pose, read-only The pose of the trackable. referenceImage trackable.referenceImage: ReferenceImage, read-only The reference image associated with the trackable.","title":"TrackableImage"},{"location":"api/trackable-image/#trackableimage","text":"A trackable that represents an image target tracked by an Image Tracker .","title":"TrackableImage"},{"location":"api/trackable-image/#properties","text":"","title":"Properties"},{"location":"api/trackable-image/#pose","text":"trackable.pose: Pose, read-only The pose of the trackable.","title":"pose"},{"location":"api/trackable-image/#referenceimage","text":"trackable.referenceImage: ReferenceImage, read-only The reference image associated with the trackable.","title":"referenceImage"},{"location":"api/trackable/","text":"Trackable A Trackable is an interface that represents something that is tracked by a Tracker .","title":"Trackable"},{"location":"api/trackable/#trackable","text":"A Trackable is an interface that represents something that is tracked by a Tracker .","title":"Trackable"},{"location":"api/tracker-result/","text":"TrackerResult An interface that represents the result generated by a tracker at a specific time. It is part of a frame . Properties tracker result.tracker: Tracker, read-only A reference to the tracker that generated this result. trackables result.trackables: Trackable[], read-only An array of zero or more trackables .","title":"TrackerResult"},{"location":"api/tracker-result/#trackerresult","text":"An interface that represents the result generated by a tracker at a specific time. It is part of a frame .","title":"TrackerResult"},{"location":"api/tracker-result/#properties","text":"","title":"Properties"},{"location":"api/tracker-result/#tracker","text":"result.tracker: Tracker, read-only A reference to the tracker that generated this result.","title":"tracker"},{"location":"api/tracker-result/#trackables","text":"result.trackables: Trackable[], read-only An array of zero or more trackables .","title":"trackables"},{"location":"api/tracker/","text":"Tracker An interface that represents a generic tracker. Trackers analyze input data in some way and are meant to be attached to a session . Refer to the concepts for more information. An Image Tracker is an implementation of a tracker. Properties type tracker.type: string, read-only A string representing the type of the tracker.","title":"Tracker"},{"location":"api/tracker/#tracker","text":"An interface that represents a generic tracker. Trackers analyze input data in some way and are meant to be attached to a session . Refer to the concepts for more information. An Image Tracker is an implementation of a tracker.","title":"Tracker"},{"location":"api/tracker/#properties","text":"","title":"Properties"},{"location":"api/tracker/#type","text":"tracker.type: string, read-only A string representing the type of the tracker.","title":"type"},{"location":"api/video-source/","text":"VideoSource A source of data linked to a <video> element. Instantiation Martins.Source.Video Martins.Source.Video(video: HTMLVideoElement): VideoSource Create a new source of data linked to the provided video . Arguments video: HTMLVideoElement . A <video> element. Returns A new source of data.","title":"VideoSource"},{"location":"api/video-source/#videosource","text":"A source of data linked to a <video> element.","title":"VideoSource"},{"location":"api/video-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/video-source/#martinssourcevideo","text":"Martins.Source.Video(video: HTMLVideoElement): VideoSource Create a new source of data linked to the provided video . Arguments video: HTMLVideoElement . A <video> element. Returns A new source of data.","title":"Martins.Source.Video"},{"location":"api/view/","text":"View An interface that represents a view of the 3D world at a moment in time. A PerspectiveView is an implementation of a View. Properties projectionMatrix view.projectionMatrix: SpeedyMatrix, read-only A 4x4 matrix that projects viewer space onto clip space.","title":"View"},{"location":"api/view/#view","text":"An interface that represents a view of the 3D world at a moment in time. A PerspectiveView is an implementation of a View.","title":"View"},{"location":"api/view/#properties","text":"","title":"Properties"},{"location":"api/view/#projectionmatrix","text":"view.projectionMatrix: SpeedyMatrix, read-only A 4x4 matrix that projects viewer space onto clip space.","title":"projectionMatrix"},{"location":"api/viewer-pose/","text":"ViewerPose The pose of a Viewer . Properties viewMatrix pose.viewMatrix: SpeedyMatrix, read-only A 4x4 matrix that converts points from world space to viewer space.","title":"ViewerPose"},{"location":"api/viewer-pose/#viewerpose","text":"The pose of a Viewer .","title":"ViewerPose"},{"location":"api/viewer-pose/#properties","text":"","title":"Properties"},{"location":"api/viewer-pose/#viewmatrix","text":"pose.viewMatrix: SpeedyMatrix, read-only A 4x4 matrix that converts points from world space to viewer space.","title":"viewMatrix"},{"location":"api/viewer/","text":"Viewer A virtual camera in 3D world space. Properties pose viewer.pose: ViewerPose, read-only The pose of the viewer. view viewer.view: View, read-only A view of the viewer (monoscopic rendering). Methods convertToViewerSpace viewer.convertToViewerSpace(pose: Pose): Pose Convert a pose from world space to viewer space. Arguments pose: Pose . A pose in world space. Returns The input pose converted to viewer space. Example const modelViewMatrix = viewer . convertToViewerSpace ( pose ). transform . matrix ;","title":"Viewer"},{"location":"api/viewer/#viewer","text":"A virtual camera in 3D world space.","title":"Viewer"},{"location":"api/viewer/#properties","text":"","title":"Properties"},{"location":"api/viewer/#pose","text":"viewer.pose: ViewerPose, read-only The pose of the viewer.","title":"pose"},{"location":"api/viewer/#view","text":"viewer.view: View, read-only A view of the viewer (monoscopic rendering).","title":"view"},{"location":"api/viewer/#methods","text":"","title":"Methods"},{"location":"api/viewer/#converttoviewerspace","text":"viewer.convertToViewerSpace(pose: Pose): Pose Convert a pose from world space to viewer space. Arguments pose: Pose . A pose in world space. Returns The input pose converted to viewer space. Example const modelViewMatrix = viewer . convertToViewerSpace ( pose ). transform . matrix ;","title":"convertToViewerSpace"},{"location":"api/viewport/","text":"Viewport The viewport is the area of the web page in which the augmented scene is displayed. Instantiation Martins.Viewport Martins.Viewport(settings: object): Viewport Create a new viewport with the specified settings . Arguments settings: object . An object with the following keys: container: HTMLDivElement . A <div> that will contain the augmented scene. hudContainer: HTMLDivElement, optional . An overlay that will be displayed in front of the augmented scene. It must be a direct child of container in the DOM tree. resolution: Resolution, optional . The resolution of the virtual scene. canvas: HTMLCanvasElement, optional . An existing canvas on which the virtual scene will be drawn. The engine automatically creates a canvas. You should only specify an existing canvas if you must. Experimental. style: string, optional. The viewport style . Since: 0.2.1 Returns A new viewport. Example const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), resolution : 'lg' }); Properties container viewport.container: HTMLDivElement, read-only The container of the viewport. hud viewport.hud: HUD, read-only The HUD . resolution viewport.resolution: Resolution, read-only The resolution of the virtual scene. virtualSize viewport.virtualSize: SpeedySize, read-only The size in pixels that matches the resolution of the virtual scene. canvas viewport.canvas: HTMLCanvasElement, read-only A <canvas> on which the virtual scene is drawn. style viewport.style: string The style determines the way the viewport appears on the screen. Different styles are applicable to different session modes . The following are valid styles: \"best-fit\" : an immersive viewport that is scaled in a way that covers the page while preserving the aspect ratio of the augmented scene. \"stretch\" : an immersive viewport that is scaled in a way that covers the entire page, stretching the augmented scene if necessary. \"inline\" : an inline viewport that follows the typical flow of a web page. The default style is \"best-fit\" in immersive mode, or \"inline\" in inline mode. Since: 0.2.1 fullscreen viewport.fullscreen: boolean, read-only Whether or not the viewport container is being displayed in fullscreen mode. Since: 0.2.1 Methods requestFullscreen viewport.requestFullscreen(): SpeedyPromise<void> Make a request to the user agent so that the viewport container is displayed in fullscreen mode. The user must interact with the page (e.g., tap on a button) in order to comply with browser policies , otherwise the request will not succeed. iPhone support At the time of this writing, the fullscreen mode is not supported on iPhone . An alternative way to create a fullscreen experience is to set the viewport style to \"stretch\" in a web app . Since: 0.2.1 Returns A promise that is resolved when the fullscreen mode is activated, or rejected on error. Example function toggleFullscreen () { if ( ! viewport . fullscreen ) { viewport . requestFullscreen (). catch ( err => { alert ( `Can't enable fullscreen mode. ` + err . toString ()); }); } else { viewport . exitFullscreen (); } } // require user interaction button . addEventListener ( 'click' , toggleFullscreen ); exitFullscreen viewport.exitFullscreen(): SpeedyPromise<void> Exit fullscreen mode. Since: 0.2.1 Returns A promise that is resolved once the fullscreen mode is no longer active, or rejected on error. The promise will be rejected if the method is called when not in fullscreen mode. isFullscreenAvailable viewport.isFullscreenAvailable(): boolean Checks the availability of the fullscreen mode on the current platform and page. Since: 0.2.1 Returns Returns true if the fullscreen mode can be activated. Events A viewport is an AREventTarget . You can listen to the following events: resize The viewport has been resized. This will happen when the user resizes the window of the web browser or when the mobile device is flipped. Example viewport . addEventListener ( 'resize' , event => { console . log ( 'The viewport has been resized.' ); });","title":"Viewport"},{"location":"api/viewport/#viewport","text":"The viewport is the area of the web page in which the augmented scene is displayed.","title":"Viewport"},{"location":"api/viewport/#instantiation","text":"","title":"Instantiation"},{"location":"api/viewport/#martinsviewport","text":"Martins.Viewport(settings: object): Viewport Create a new viewport with the specified settings . Arguments settings: object . An object with the following keys: container: HTMLDivElement . A <div> that will contain the augmented scene. hudContainer: HTMLDivElement, optional . An overlay that will be displayed in front of the augmented scene. It must be a direct child of container in the DOM tree. resolution: Resolution, optional . The resolution of the virtual scene. canvas: HTMLCanvasElement, optional . An existing canvas on which the virtual scene will be drawn. The engine automatically creates a canvas. You should only specify an existing canvas if you must. Experimental. style: string, optional. The viewport style . Since: 0.2.1 Returns A new viewport. Example const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), resolution : 'lg' });","title":"Martins.Viewport"},{"location":"api/viewport/#properties","text":"","title":"Properties"},{"location":"api/viewport/#container","text":"viewport.container: HTMLDivElement, read-only The container of the viewport.","title":"container"},{"location":"api/viewport/#hud","text":"viewport.hud: HUD, read-only The HUD .","title":"hud"},{"location":"api/viewport/#resolution","text":"viewport.resolution: Resolution, read-only The resolution of the virtual scene.","title":"resolution"},{"location":"api/viewport/#virtualsize","text":"viewport.virtualSize: SpeedySize, read-only The size in pixels that matches the resolution of the virtual scene.","title":"virtualSize"},{"location":"api/viewport/#canvas","text":"viewport.canvas: HTMLCanvasElement, read-only A <canvas> on which the virtual scene is drawn.","title":"canvas"},{"location":"api/viewport/#style","text":"viewport.style: string The style determines the way the viewport appears on the screen. Different styles are applicable to different session modes . The following are valid styles: \"best-fit\" : an immersive viewport that is scaled in a way that covers the page while preserving the aspect ratio of the augmented scene. \"stretch\" : an immersive viewport that is scaled in a way that covers the entire page, stretching the augmented scene if necessary. \"inline\" : an inline viewport that follows the typical flow of a web page. The default style is \"best-fit\" in immersive mode, or \"inline\" in inline mode. Since: 0.2.1","title":"style"},{"location":"api/viewport/#fullscreen","text":"viewport.fullscreen: boolean, read-only Whether or not the viewport container is being displayed in fullscreen mode. Since: 0.2.1","title":"fullscreen"},{"location":"api/viewport/#methods","text":"","title":"Methods"},{"location":"api/viewport/#requestfullscreen","text":"viewport.requestFullscreen(): SpeedyPromise<void> Make a request to the user agent so that the viewport container is displayed in fullscreen mode. The user must interact with the page (e.g., tap on a button) in order to comply with browser policies , otherwise the request will not succeed. iPhone support At the time of this writing, the fullscreen mode is not supported on iPhone . An alternative way to create a fullscreen experience is to set the viewport style to \"stretch\" in a web app . Since: 0.2.1 Returns A promise that is resolved when the fullscreen mode is activated, or rejected on error. Example function toggleFullscreen () { if ( ! viewport . fullscreen ) { viewport . requestFullscreen (). catch ( err => { alert ( `Can't enable fullscreen mode. ` + err . toString ()); }); } else { viewport . exitFullscreen (); } } // require user interaction button . addEventListener ( 'click' , toggleFullscreen );","title":"requestFullscreen"},{"location":"api/viewport/#exitfullscreen","text":"viewport.exitFullscreen(): SpeedyPromise<void> Exit fullscreen mode. Since: 0.2.1 Returns A promise that is resolved once the fullscreen mode is no longer active, or rejected on error. The promise will be rejected if the method is called when not in fullscreen mode.","title":"exitFullscreen"},{"location":"api/viewport/#isfullscreenavailable","text":"viewport.isFullscreenAvailable(): boolean Checks the availability of the fullscreen mode on the current platform and page. Since: 0.2.1 Returns Returns true if the fullscreen mode can be activated.","title":"isFullscreenAvailable"},{"location":"api/viewport/#events","text":"A viewport is an AREventTarget . You can listen to the following events:","title":"Events"},{"location":"api/viewport/#resize","text":"The viewport has been resized. This will happen when the user resizes the window of the web browser or when the mobile device is flipped. Example viewport . addEventListener ( 'resize' , event => { console . log ( 'The viewport has been resized.' ); });","title":"resize"},{"location":"getting-started/","text":"Welcome to MARTINS.js! Create amazing Augmented Reality experiences with MARTINS.js , a GPU-accelerated Augmented Reality engine for the web. Get started Try a demo Support my work Features Runs everywhere : on Android, on iOS, and even on Desktop computers. No need of AR-capable devices. Image tracking . Use it to track detailed images such as: book covers, cartoons and photos. Why use MARTINS.js? No need to download apps! MARTINS.js is a WebAR engine: it runs in web browsers. Users can access the AR experiences immediately. Easy to get started! MARTINS.js can be used with a <script> tag in your page. A static HTML page is enough to get started. Fast and powerful! MARTINS.js is GPU-accelerated. It uses WebGL2 and WebAssembly for turbocharged performance. Fully standalone! MARTINS.js is built from scratch using standard web technologies. No need for additional hardware or software! Free and open-source! Get started right away! Browser compatibility MARTINS.js is compatible with all major web browsers: Chrome Edge Firefox Opera Safari* * use Safari 15.2 or later. MARTINS.js requires WebGL2 and WebAssembly. About MARTINS.js is developed by Alexandre Martins and released under the LGPL . It is based on Speedy Vision .","title":"Welcome"},{"location":"getting-started/#welcome-to-martinsjs","text":"Create amazing Augmented Reality experiences with MARTINS.js , a GPU-accelerated Augmented Reality engine for the web. Get started Try a demo Support my work","title":"Welcome to MARTINS.js!"},{"location":"getting-started/#features","text":"Runs everywhere : on Android, on iOS, and even on Desktop computers. No need of AR-capable devices. Image tracking . Use it to track detailed images such as: book covers, cartoons and photos.","title":"Features"},{"location":"getting-started/#why-use-martinsjs","text":"No need to download apps! MARTINS.js is a WebAR engine: it runs in web browsers. Users can access the AR experiences immediately. Easy to get started! MARTINS.js can be used with a <script> tag in your page. A static HTML page is enough to get started. Fast and powerful! MARTINS.js is GPU-accelerated. It uses WebGL2 and WebAssembly for turbocharged performance. Fully standalone! MARTINS.js is built from scratch using standard web technologies. No need for additional hardware or software! Free and open-source! Get started right away!","title":"Why use MARTINS.js?"},{"location":"getting-started/#browser-compatibility","text":"MARTINS.js is compatible with all major web browsers: Chrome Edge Firefox Opera Safari* * use Safari 15.2 or later. MARTINS.js requires WebGL2 and WebAssembly.","title":"Browser compatibility"},{"location":"getting-started/#about","text":"MARTINS.js is developed by Alexandre Martins and released under the LGPL . It is based on Speedy Vision .","title":"About"},{"location":"getting-started/activate-your-webcam/","text":"Activate your webcam In this section we're going to learn how to use your webcam to capture the video. We're also going to polish our work and make it presentable to users. Change the source of data Instead of using a video file, we're going to use your webcam. We simply need to change the source of data and instruct MARTINS.js to use your webcam. We'll do it with 1 new line of code! ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); //const video = document.getElementById('my-video'); // comment this line //const source = Martins.Source.Video(video); // comment this line const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Let's also comment (or remove) the <video> tag from the HTML file - we no longer need it: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > Open http://localhost:8000 and... ta-da! The web browser will ask for your permission to access the camera. Have fun. Before using a webcam Pay attention to the following: Low-quality cameras should be avoided. A camera of a typical smartphone is probably good enough. Don't move the camera / the target image too quickly, as quick movements produce motion blur. Ensure good lighting conditions (see below). Check your physical scene Good lighting conditions are important for a good user experience. Even though the MARTINS.js can handle various lighting conditions, you should get your physical scene appropriately illuminated. When developing your own WebAR experiences, ask yourself: Will my users experience AR indoors? If so, make sure that the room is sufficiently illuminated. Will my users experience AR outdoors? In this case, make sure that users interact with your AR experience during the day, or have that interaction happen in a place with sufficient artificial lighting. When printing your reference images, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. If you're using a screen to display the reference image, make sure to adjust the brightness. Too much brightness causes overexposure and loss of detail, leading to tracking difficulties. Not enough brightness is also undesirable, because it makes the reference image look too dark in the video. Screen reflections are also undesirable. Use HTTPS When distributing your WebAR experiences over the internet, make sure to use HTTPS. Web browsers will only allow access to the webcam in secure contexts. Here is the reference image in case you need it again: Reference Image Create a scan gimmick Let's polish our work. When the tracker is scanning the physical scene, we'll display a visual cue suggesting the user to frame the target image. I'll call that a scan gimmick. Save the image below as scan.png : Scan gimmick In order to display that scan gimmick, we need to create a HUD ( Heads-Up Display ). A HUD is an overlay used to display 2D content in front of the augmented scene. It's part of the viewport. Modify index.html and ar-demo.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > <!-- <script> tags of the rendering engine of your choice --> < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; } # scan { width : 100 % ; height : 100 % ; object-fit : contain ; opacity : 0.75 ; } </ style > </ head > < body > < div id = \"ar-viewport\" > < div id = \"ar-hud\" hidden > < img id = \"scan\" src = \"scan.png\" > </ div > </ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Open http://localhost:8000 . Now you can see the scan gimmick being displayed... all the time?! Configure the scan gimmick The scan gimmick should only be displayed when the tracker is scanning the physical scene. We should hide it as soon as a target image is recognized. If the tracking is lost, then we need to display it again because we're back in scanning mode. A simple way to know whether or not we're tracking a target image is to use events. We're going to add two event listeners to our tracker. If a targetfound event happens, we hide the scan gimmick. If a targetlost event happens, we show the scan gimmick again. ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; }); return session ; } Hide the gizmos Let's polish our work even more by hiding the gizmos. You may just set gizmos to false in Martins.startSession() and there will be no more gizmos. Do the same to hide the stats panel. Let me show you a different approach. Instead of getting rid of the gizmos completely, we're going to hide them partially. They will be displayed when the tracker is scanning the physical scene, but not when the physical scene is being augmented. That's easy to do with the event listeners we have just set up: ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; session . gizmos . visible = false ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; session . gizmos . visible = true ; }); return session ; } Open http://localhost:8000 again. Enjoy your WebAR experience!","title":"Activate your webcam"},{"location":"getting-started/activate-your-webcam/#activate-your-webcam","text":"In this section we're going to learn how to use your webcam to capture the video. We're also going to polish our work and make it presentable to users.","title":"Activate your webcam"},{"location":"getting-started/activate-your-webcam/#change-the-source-of-data","text":"Instead of using a video file, we're going to use your webcam. We simply need to change the source of data and instruct MARTINS.js to use your webcam. We'll do it with 1 new line of code! ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); //const video = document.getElementById('my-video'); // comment this line //const source = Martins.Source.Video(video); // comment this line const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Let's also comment (or remove) the <video> tag from the HTML file - we no longer need it: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > Open http://localhost:8000 and... ta-da! The web browser will ask for your permission to access the camera. Have fun. Before using a webcam Pay attention to the following: Low-quality cameras should be avoided. A camera of a typical smartphone is probably good enough. Don't move the camera / the target image too quickly, as quick movements produce motion blur. Ensure good lighting conditions (see below). Check your physical scene Good lighting conditions are important for a good user experience. Even though the MARTINS.js can handle various lighting conditions, you should get your physical scene appropriately illuminated. When developing your own WebAR experiences, ask yourself: Will my users experience AR indoors? If so, make sure that the room is sufficiently illuminated. Will my users experience AR outdoors? In this case, make sure that users interact with your AR experience during the day, or have that interaction happen in a place with sufficient artificial lighting. When printing your reference images, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. If you're using a screen to display the reference image, make sure to adjust the brightness. Too much brightness causes overexposure and loss of detail, leading to tracking difficulties. Not enough brightness is also undesirable, because it makes the reference image look too dark in the video. Screen reflections are also undesirable. Use HTTPS When distributing your WebAR experiences over the internet, make sure to use HTTPS. Web browsers will only allow access to the webcam in secure contexts. Here is the reference image in case you need it again: Reference Image","title":"Change the source of data"},{"location":"getting-started/activate-your-webcam/#create-a-scan-gimmick","text":"Let's polish our work. When the tracker is scanning the physical scene, we'll display a visual cue suggesting the user to frame the target image. I'll call that a scan gimmick. Save the image below as scan.png : Scan gimmick In order to display that scan gimmick, we need to create a HUD ( Heads-Up Display ). A HUD is an overlay used to display 2D content in front of the augmented scene. It's part of the viewport. Modify index.html and ar-demo.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > <!-- <script> tags of the rendering engine of your choice --> < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; } # scan { width : 100 % ; height : 100 % ; object-fit : contain ; opacity : 0.75 ; } </ style > </ head > < body > < div id = \"ar-viewport\" > < div id = \"ar-hud\" hidden > < img id = \"scan\" src = \"scan.png\" > </ div > </ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Open http://localhost:8000 . Now you can see the scan gimmick being displayed... all the time?!","title":"Create a scan gimmick"},{"location":"getting-started/activate-your-webcam/#configure-the-scan-gimmick","text":"The scan gimmick should only be displayed when the tracker is scanning the physical scene. We should hide it as soon as a target image is recognized. If the tracking is lost, then we need to display it again because we're back in scanning mode. A simple way to know whether or not we're tracking a target image is to use events. We're going to add two event listeners to our tracker. If a targetfound event happens, we hide the scan gimmick. If a targetlost event happens, we show the scan gimmick again. ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; }); return session ; }","title":"Configure the scan gimmick"},{"location":"getting-started/activate-your-webcam/#hide-the-gizmos","text":"Let's polish our work even more by hiding the gizmos. You may just set gizmos to false in Martins.startSession() and there will be no more gizmos. Do the same to hide the stats panel. Let me show you a different approach. Instead of getting rid of the gizmos completely, we're going to hide them partially. They will be displayed when the tracker is scanning the physical scene, but not when the physical scene is being augmented. That's easy to do with the event listeners we have just set up: ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; session . gizmos . visible = false ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; session . gizmos . visible = true ; }); return session ; } Open http://localhost:8000 again. Enjoy your WebAR experience!","title":"Hide the gizmos"},{"location":"getting-started/concepts/","text":"Concepts Before diving into AR with you, I need to introduce a few concepts. Please take the time to read them all. Feel free to come back to this page at any time. Fundamental concepts Let me clarify what I mean by terms such as Augmented Reality and WebAR: Augmented Reality is the augmentation of physical reality with virtual elements. We typically augment physical reality with imagery generated by computer graphics. In this context, the word augment means: to blend the physical and the virtual imagery in a visually correlated manner. 1.1. AR is an abbreviation of Augmented Reality. An Augmented Reality experience is a computer program designed to let users directly experience Augmented Reality 1 . Augmented Reality experiences come in different shapes. Some are designed for smartphones and tablets, others for special headsets, and so on. WebAR is a set of technologies used to create Augmented Reality experiences that run in web browsers. WebAR makes it easy for users to experience AR, because they can have immediate access to the AR experiences. All they have to do is open a web page. They are not tied to specific platforms and they also don't need to download apps. A WebAR experience is an Augmented Reality experience developed using WebAR technology. MARTINS.js is a WebAR technology. I also call it a WebAR engine. Lots of computations have to be performed behind the scenes in order to make an Augmented Reality experience possible. MARTINS.js uses the GPU 2 to accelerate many of those computations. In fact, the GPU and the CPU 3 are used together. This approach improves the performance of the WebAR experience and ultimately leads to a better user experience. Now that those terms are clarified, I say this: you can use MARTINS.js to create amazing WebAR experiences! Practical concepts Let me explain some concepts that you'll see over and over again when developing WebAR experiences with MARTINS.js: The experience of Augmented Reality is created by augmenting the physical scene with the virtual scene. 1.1. The physical scene is a scene of the physical world. 1.2. The virtual scene is a scene generated by computer graphics. 1.3. The augmented scene is the physical scene augmented with the virtual scene. A session is a central component of a WebAR experience. It handles the main loop . The main loop performs two central tasks: it analyzes the input data and then passes the result of that analysis to the user callback. 2.1. The user callback is a function that updates and renders the virtual scene. 2.2. The main loop is repeated until the session ends. The session ends when the user closes the web page, or by deliberate command. A session has one or more sources of data linked to it. A typical source of data is a video stream. Such a stream usually comes from a webcam or from a video file. 3.1. A source of data produces input data . A tracker is a subsystem of the WebAR engine that analyzes input data in some way. Trackers are meant to be attached to a session. Example: an image tracker is a type of tracker. If we attach an image tracker to a session, then we will be able to track images in that session. The user callback receives a frame . A frame is an object that holds data for rendering the virtual scene in a way that is consistent with the input data at a particular moment in time. Simply put, frames help us augment the physical scene with the virtual scene. 5.1. The data held by a frame is computed by the trackers that are attached to the session. A session is linked to a viewport . The viewport is the area in which we'll display the augmented scene. It's represented by a container defined by a suitable HTML element, typically a <div> . A session has a mode . The mode can be either immersive or inline. 7.1. In immersive mode, the augmented scene is displayed in such a way that it occupies, in a best-fit manner, the entire area of the screen in which the web page is shown. Think of it as a kind of fullscreen. The immersive mode is what is typically wanted. 7.2. In inline mode, the augmented scene is displayed in a way that is consistent with the typical flow of a web page. We can display the augmented scene in a web page such as this one - in the middle of text, links and other elements. Did you read it all? Cool, so let's create our first WebAR experience! It gets to be fun, I promise! This definition of AR experience is convenient in different ways. For example, it makes the term \"WebAR experience\", which is arguably better than \"WebAR app\" (no apps are downloaded), well-defined. I make a distinction between AR experience and experience of AR. An experience of AR is an event in consciousness in which AR is experienced. I sometimes use the latter definition. \u21a9 Graphics Processing Unit \u21a9 Central Processing Unit \u21a9","title":"Concepts"},{"location":"getting-started/concepts/#concepts","text":"Before diving into AR with you, I need to introduce a few concepts. Please take the time to read them all. Feel free to come back to this page at any time.","title":"Concepts"},{"location":"getting-started/concepts/#fundamental-concepts","text":"Let me clarify what I mean by terms such as Augmented Reality and WebAR: Augmented Reality is the augmentation of physical reality with virtual elements. We typically augment physical reality with imagery generated by computer graphics. In this context, the word augment means: to blend the physical and the virtual imagery in a visually correlated manner. 1.1. AR is an abbreviation of Augmented Reality. An Augmented Reality experience is a computer program designed to let users directly experience Augmented Reality 1 . Augmented Reality experiences come in different shapes. Some are designed for smartphones and tablets, others for special headsets, and so on. WebAR is a set of technologies used to create Augmented Reality experiences that run in web browsers. WebAR makes it easy for users to experience AR, because they can have immediate access to the AR experiences. All they have to do is open a web page. They are not tied to specific platforms and they also don't need to download apps. A WebAR experience is an Augmented Reality experience developed using WebAR technology. MARTINS.js is a WebAR technology. I also call it a WebAR engine. Lots of computations have to be performed behind the scenes in order to make an Augmented Reality experience possible. MARTINS.js uses the GPU 2 to accelerate many of those computations. In fact, the GPU and the CPU 3 are used together. This approach improves the performance of the WebAR experience and ultimately leads to a better user experience. Now that those terms are clarified, I say this: you can use MARTINS.js to create amazing WebAR experiences!","title":"Fundamental concepts"},{"location":"getting-started/concepts/#practical-concepts","text":"Let me explain some concepts that you'll see over and over again when developing WebAR experiences with MARTINS.js: The experience of Augmented Reality is created by augmenting the physical scene with the virtual scene. 1.1. The physical scene is a scene of the physical world. 1.2. The virtual scene is a scene generated by computer graphics. 1.3. The augmented scene is the physical scene augmented with the virtual scene. A session is a central component of a WebAR experience. It handles the main loop . The main loop performs two central tasks: it analyzes the input data and then passes the result of that analysis to the user callback. 2.1. The user callback is a function that updates and renders the virtual scene. 2.2. The main loop is repeated until the session ends. The session ends when the user closes the web page, or by deliberate command. A session has one or more sources of data linked to it. A typical source of data is a video stream. Such a stream usually comes from a webcam or from a video file. 3.1. A source of data produces input data . A tracker is a subsystem of the WebAR engine that analyzes input data in some way. Trackers are meant to be attached to a session. Example: an image tracker is a type of tracker. If we attach an image tracker to a session, then we will be able to track images in that session. The user callback receives a frame . A frame is an object that holds data for rendering the virtual scene in a way that is consistent with the input data at a particular moment in time. Simply put, frames help us augment the physical scene with the virtual scene. 5.1. The data held by a frame is computed by the trackers that are attached to the session. A session is linked to a viewport . The viewport is the area in which we'll display the augmented scene. It's represented by a container defined by a suitable HTML element, typically a <div> . A session has a mode . The mode can be either immersive or inline. 7.1. In immersive mode, the augmented scene is displayed in such a way that it occupies, in a best-fit manner, the entire area of the screen in which the web page is shown. Think of it as a kind of fullscreen. The immersive mode is what is typically wanted. 7.2. In inline mode, the augmented scene is displayed in a way that is consistent with the typical flow of a web page. We can display the augmented scene in a web page such as this one - in the middle of text, links and other elements. Did you read it all? Cool, so let's create our first WebAR experience! It gets to be fun, I promise! This definition of AR experience is convenient in different ways. For example, it makes the term \"WebAR experience\", which is arguably better than \"WebAR app\" (no apps are downloaded), well-defined. I make a distinction between AR experience and experience of AR. An experience of AR is an event in consciousness in which AR is experienced. I sometimes use the latter definition. \u21a9 Graphics Processing Unit \u21a9 Central Processing Unit \u21a9","title":"Practical concepts"},{"location":"getting-started/create-the-augmented-scene/","text":"Create the augmented scene Now that the image is being tracked, the next step is to render a virtual scene on top of it. You need a 3D rendering technology to do that. Pick a 3D rendering technology MARTINS.js is not a 3D rendering technology. It is an Augmented Reality technology that provides the data you need in order to augment your physical scenes. There are free and open-source 3D rendering technologies for the web that you can find online and use with MARTINS.js. Popular solutions include: A-Frame, Babylon.js and Three.js. You can also use other solutions. MARTINS.js lets you pick any 3D rendering technology. Once you pick a 3D rendering technology, you need to integrate it with MARTINS.js. There is a code that is responsible for that integration. I call it a glue code . Among other things, a glue code transports the tracking results from MARTINS.js to the 3D rendering technology of your choice - it really is a \"glue\" connecting them. Write the glue code Writing a glue code is a task of moderate complexity. It requires dealing with matrices, with performance issues, and with some idiosyncrasies of the 3D rendering technologies in order to make sure it all works as intended. It is advisable to have specialized knowledge of computer graphics programming in order to write a glue code that works correctly. I provide easy-to-use glue codes that work with different 3D rendering technologies in my demos, so that you don't need to deal with the complexity. Those glue codes are JavaScript (.js) files. You just need to add a glue code to your web page (e.g., via a <script> tag) and then the integration will be done for you. It's really that simple! Find the glue codes in my demos Create the virtual scene Once you plug in the glue code, you'll be using the 3D rendering technology of your choice to create the virtual scene. The physical scene will be automatically augmented with the virtual scene, thus creating the augmented scene. An augmented scene with a 3D model from Kenney Let me tell you a bit more about the 3D rendering technologies I just mentioned. A-Frame A-Frame is an open-source framework used to build virtual reality (VR) experiences for the web. When you combine it with MARTINS.js, you become able to use it to create AR experiences too - without the need of special hardware or software. A-Frame is built on top of Three.js and extends it in powerful ways. It introduces a HTML-based declarative approach for scene graphs , empowering them with the Entity-Component-System , a software pattern commonly used in game development. Sounds complicated? It is not! A-Frame is easy for beginners and pleasing for experts. A simple scene is declared like this: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > <!-- include A-Frame --> < script src = \"aframe-vX.Y.Z.min.js\" ></ script > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > <!-- this is my glue code for A-Frame --> < script src = \"aframe-with-martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > <!-- This is a scene --> < a-scene ar-scene > < a-camera ar-camera ></ a-camera > <!-- Whatever you add to <ar-root> will appear in AR --> < ar-root > < a-entity gltf-model = \"#my-3d-model\" ></ a-entity > </ ar-root > <!-- Declare external media files here --> < a-assets > < a-asset-item id = \"my-3d-model\" src = \"my-3d-model.glb\" ></ a-asset-item > </ a-assets > </ a-scene > </ body > </ html > <ar-root> is not part of A-Frame, but it becomes available as soon as you plug in my glue code. A-Frame lets you create animated scenes with special effects simply by declaring things, like in the above example. In many cases, writing new JavaScript code is not needed. A-Frame also includes a visual inspector that makes things really easy for non-coders. Babylon.js Babylon.js is a powerful open-source game and 3D rendering engine for the web. It includes pretty much all features you commonly find in 3D rendering engines (scene graphs, lights, materials, meshes, etc.), plus systems that are specific to game engines (animation engine, audio engine, collision system, physics system, support for sprites, etc.), plus all kinds of sophisticated features for various applications. Babylon.js has an amazing documentation with plenty of learning resources. Even though it can be used by beginners, it's recommended to have working JavaScript experience before creating projects with it. Three.js Three.js is a popular open-source JavaScript library used to render 3D graphics in web browsers. It supports many features, including: scene graphs, cameras, animations, lights, materials, loading of 3D models, mathematical utilities, special effects, and more. It has an active and vibrant community. Many community-made extensions are available. Three.js often uses WebGL to draw 3D graphics. WebGL is a low-level rasterization engine that draws points, lines and triangles. It's seldom used directly by the developers of applications. Using Three.js requires more JavaScript experience than using A-Frame in most cases, but it's also a great choice if you're comfortable with coding. Compared to A-Frame, Three.js offers you additional freedom on how you can organize your code, because it's a library, not a framework.","title":"Create the augmented scene"},{"location":"getting-started/create-the-augmented-scene/#create-the-augmented-scene","text":"Now that the image is being tracked, the next step is to render a virtual scene on top of it. You need a 3D rendering technology to do that.","title":"Create the augmented scene"},{"location":"getting-started/create-the-augmented-scene/#pick-a-3d-rendering-technology","text":"MARTINS.js is not a 3D rendering technology. It is an Augmented Reality technology that provides the data you need in order to augment your physical scenes. There are free and open-source 3D rendering technologies for the web that you can find online and use with MARTINS.js. Popular solutions include: A-Frame, Babylon.js and Three.js. You can also use other solutions. MARTINS.js lets you pick any 3D rendering technology. Once you pick a 3D rendering technology, you need to integrate it with MARTINS.js. There is a code that is responsible for that integration. I call it a glue code . Among other things, a glue code transports the tracking results from MARTINS.js to the 3D rendering technology of your choice - it really is a \"glue\" connecting them.","title":"Pick a 3D rendering technology"},{"location":"getting-started/create-the-augmented-scene/#write-the-glue-code","text":"Writing a glue code is a task of moderate complexity. It requires dealing with matrices, with performance issues, and with some idiosyncrasies of the 3D rendering technologies in order to make sure it all works as intended. It is advisable to have specialized knowledge of computer graphics programming in order to write a glue code that works correctly. I provide easy-to-use glue codes that work with different 3D rendering technologies in my demos, so that you don't need to deal with the complexity. Those glue codes are JavaScript (.js) files. You just need to add a glue code to your web page (e.g., via a <script> tag) and then the integration will be done for you. It's really that simple! Find the glue codes in my demos","title":"Write the glue code"},{"location":"getting-started/create-the-augmented-scene/#create-the-virtual-scene","text":"Once you plug in the glue code, you'll be using the 3D rendering technology of your choice to create the virtual scene. The physical scene will be automatically augmented with the virtual scene, thus creating the augmented scene. An augmented scene with a 3D model from Kenney Let me tell you a bit more about the 3D rendering technologies I just mentioned.","title":"Create the virtual scene"},{"location":"getting-started/create-the-augmented-scene/#a-frame","text":"A-Frame is an open-source framework used to build virtual reality (VR) experiences for the web. When you combine it with MARTINS.js, you become able to use it to create AR experiences too - without the need of special hardware or software. A-Frame is built on top of Three.js and extends it in powerful ways. It introduces a HTML-based declarative approach for scene graphs , empowering them with the Entity-Component-System , a software pattern commonly used in game development. Sounds complicated? It is not! A-Frame is easy for beginners and pleasing for experts. A simple scene is declared like this: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > <!-- include A-Frame --> < script src = \"aframe-vX.Y.Z.min.js\" ></ script > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > <!-- this is my glue code for A-Frame --> < script src = \"aframe-with-martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > <!-- This is a scene --> < a-scene ar-scene > < a-camera ar-camera ></ a-camera > <!-- Whatever you add to <ar-root> will appear in AR --> < ar-root > < a-entity gltf-model = \"#my-3d-model\" ></ a-entity > </ ar-root > <!-- Declare external media files here --> < a-assets > < a-asset-item id = \"my-3d-model\" src = \"my-3d-model.glb\" ></ a-asset-item > </ a-assets > </ a-scene > </ body > </ html > <ar-root> is not part of A-Frame, but it becomes available as soon as you plug in my glue code. A-Frame lets you create animated scenes with special effects simply by declaring things, like in the above example. In many cases, writing new JavaScript code is not needed. A-Frame also includes a visual inspector that makes things really easy for non-coders.","title":"A-Frame"},{"location":"getting-started/create-the-augmented-scene/#babylonjs","text":"Babylon.js is a powerful open-source game and 3D rendering engine for the web. It includes pretty much all features you commonly find in 3D rendering engines (scene graphs, lights, materials, meshes, etc.), plus systems that are specific to game engines (animation engine, audio engine, collision system, physics system, support for sprites, etc.), plus all kinds of sophisticated features for various applications. Babylon.js has an amazing documentation with plenty of learning resources. Even though it can be used by beginners, it's recommended to have working JavaScript experience before creating projects with it.","title":"Babylon.js"},{"location":"getting-started/create-the-augmented-scene/#threejs","text":"Three.js is a popular open-source JavaScript library used to render 3D graphics in web browsers. It supports many features, including: scene graphs, cameras, animations, lights, materials, loading of 3D models, mathematical utilities, special effects, and more. It has an active and vibrant community. Many community-made extensions are available. Three.js often uses WebGL to draw 3D graphics. WebGL is a low-level rasterization engine that draws points, lines and triangles. It's seldom used directly by the developers of applications. Using Three.js requires more JavaScript experience than using A-Frame in most cases, but it's also a great choice if you're comfortable with coding. Compared to A-Frame, Three.js offers you additional freedom on how you can organize your code, because it's a library, not a framework.","title":"Three.js"},{"location":"getting-started/guidelines-for-images/","text":"Guidelines for Reference Images Some images are more suitable for tracking than others. For best results, pick images that are distinct, asymmetric and detailed. Let me show you some examples. Distinct A distinct image has distinguishable areas - quite unlike a repetitive pattern! Distinct Not distinct Asymmetric Asymmetric images help the engine determine their orientation. When evaluating symmetry, you must not take colors into account. Asymmetric Symmetric Detailed A detailed image has lots of details with sufficient contrast. There's not much blank space! Detailed Not detailed Other considerations Aspect ratio Prefer images whose aspect ratio (the ratio width \u00f7 height ) is somewhere between the aspect ratio of the target device (16:9 is a common aspect ratio) and 1:1 (a square). It's okay to use landscape or portrait mode - the engine will make the necessary adjustments. Resolution Using a Ultra HD image is of no benefit, because the engine will downscale it. A tiny image isn't desirable either, because some details may be lost and the engine will likely have to upscale it. Use an image that has its details preserved. It's even better if that image can be loaded quickly! Physical materials When printing your images, keep the following in mind: Prefer non-reflective materials. Avoid shiny materials such as glossy paper. Reflections may generate artifacts in the video and interfere with the tracking. Materials should be rigid. Don't use something that can be distorted too easily. Use quality materials. Brightness on screens If you're using a screen to display your images, make sure to adjust its brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable. Test it! In addition to the guidelines presented above, you should always experiment with your images and make sure it all works as intended. Keep in mind that proper lighting of the physical environment is also very important!","title":"Guidelines for Images"},{"location":"getting-started/guidelines-for-images/#guidelines-for-reference-images","text":"Some images are more suitable for tracking than others. For best results, pick images that are distinct, asymmetric and detailed. Let me show you some examples.","title":"Guidelines for Reference Images"},{"location":"getting-started/guidelines-for-images/#distinct","text":"A distinct image has distinguishable areas - quite unlike a repetitive pattern! Distinct Not distinct","title":"Distinct"},{"location":"getting-started/guidelines-for-images/#asymmetric","text":"Asymmetric images help the engine determine their orientation. When evaluating symmetry, you must not take colors into account. Asymmetric Symmetric","title":"Asymmetric"},{"location":"getting-started/guidelines-for-images/#detailed","text":"A detailed image has lots of details with sufficient contrast. There's not much blank space! Detailed Not detailed","title":"Detailed"},{"location":"getting-started/guidelines-for-images/#other-considerations","text":"","title":"Other considerations"},{"location":"getting-started/guidelines-for-images/#aspect-ratio","text":"Prefer images whose aspect ratio (the ratio width \u00f7 height ) is somewhere between the aspect ratio of the target device (16:9 is a common aspect ratio) and 1:1 (a square). It's okay to use landscape or portrait mode - the engine will make the necessary adjustments.","title":"Aspect ratio"},{"location":"getting-started/guidelines-for-images/#resolution","text":"Using a Ultra HD image is of no benefit, because the engine will downscale it. A tiny image isn't desirable either, because some details may be lost and the engine will likely have to upscale it. Use an image that has its details preserved. It's even better if that image can be loaded quickly!","title":"Resolution"},{"location":"getting-started/guidelines-for-images/#physical-materials","text":"When printing your images, keep the following in mind: Prefer non-reflective materials. Avoid shiny materials such as glossy paper. Reflections may generate artifacts in the video and interfere with the tracking. Materials should be rigid. Don't use something that can be distorted too easily. Use quality materials.","title":"Physical materials"},{"location":"getting-started/guidelines-for-images/#brightness-on-screens","text":"If you're using a screen to display your images, make sure to adjust its brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable.","title":"Brightness on screens"},{"location":"getting-started/guidelines-for-images/#test-it","text":"In addition to the guidelines presented above, you should always experiment with your images and make sure it all works as intended. Keep in mind that proper lighting of the physical environment is also very important!","title":"Test it!"},{"location":"getting-started/introduction/","text":"Introduction Augmented Reality (AR) has applications in many fields, including: games, marketing, commerce, education, arts, tourism, sports, healthcare, and so on. AR brings many exciting possibilities for creative projects - and you too can get into it! Traditionally, users were required to download (sometimes large) apps to experience AR. That was an obstacle for adoption. What if we dropped the need for apps and just required a web browser instead? Users already have web browsers! That's where WebAR comes in. Image Tracking What to expect from this guide We will create together an Augmented Reality experience that runs in web browsers. You will learn how to track an image in real-time. This is a common use case of Augmented Reality technology. No matter if you are a beginner, an expert, or somewhere in-between, set yourself at ease: this step-by-step guide can be followed by you.","title":"Introduction"},{"location":"getting-started/introduction/#introduction","text":"Augmented Reality (AR) has applications in many fields, including: games, marketing, commerce, education, arts, tourism, sports, healthcare, and so on. AR brings many exciting possibilities for creative projects - and you too can get into it! Traditionally, users were required to download (sometimes large) apps to experience AR. That was an obstacle for adoption. What if we dropped the need for apps and just required a web browser instead? Users already have web browsers! That's where WebAR comes in. Image Tracking","title":"Introduction"},{"location":"getting-started/introduction/#what-to-expect-from-this-guide","text":"We will create together an Augmented Reality experience that runs in web browsers. You will learn how to track an image in real-time. This is a common use case of Augmented Reality technology. No matter if you are a beginner, an expert, or somewhere in-between, set yourself at ease: this step-by-step guide can be followed by you.","title":"What to expect from this guide"},{"location":"getting-started/next-steps/","text":"Next steps Congratulations! You have created your first WebAR experience with MARTINS.js! Let me tell you some of the steps you can take from now on. Change the power preference Image tracking is no trivial task: lots of computations are being performed behind the scenes. The WebAR engine prioritizes processing performance over power consumption by default. You may reduce power consumption by reducing processing performance. This is simple to do: just set Martins.Settings.powerPreference to \"low-power\" . ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } Martins . Settings . powerPreference = 'low-power' ; // OPTIONAL const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; session . gizmos . visible = false ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; session . gizmos . visible = true ; }); return session ; } When you enable low-power mode, the WebAR engine will target a framerate of 30. In many cases, this is still acceptable for a good user experience. I suggest you test both ways! I emphasize that you are not required to enable low-power mode. Enable it if power consumption is an issue for you. If it isn't, you may also experiment with the \"high-performance\" mode. When should I use low-power mode? If you're targeting mobile devices, test your WebAR experiences with low-power mode. If you decide that the lower framerate is still acceptable, keep the low-power mode in order to save battery life. Add multiple virtual scenes You can add multiple reference images to the reference image database. Each of those images can correspond to a different virtual scene. The virtual scene that shows up depends on the target image that is being tracked. Explore the API to see how you can have multiple virtual scenes in a single web page. Don't go overboard with this, though: the web page should load fast. Too much content may impact loading times. Keep your media files small and load your models asynchronously if possible. Publish your WebAR experiences So far we've just created a static HTML page. The next step is to make your page available on the web. Your pages must be served over HTTPS - that's important for webcam access! Tip: use a QR code If you intend to print your reference images, consider adding a QR code nearby. The QR code should point to your web page. Users can then just scan your QR code to open your WebAR experience. Easy! Use the minified code When deploying your WebAR experiences, make sure to include the minified martins.min.js file instead of the regular martins.js . The latter is suitable for development. The former, for production. Support my work If you came this far in the guide, WebAR probably excites you. It is definitely something you want. I know, it is awesome! The possibilities are endless. Even better than getting your creative juices boiling with enthusiasm is the feeling of joy I have for sharing this work with you. I develop MARTINS.js independently. Creating this WebAR engine required a lot of time, effort, skill and specialized knowledge. Please support my work today, so that I can make it even more awesome! Support my work","title":"Next steps"},{"location":"getting-started/next-steps/#next-steps","text":"Congratulations! You have created your first WebAR experience with MARTINS.js! Let me tell you some of the steps you can take from now on.","title":"Next steps"},{"location":"getting-started/next-steps/#change-the-power-preference","text":"Image tracking is no trivial task: lots of computations are being performed behind the scenes. The WebAR engine prioritizes processing performance over power consumption by default. You may reduce power consumption by reducing processing performance. This is simple to do: just set Martins.Settings.powerPreference to \"low-power\" . ar-demo.js async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } Martins . Settings . powerPreference = 'low-power' ; // OPTIONAL const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = Martins.Source.Video(video); const source = Martins . Source . Camera (); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; session . gizmos . visible = false ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; session . gizmos . visible = true ; }); return session ; } When you enable low-power mode, the WebAR engine will target a framerate of 30. In many cases, this is still acceptable for a good user experience. I suggest you test both ways! I emphasize that you are not required to enable low-power mode. Enable it if power consumption is an issue for you. If it isn't, you may also experiment with the \"high-performance\" mode. When should I use low-power mode? If you're targeting mobile devices, test your WebAR experiences with low-power mode. If you decide that the lower framerate is still acceptable, keep the low-power mode in order to save battery life.","title":"Change the power preference"},{"location":"getting-started/next-steps/#add-multiple-virtual-scenes","text":"You can add multiple reference images to the reference image database. Each of those images can correspond to a different virtual scene. The virtual scene that shows up depends on the target image that is being tracked. Explore the API to see how you can have multiple virtual scenes in a single web page. Don't go overboard with this, though: the web page should load fast. Too much content may impact loading times. Keep your media files small and load your models asynchronously if possible.","title":"Add multiple virtual scenes"},{"location":"getting-started/next-steps/#publish-your-webar-experiences","text":"So far we've just created a static HTML page. The next step is to make your page available on the web. Your pages must be served over HTTPS - that's important for webcam access! Tip: use a QR code If you intend to print your reference images, consider adding a QR code nearby. The QR code should point to your web page. Users can then just scan your QR code to open your WebAR experience. Easy! Use the minified code When deploying your WebAR experiences, make sure to include the minified martins.min.js file instead of the regular martins.js . The latter is suitable for development. The former, for production.","title":"Publish your WebAR experiences"},{"location":"getting-started/next-steps/#support-my-work","text":"If you came this far in the guide, WebAR probably excites you. It is definitely something you want. I know, it is awesome! The possibilities are endless. Even better than getting your creative juices boiling with enthusiasm is the feeling of joy I have for sharing this work with you. I develop MARTINS.js independently. Creating this WebAR engine required a lot of time, effort, skill and specialized knowledge. Please support my work today, so that I can make it even more awesome! Support my work","title":"Support my work"},{"location":"getting-started/set-up-a-web-server/","text":"Set up a web server Let's prepare our local environment in order to create our first WebAR experience. Create a file structure Let's create a file structure for our AR experience: Create a new folder called ar-demo in your filesystem Download the latest release of MARTINS.js and extract dist/martins.js to ar-demo/ Create a new empty file called index.html and store it in ar-demo/ You will have the following file structure: ar-demo/ \u251c\u2500\u2500 index.html \u2514\u2500\u2500 martins.js Add boilerplate code Use the code editor of your choice to write the following content to index.html : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > </ body > </ html > Set up a local web server Let's set up a local web server in your machine for development purposes. I'll be showing you an easy-to-follow approach. Feel free to use a different approach if you're an experienced web developer. Graphical interface Command line This is an easy solution that works on Windows, Linux and macOS: Download and run Servez , a simple web server for local web development. In Folder to Serve , specify the ar-demo folder we have just created. Change the Port to 8000 . Click on Start to launch the local web server. Click on Launch Browser to open a web browser at http://localhost:8000 . Setting up Servez: make sure that Folder to Serve points to the correct path in your filesystem! If you're familiar with the command line, you can use programs such as python , node or php to launch a local web server. Navigate to the ar-demo directory and then run: Python 2 Python 3 Node.js PHP python -m SimpleHTTPServer 8000 python3 -m http.server 8000 npx http-server -p 8000 php -S localhost:8000 Next, open your web browser and go to http://localhost:8000 . You should see a blue screen in your web browser: Blue Screen of Success If you see that blue screen, you're ready to proceed. If not, review your settings. Why port 8000? Port 8000 is commonly used in web development. Although you may use a different port, I suggest that you stick to this convention throughout this guide.","title":"Set up a web server"},{"location":"getting-started/set-up-a-web-server/#set-up-a-web-server","text":"Let's prepare our local environment in order to create our first WebAR experience.","title":"Set up a web server"},{"location":"getting-started/set-up-a-web-server/#create-a-file-structure","text":"Let's create a file structure for our AR experience: Create a new folder called ar-demo in your filesystem Download the latest release of MARTINS.js and extract dist/martins.js to ar-demo/ Create a new empty file called index.html and store it in ar-demo/ You will have the following file structure: ar-demo/ \u251c\u2500\u2500 index.html \u2514\u2500\u2500 martins.js","title":"Create a file structure"},{"location":"getting-started/set-up-a-web-server/#add-boilerplate-code","text":"Use the code editor of your choice to write the following content to index.html : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > </ body > </ html >","title":"Add boilerplate code"},{"location":"getting-started/set-up-a-web-server/#set-up-a-local-web-server","text":"Let's set up a local web server in your machine for development purposes. I'll be showing you an easy-to-follow approach. Feel free to use a different approach if you're an experienced web developer. Graphical interface Command line This is an easy solution that works on Windows, Linux and macOS: Download and run Servez , a simple web server for local web development. In Folder to Serve , specify the ar-demo folder we have just created. Change the Port to 8000 . Click on Start to launch the local web server. Click on Launch Browser to open a web browser at http://localhost:8000 . Setting up Servez: make sure that Folder to Serve points to the correct path in your filesystem! If you're familiar with the command line, you can use programs such as python , node or php to launch a local web server. Navigate to the ar-demo directory and then run: Python 2 Python 3 Node.js PHP python -m SimpleHTTPServer 8000 python3 -m http.server 8000 npx http-server -p 8000 php -S localhost:8000 Next, open your web browser and go to http://localhost:8000 . You should see a blue screen in your web browser: Blue Screen of Success If you see that blue screen, you're ready to proceed. If not, review your settings. Why port 8000? Port 8000 is commonly used in web development. Although you may use a different port, I suggest that you stick to this convention throughout this guide.","title":"Set up a local web server"},{"location":"getting-started/set-up-the-session/","text":"Set up the session Now we're going to track our reference image for the first time! Create the viewport We begin by creating the viewport. Remember that the viewport is the area in which we'll display the augmented scene. Add the following to index.html and to ar-demo.js : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); } catch ( error ) { alert ( error . message ); } }; Create the source of data Let's set up our source of data. We get the HTMLVideoElement corresponding to the test video and then we use it to instantiate a video source of data. Write the following to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = Martins . Source . Video ( video ); } catch ( error ) { alert ( error . message ); } }; Start the session The session is a central component of a WebAR experience. The Martins namespace has a very special method called startSession . It receives a settings dictionary that lets us configure the new session in different ways. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = Martins . Source . Video ( video ); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); } catch ( error ) { alert ( error . message ); } }; Most of the settings passed to startSession correspond directly to the concepts we saw earlier. We're starting a new session in immersive mode, with the tracker, source of data and viewport that we have just configured. Let me explain what stats and gizmos mean: When you set stats: true , you're asking the engine to display a stats panel that shows useful data such as the current framerate. This is useful when developing WebAR experiences, but you should disable it in production. The option gizmos: true enables the gizmos. Gizmos are visual artifacts that help you visualize the current state of the tracker. They too are useful in development. In production, you may disable them or enable them partially (more on that later). Open http://localhost:8000 . You should see the tracking in action. Even though there is no virtual scene yet, the gizmos will show you the image being tracked. Image tracking in action! The code I have just presented is, in essence, what you need to start a session. I'm going to move it to a new function called startARSession for convenience: ar-demo.js window . onload = async function () { try { const session = await startARSession (); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = Martins . Source . Video ( video ); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Now all you have to do to start a new session is call startARSession() ! Write the user callback The user callback is a function responsible for updating and rendering the virtual scene. We have no virtual scene at the moment, but we can already set up that function. In order to do this, we must call session.requestAnimationFrame() and pass the user callback as an argument. ar-demo.js window . onload = async function () { try { const session = await startARSession (); function animate ( time , frame ) { session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate ); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { // ... } requestAnimationFrame Note that session.requestAnimationFrame() is different from window.requestAnimationFrame() . The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser.","title":"Set up the session"},{"location":"getting-started/set-up-the-session/#set-up-the-session","text":"Now we're going to track our reference image for the first time!","title":"Set up the session"},{"location":"getting-started/set-up-the-session/#create-the-viewport","text":"We begin by creating the viewport. Remember that the viewport is the area in which we'll display the augmented scene. Add the following to index.html and to ar-demo.js : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); } catch ( error ) { alert ( error . message ); } };","title":"Create the viewport"},{"location":"getting-started/set-up-the-session/#create-the-source-of-data","text":"Let's set up our source of data. We get the HTMLVideoElement corresponding to the test video and then we use it to instantiate a video source of data. Write the following to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = Martins . Source . Video ( video ); } catch ( error ) { alert ( error . message ); } };","title":"Create the source of data"},{"location":"getting-started/set-up-the-session/#start-the-session","text":"The session is a central component of a WebAR experience. The Martins namespace has a very special method called startSession . It receives a settings dictionary that lets us configure the new session in different ways. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = Martins . Source . Video ( video ); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); } catch ( error ) { alert ( error . message ); } }; Most of the settings passed to startSession correspond directly to the concepts we saw earlier. We're starting a new session in immersive mode, with the tracker, source of data and viewport that we have just configured. Let me explain what stats and gizmos mean: When you set stats: true , you're asking the engine to display a stats panel that shows useful data such as the current framerate. This is useful when developing WebAR experiences, but you should disable it in production. The option gizmos: true enables the gizmos. Gizmos are visual artifacts that help you visualize the current state of the tracker. They too are useful in development. In production, you may disable them or enable them partially (more on that later). Open http://localhost:8000 . You should see the tracking in action. Even though there is no virtual scene yet, the gizmos will show you the image being tracked. Image tracking in action! The code I have just presented is, in essence, what you need to start a session. I'm going to move it to a new function called startARSession for convenience: ar-demo.js window . onload = async function () { try { const session = await startARSession (); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = Martins . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = Martins . Source . Video ( video ); const session = await Martins . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Now all you have to do to start a new session is call startARSession() !","title":"Start the session"},{"location":"getting-started/set-up-the-session/#write-the-user-callback","text":"The user callback is a function responsible for updating and rendering the virtual scene. We have no virtual scene at the moment, but we can already set up that function. In order to do this, we must call session.requestAnimationFrame() and pass the user callback as an argument. ar-demo.js window . onload = async function () { try { const session = await startARSession (); function animate ( time , frame ) { session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate ); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { // ... } requestAnimationFrame Note that session.requestAnimationFrame() is different from window.requestAnimationFrame() . The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser.","title":"Write the user callback"},{"location":"getting-started/set-up-the-tracker/","text":"Set up the tracker In this section we'll learn how to set up the tracker. Later on we'll see how to use the tracker to track an image in a video, with its position and orientation in 3D. Add a reference image The first thing we need to do is add the image we want to track to our web page. We'll be calling that a reference image . We simply pick a suitable image and add an <img> tag to the page. Not all images are suitable for tracking. Images should be distinct, detailed and asymmetrical. I discuss this in detail in Guidelines for Images . For now we'll just use the following image: Reference Image Download the image to the ar-demo/ folder. Save it as my-reference-image.webp . Next, let's add the reference image to our web page. Add an <img> tag to the <body> of the page as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" > </ body > </ html > Reload the page. You should see the reference image: Reference image in a web page If you don't see the image, make sure that there are no errors in the filename. Once you see that the image is being properly loaded, there is no need to keep it visible. Let's add the hidden attribute to the <img> tag: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > </ body > </ html > Add a test video We're going to be tracking that reference image in a test video. Please save the following video as my-video.webm and my-video.mp4 in ar-demo/ . Later on I'll tell you how to use your webcam instead. This is the expected directory structure at this point: ar-demo/ \u251c\u2500\u2500 index.html \u251c\u2500\u2500 martins.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm Let's include the test video in our page. Add a <video> tag as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > Instantiate an Image Tracker In order to track the reference image in our video, we need an Image Tracker. Remember that a tracker is a subsystem of the WebAR engine that analyzes input data in some way. An Image Tracker is a tracker that finds and tracks reference images in a video stream. Before we track anything with an Image Tracker, we must tell it what to track. There are two steps to this: first, we instantiate an Image Tracker. Next, we link our reference image to it. We'll be writing a little bit of JavaScript code now. In order to keep our code clean, we'll be writing the JavaScript code to a new file. Let's add a <script> tag below martins.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > Create a new file called ar-demo.js and store it in the ar-demo/ folder. Write the following contents to it: ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); } catch ( error ) { alert ( error . message ); } }; The Martins namespace holds the various elements featured by the engine. We'll be using it extensively. MARTINS.js only requires standard web technologies that have been around for a while. Still, it's a good practice to check if those technologies are supported by the target system. If they are not, we display a message and quit. If they are, we instantiate an Image Tracker. Before moving on, make sure that you have the following directory structure at this point: ar-demo/ \u251c\u2500\u2500 ar-demo.js \u251c\u2500\u2500 index.html \u251c\u2500\u2500 martins.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm Link the image to the tracker Our Image Tracker has an internal database of reference images that it's capable of tracking. We call it a reference image database . To link a reference image to the tracker means to add that image to the database. When linking a reference image to the tracker, the appropriate HTMLImageElement must be provided to the database. You may optionally assign a name to the image, so that you can identify it later on. If you don't, an automatically generated name will be assigned for you. Let's link the image to the tracker. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); } catch ( error ) { alert ( error . message ); } }; Reload the page. If you see no errors popping up, it means that the image is linked to the tracker. You're ready to proceed!","title":"Set up the tracker"},{"location":"getting-started/set-up-the-tracker/#set-up-the-tracker","text":"In this section we'll learn how to set up the tracker. Later on we'll see how to use the tracker to track an image in a video, with its position and orientation in 3D.","title":"Set up the tracker"},{"location":"getting-started/set-up-the-tracker/#add-a-reference-image","text":"The first thing we need to do is add the image we want to track to our web page. We'll be calling that a reference image . We simply pick a suitable image and add an <img> tag to the page. Not all images are suitable for tracking. Images should be distinct, detailed and asymmetrical. I discuss this in detail in Guidelines for Images . For now we'll just use the following image: Reference Image Download the image to the ar-demo/ folder. Save it as my-reference-image.webp . Next, let's add the reference image to our web page. Add an <img> tag to the <body> of the page as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" > </ body > </ html > Reload the page. You should see the reference image: Reference image in a web page If you don't see the image, make sure that there are no errors in the filename. Once you see that the image is being properly loaded, there is no need to keep it visible. Let's add the hidden attribute to the <img> tag: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > </ body > </ html >","title":"Add a reference image"},{"location":"getting-started/set-up-the-tracker/#add-a-test-video","text":"We're going to be tracking that reference image in a test video. Please save the following video as my-video.webm and my-video.mp4 in ar-demo/ . Later on I'll tell you how to use your webcam instead. This is the expected directory structure at this point: ar-demo/ \u251c\u2500\u2500 index.html \u251c\u2500\u2500 martins.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm Let's include the test video in our page. Add a <video> tag as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html >","title":"Add a test video"},{"location":"getting-started/set-up-the-tracker/#instantiate-an-image-tracker","text":"In order to track the reference image in our video, we need an Image Tracker. Remember that a tracker is a subsystem of the WebAR engine that analyzes input data in some way. An Image Tracker is a tracker that finds and tracks reference images in a video stream. Before we track anything with an Image Tracker, we must tell it what to track. There are two steps to this: first, we instantiate an Image Tracker. Next, we link our reference image to it. We'll be writing a little bit of JavaScript code now. In order to keep our code clean, we'll be writing the JavaScript code to a new file. Let's add a <script> tag below martins.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > MARTINS.js WebAR demo </ title > < script src = \"martins.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > Create a new file called ar-demo.js and store it in the ar-demo/ folder. Write the following contents to it: ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); } catch ( error ) { alert ( error . message ); } }; The Martins namespace holds the various elements featured by the engine. We'll be using it extensively. MARTINS.js only requires standard web technologies that have been around for a while. Still, it's a good practice to check if those technologies are supported by the target system. If they are not, we display a message and quit. If they are, we instantiate an Image Tracker. Before moving on, make sure that you have the following directory structure at this point: ar-demo/ \u251c\u2500\u2500 ar-demo.js \u251c\u2500\u2500 index.html \u251c\u2500\u2500 martins.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm","title":"Instantiate an Image Tracker"},{"location":"getting-started/set-up-the-tracker/#link-the-image-to-the-tracker","text":"Our Image Tracker has an internal database of reference images that it's capable of tracking. We call it a reference image database . To link a reference image to the tracker means to add that image to the database. When linking a reference image to the tracker, the appropriate HTMLImageElement must be provided to the database. You may optionally assign a name to the image, so that you can identify it later on. If you don't, an automatically generated name will be assigned for you. Let's link the image to the tracker. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! Martins . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = Martins . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); } catch ( error ) { alert ( error . message ); } }; Reload the page. If you see no errors popping up, it means that the image is linked to the tracker. You're ready to proceed!","title":"Link the image to the tracker"}]}