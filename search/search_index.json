{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"demos/","text":"","title":"Demos"},{"location":"download/","text":"","title":"Download"},{"location":"faq/","text":"Questions & Answers What is encantar.js? encantar.js is a standalone GPU-accelerated Augmented Reality engine for the web. The name is derived from the Portuguese and Spanish word encantar , which means: to enchant, to delight, to love, to fascinate, to put a magical spell on someone or something. What is WebAR? Refer to the concepts . What are your recommendations for WebAR? For a good experience with WebAR: Don't move the camera nor the target image too quickly. This produces motion blur. The target image should appear clearly in the video. The physical environment should be properly illuminated. If you're scanning the image on a screen, make sure to adjust the brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable. If you print the image, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. Avoid low-quality cameras. Cameras of common smartphones are okay. Is this WebXR? No, encantar.js is not WebXR. The WebXR API allows you to access functionalities of VR and AR-capable devices in web browsers. It relies on other technologies, such as Google's ARCore or Apple's ARKit, to run the show. Those technologies are great, though they are supported on specific devices, which may or may not match your users' devices. On the other hand, encantar.js is fully standalone and is built from scratch using standard web technologies such as WebGL2 and WebAssembly, which are widely available. My intention is to give it broad compatibility. Why do my models appear \"laid down\" in AR? encantar.js uses a right-handed coordinate system with the Z-axis pointing \"up\". The same convention is used in Blender . When exporting your own models, make sure that the Z-axis points \"up\" and that the ground plane is the XY-plane. If your models appear \"laid down\" in AR, this is probably the issue. Fix with code Fixing the orientation of the model is the preferred solution. However, you can also fix the issue with code: add a node (entity) to the scene graph and make it rotate its children by 90 degrees around the x-axis. Can I increase the resolution of the tracking? Yes. You can increase the resolution of the tracker , as well as the resolution of the camera , using the API. You can also increase the resolution of the rendered virtual scene by setting the resolution of the viewport . Performance is affected by various factors such as upload times (GPU). Test your AR experience on your target devices to find a good balance between performance and increased resolution. I am enchanted! I know!","title":"Questions & Answers"},{"location":"faq/#questions-answers","text":"","title":"Questions &amp; Answers"},{"location":"faq/#what-is-encantarjs","text":"encantar.js is a standalone GPU-accelerated Augmented Reality engine for the web. The name is derived from the Portuguese and Spanish word encantar , which means: to enchant, to delight, to love, to fascinate, to put a magical spell on someone or something.","title":"What is encantar.js?"},{"location":"faq/#what-is-webar","text":"Refer to the concepts .","title":"What is WebAR?"},{"location":"faq/#what-are-your-recommendations-for-webar","text":"For a good experience with WebAR: Don't move the camera nor the target image too quickly. This produces motion blur. The target image should appear clearly in the video. The physical environment should be properly illuminated. If you're scanning the image on a screen, make sure to adjust the brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable. If you print the image, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. Avoid low-quality cameras. Cameras of common smartphones are okay.","title":"What are your recommendations for WebAR?"},{"location":"faq/#is-this-webxr","text":"No, encantar.js is not WebXR. The WebXR API allows you to access functionalities of VR and AR-capable devices in web browsers. It relies on other technologies, such as Google's ARCore or Apple's ARKit, to run the show. Those technologies are great, though they are supported on specific devices, which may or may not match your users' devices. On the other hand, encantar.js is fully standalone and is built from scratch using standard web technologies such as WebGL2 and WebAssembly, which are widely available. My intention is to give it broad compatibility.","title":"Is this WebXR?"},{"location":"faq/#why-do-my-models-appear-laid-down-in-ar","text":"encantar.js uses a right-handed coordinate system with the Z-axis pointing \"up\". The same convention is used in Blender . When exporting your own models, make sure that the Z-axis points \"up\" and that the ground plane is the XY-plane. If your models appear \"laid down\" in AR, this is probably the issue. Fix with code Fixing the orientation of the model is the preferred solution. However, you can also fix the issue with code: add a node (entity) to the scene graph and make it rotate its children by 90 degrees around the x-axis.","title":"Why do my models appear \"laid down\" in AR?"},{"location":"faq/#can-i-increase-the-resolution-of-the-tracking","text":"Yes. You can increase the resolution of the tracker , as well as the resolution of the camera , using the API. You can also increase the resolution of the rendered virtual scene by setting the resolution of the viewport . Performance is affected by various factors such as upload times (GPU). Test your AR experience on your target devices to find a good balance between performance and increased resolution.","title":"Can I increase the resolution of the tracking?"},{"location":"faq/#i-am-enchanted","text":"I know!","title":"I am enchanted!"},{"location":"license/","text":"GNU LESSER GENERAL PUBLIC LICENSE Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. This version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below. 0. Additional Definitions. As used herein, \"this License\" refers to version 3 of the GNU Lesser General Public License, and the \"GNU GPL\" refers to version 3 of the GNU General Public License. \"The Library\" refers to a covered work governed by this License, other than an Application or a Combined Work as defined below. An \"Application\" is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library. A \"Combined Work\" is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the \"Linked Version\". The \"Minimal Corresponding Source\" for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version. The \"Corresponding Application Code\" for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work. 1. Exception to Section 3 of the GNU GPL. You may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL. 2. Conveying Modified Versions. If you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version: a) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or b) under the GNU GPL, with none of the additional permissions of this License applicable to that copy. 3. Object Code Incorporating Material from Library Header Files. The object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following: a) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the object code with a copy of the GNU GPL and this license document. 4. Combined Works. You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following: a) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the Combined Work with a copy of the GNU GPL and this license document. c) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document. d) Do one of the following: 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. e) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.) 5. Combined Libraries. You may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License. b) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work. 6. Revised Versions of the GNU Lesser General Public License. The Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation. If the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.","title":"License"},{"location":"license/#gnu-lesser-general-public-license","text":"Version 3, 29 June 2007 Copyright (C) 2007 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. This version of the GNU Lesser General Public License incorporates the terms and conditions of version 3 of the GNU General Public License, supplemented by the additional permissions listed below.","title":"GNU LESSER GENERAL PUBLIC LICENSE"},{"location":"license/#0-additional-definitions","text":"As used herein, \"this License\" refers to version 3 of the GNU Lesser General Public License, and the \"GNU GPL\" refers to version 3 of the GNU General Public License. \"The Library\" refers to a covered work governed by this License, other than an Application or a Combined Work as defined below. An \"Application\" is any work that makes use of an interface provided by the Library, but which is not otherwise based on the Library. Defining a subclass of a class defined by the Library is deemed a mode of using an interface provided by the Library. A \"Combined Work\" is a work produced by combining or linking an Application with the Library. The particular version of the Library with which the Combined Work was made is also called the \"Linked Version\". The \"Minimal Corresponding Source\" for a Combined Work means the Corresponding Source for the Combined Work, excluding any source code for portions of the Combined Work that, considered in isolation, are based on the Application, and not on the Linked Version. The \"Corresponding Application Code\" for a Combined Work means the object code and/or source code for the Application, including any data and utility programs needed for reproducing the Combined Work from the Application, but excluding the System Libraries of the Combined Work.","title":"0. Additional Definitions."},{"location":"license/#1-exception-to-section-3-of-the-gnu-gpl","text":"You may convey a covered work under sections 3 and 4 of this License without being bound by section 3 of the GNU GPL.","title":"1. Exception to Section 3 of the GNU GPL."},{"location":"license/#2-conveying-modified-versions","text":"If you modify a copy of the Library, and, in your modifications, a facility refers to a function or data to be supplied by an Application that uses the facility (other than as an argument passed when the facility is invoked), then you may convey a copy of the modified version: a) under this License, provided that you make a good faith effort to ensure that, in the event an Application does not supply the function or data, the facility still operates, and performs whatever part of its purpose remains meaningful, or b) under the GNU GPL, with none of the additional permissions of this License applicable to that copy.","title":"2. Conveying Modified Versions."},{"location":"license/#3-object-code-incorporating-material-from-library-header-files","text":"The object code form of an Application may incorporate material from a header file that is part of the Library. You may convey such object code under terms of your choice, provided that, if the incorporated material is not limited to numerical parameters, data structure layouts and accessors, or small macros, inline functions and templates (ten or fewer lines in length), you do both of the following: a) Give prominent notice with each copy of the object code that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the object code with a copy of the GNU GPL and this license document.","title":"3. Object Code Incorporating Material from Library Header Files."},{"location":"license/#4-combined-works","text":"You may convey a Combined Work under terms of your choice that, taken together, effectively do not restrict modification of the portions of the Library contained in the Combined Work and reverse engineering for debugging such modifications, if you also do each of the following: a) Give prominent notice with each copy of the Combined Work that the Library is used in it and that the Library and its use are covered by this License. b) Accompany the Combined Work with a copy of the GNU GPL and this license document. c) For a Combined Work that displays copyright notices during execution, include the copyright notice for the Library among these notices, as well as a reference directing the user to the copies of the GNU GPL and this license document. d) Do one of the following: 0) Convey the Minimal Corresponding Source under the terms of this License, and the Corresponding Application Code in a form suitable for, and under terms that permit, the user to recombine or relink the Application with a modified version of the Linked Version to produce a modified Combined Work, in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source. 1) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (a) uses at run time a copy of the Library already present on the user's computer system, and (b) will operate properly with a modified version of the Library that is interface-compatible with the Linked Version. e) Provide Installation Information, but only if you would otherwise be required to provide such information under section 6 of the GNU GPL, and only to the extent that such information is necessary to install and execute a modified version of the Combined Work produced by recombining or relinking the Application with a modified version of the Linked Version. (If you use option 4d0, the Installation Information must accompany the Minimal Corresponding Source and Corresponding Application Code. If you use option 4d1, you must provide the Installation Information in the manner specified by section 6 of the GNU GPL for conveying Corresponding Source.)","title":"4. Combined Works."},{"location":"license/#5-combined-libraries","text":"You may place library facilities that are a work based on the Library side by side in a single library together with other library facilities that are not Applications and are not covered by this License, and convey such a combined library under terms of your choice, if you do both of the following: a) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities, conveyed under the terms of this License. b) Give prominent notice with the combined library that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.","title":"5. Combined Libraries."},{"location":"license/#6-revised-versions-of-the-gnu-lesser-general-public-license","text":"The Free Software Foundation may publish revised and/or new versions of the GNU Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. Each version is given a distinguishing version number. If the Library as you received it specifies that a certain numbered version of the GNU Lesser General Public License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that published version or of any later version published by the Free Software Foundation. If the Library as you received it does not specify a version number of the GNU Lesser General Public License, you may choose any version of the GNU Lesser General Public License ever published by the Free Software Foundation. If the Library as you received it specifies that a proxy can decide whether future versions of the GNU Lesser General Public License shall apply, that proxy's public statement of acceptance of any version is permanent authorization for you to choose that version for the Library.","title":"6. Revised Versions of the GNU Lesser General Public License."},{"location":"support-my-work/","text":"","title":"Support my work"},{"location":"api/ar-event-listener/","text":"AREventListener A function that is linked to an AREventTarget . It is called as soon as that AREventTarget receives an AREvent of a specific type . The event is passed as an argument.","title":"AREventListener"},{"location":"api/ar-event-listener/#areventlistener","text":"A function that is linked to an AREventTarget . It is called as soon as that AREventTarget receives an AREvent of a specific type . The event is passed as an argument.","title":"AREventListener"},{"location":"api/ar-event-target/","text":"AREventTarget An AREventTarget is an object that is able to receive AREvents . You may add event listeners to it in order to listen to \"relevant changes\" in its state. Methods addEventListener target.addEventListener(type: AREventType, listener: AREventListener): void Add an event listener to target . Arguments type: AREventType . The type of event you intend to listen to. listener: AREventListener . The event listener you intend to add. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); }); removeEventListener target.removeEventListener(type: AREventType, listener: AREventListener): void Remove an event listener from target . Arguments type: AREventType . The type of event you are listening to. listener: AREventListener . The event listener you intend to remove.","title":"AREventTarget"},{"location":"api/ar-event-target/#areventtarget","text":"An AREventTarget is an object that is able to receive AREvents . You may add event listeners to it in order to listen to \"relevant changes\" in its state.","title":"AREventTarget"},{"location":"api/ar-event-target/#methods","text":"","title":"Methods"},{"location":"api/ar-event-target/#addeventlistener","text":"target.addEventListener(type: AREventType, listener: AREventListener): void Add an event listener to target . Arguments type: AREventType . The type of event you intend to listen to. listener: AREventListener . The event listener you intend to add. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); });","title":"addEventListener"},{"location":"api/ar-event-target/#removeeventlistener","text":"target.removeEventListener(type: AREventType, listener: AREventListener): void Remove an event listener from target . Arguments type: AREventType . The type of event you are listening to. listener: AREventListener . The event listener you intend to remove.","title":"removeEventListener"},{"location":"api/ar-event-type/","text":"AREventType An AREventType is a string representing the type of an AREvent . The documentation of the different AREventTargets (e.g., Session ) specify which event types are valid for those targets.","title":"AREventType"},{"location":"api/ar-event-type/#areventtype","text":"An AREventType is a string representing the type of an AREvent . The documentation of the different AREventTargets (e.g., Session ) specify which event types are valid for those targets.","title":"AREventType"},{"location":"api/ar-event/","text":"AREvent An AREvent is an Event sent to an AREventTarget . AREvents are used to notify AREventListeners about \"relevant changes\" in the state of AREventTargets . Properties type event.type: AREventType An AREventType representing the type of the event.","title":"AREvent"},{"location":"api/ar-event/#arevent","text":"An AREvent is an Event sent to an AREventTarget . AREvents are used to notify AREventListeners about \"relevant changes\" in the state of AREventTargets .","title":"AREvent"},{"location":"api/ar-event/#properties","text":"","title":"Properties"},{"location":"api/ar-event/#type","text":"event.type: AREventType An AREventType representing the type of the event.","title":"type"},{"location":"api/ar/","text":"AR The AR namespace is the entry point of the features and components of encantar.js. Properties Settings AR.Settings: Settings, read-only The settings of the engine. version AR.version: string, read-only The version of encantar.js. Methods isSupported AR.isSupported(): boolean Checks if the user agent is capable of running the engine. Returns Returns true if the user agent is compatible with the engine, or false otherwise.","title":"AR"},{"location":"api/ar/#ar","text":"The AR namespace is the entry point of the features and components of encantar.js.","title":"AR"},{"location":"api/ar/#properties","text":"","title":"Properties"},{"location":"api/ar/#settings","text":"AR.Settings: Settings, read-only The settings of the engine.","title":"Settings"},{"location":"api/ar/#version","text":"AR.version: string, read-only The version of encantar.js.","title":"version"},{"location":"api/ar/#methods","text":"","title":"Methods"},{"location":"api/ar/#issupported","text":"AR.isSupported(): boolean Checks if the user agent is capable of running the engine. Returns Returns true if the user agent is compatible with the engine, or false otherwise.","title":"isSupported"},{"location":"api/camera-source/","text":"CameraSource A source of data linked to a webcam. Instantiation AR.Source.Camera AR.Source.Camera(settings: object): CameraSource Create a new webcam-based source of data with the specified settings . Arguments settings: object, optional . An object with the following keys (all are optional): resolution: Resolution . The desired resolution of the video. The higher the resolution, the longer it takes for the video to be uploaded to the GPU, which impacts performance. The lower the resolution, the less accurate the tracking will be. Suggested values: \"md+\" , \"md\" , \"sm+\" , \"sm\" . aspectRatio: number . A hint specifying the preferred aspect ratio of the video. constraints: MediaTrackConstraints . Additional video constraints that will be passed to navigator.mediaDevices.getUserMedia() . Returns A new webcam-based source of data. Example const webcam = AR . Source . Camera ({ resolution : 'md+' , constraints : { facingMode : 'environment' // will prefer the rear camera on mobile devices //facingMode: 'user' // will prefer the front camera on mobile devices } }); Properties resolution source.resolution: Resolution, read-only The resolution of this source of data.","title":"CameraSource"},{"location":"api/camera-source/#camerasource","text":"A source of data linked to a webcam.","title":"CameraSource"},{"location":"api/camera-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/camera-source/#arsourcecamera","text":"AR.Source.Camera(settings: object): CameraSource Create a new webcam-based source of data with the specified settings . Arguments settings: object, optional . An object with the following keys (all are optional): resolution: Resolution . The desired resolution of the video. The higher the resolution, the longer it takes for the video to be uploaded to the GPU, which impacts performance. The lower the resolution, the less accurate the tracking will be. Suggested values: \"md+\" , \"md\" , \"sm+\" , \"sm\" . aspectRatio: number . A hint specifying the preferred aspect ratio of the video. constraints: MediaTrackConstraints . Additional video constraints that will be passed to navigator.mediaDevices.getUserMedia() . Returns A new webcam-based source of data. Example const webcam = AR . Source . Camera ({ resolution : 'md+' , constraints : { facingMode : 'environment' // will prefer the rear camera on mobile devices //facingMode: 'user' // will prefer the front camera on mobile devices } });","title":"AR.Source.Camera"},{"location":"api/camera-source/#properties","text":"","title":"Properties"},{"location":"api/camera-source/#resolution","text":"source.resolution: Resolution, read-only The resolution of this source of data.","title":"resolution"},{"location":"api/canvas-source/","text":"CanvasSource A source of data linked to a <canvas> element. Instantiation AR.Source.Canvas AR.Source.Canvas(canvas: HTMLCanvasElement): CanvasSource Create a new source of data linked to the provided canvas . Arguments canvas: HTMLCanvasElement . A <canvas> element. Returns A new source of data.","title":"CanvasSource"},{"location":"api/canvas-source/#canvassource","text":"A source of data linked to a <canvas> element.","title":"CanvasSource"},{"location":"api/canvas-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/canvas-source/#arsourcecanvas","text":"AR.Source.Canvas(canvas: HTMLCanvasElement): CanvasSource Create a new source of data linked to the provided canvas . Arguments canvas: HTMLCanvasElement . A <canvas> element. Returns A new source of data.","title":"AR.Source.Canvas"},{"location":"api/frame/","text":"Frame A Frame holds data for augmenting the physical scene with the virtual scene. Properties session frame.session: Session, read-only A reference to the session . results frame.results: Iterable<TrackerResult>, read-only Use this property to iterate through the results generated by the trackers . Example function animate ( time , frame ) { for ( const result of frame . results ) { // ... } session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate );","title":"Frame"},{"location":"api/frame/#frame","text":"A Frame holds data for augmenting the physical scene with the virtual scene.","title":"Frame"},{"location":"api/frame/#properties","text":"","title":"Properties"},{"location":"api/frame/#session","text":"frame.session: Session, read-only A reference to the session .","title":"session"},{"location":"api/frame/#results","text":"frame.results: Iterable<TrackerResult>, read-only Use this property to iterate through the results generated by the trackers . Example function animate ( time , frame ) { for ( const result of frame . results ) { // ... } session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate );","title":"results"},{"location":"api/gizmos/","text":"Gizmos Gizmos provide visual cues about the state of the trackers . They are particularly useful during development. Properties visible gizmos.visible: boolean Whether or not the gizmos are visible.","title":"Gizmos"},{"location":"api/gizmos/#gizmos","text":"Gizmos provide visual cues about the state of the trackers . They are particularly useful during development.","title":"Gizmos"},{"location":"api/gizmos/#properties","text":"","title":"Properties"},{"location":"api/gizmos/#visible","text":"gizmos.visible: boolean Whether or not the gizmos are visible.","title":"visible"},{"location":"api/hud/","text":"HUD A HUD (Heads Up Display) is an overlay used to display 2D elements that do not correlate with the physical scene. It's part of a viewport and occupies its entire space. It appears in front of the augmented scene. Properties container hud.container: HTMLDivElement, read-only The container of the HUD. visible hud.visible: boolean Whether or not the HUD is visible.","title":"HUD"},{"location":"api/hud/#hud","text":"A HUD (Heads Up Display) is an overlay used to display 2D elements that do not correlate with the physical scene. It's part of a viewport and occupies its entire space. It appears in front of the augmented scene.","title":"HUD"},{"location":"api/hud/#properties","text":"","title":"Properties"},{"location":"api/hud/#container","text":"hud.container: HTMLDivElement, read-only The container of the HUD.","title":"container"},{"location":"api/hud/#visible","text":"hud.visible: boolean Whether or not the HUD is visible.","title":"visible"},{"location":"api/image-tracker-result/","text":"ImageTrackerResult A result generated by an Image Tracker . Properties tracker result.tracker: ImageTracker, read-only A reference to the Image Tracker that generated this result. trackables result.trackables: TrackableImage[], read-only An array of zero or one TrackableImage object(s). viewer result.viewer: Viewer | undefined, read-only A viewer associated with the trackable. If there is no trackable, this property will be undefined .","title":"ImageTrackerResult"},{"location":"api/image-tracker-result/#imagetrackerresult","text":"A result generated by an Image Tracker .","title":"ImageTrackerResult"},{"location":"api/image-tracker-result/#properties","text":"","title":"Properties"},{"location":"api/image-tracker-result/#tracker","text":"result.tracker: ImageTracker, read-only A reference to the Image Tracker that generated this result.","title":"tracker"},{"location":"api/image-tracker-result/#trackables","text":"result.trackables: TrackableImage[], read-only An array of zero or one TrackableImage object(s).","title":"trackables"},{"location":"api/image-tracker-result/#viewer","text":"result.viewer: Viewer | undefined, read-only A viewer associated with the trackable. If there is no trackable, this property will be undefined .","title":"viewer"},{"location":"api/image-tracker/","text":"ImageTracker A tracker that tracks images in a video. Images are tracked using templates known as reference images . Instantiation AR.Tracker.ImageTracker AR.Tracker.ImageTracker(): ImageTracker Instantiate an image tracker. Returns A new image tracker. Example const imageTracker = AR . Tracker . ImageTracker (); Properties type tracker.type: string, read-only The string \"image-tracker\" . state tracker.state: string, read-only The current state of the tracker. database tracker.database: ReferenceImageDatabase, read-only A database of reference images . resolution tracker.resolution: Resolution The resolution adopted by the computer vision algorithms implemented in the tracker. Higher resolutions improve the tracking quality, but are computationally more expensive. Note that this resolution is different from, and should not be larger than, the resolution of the camera ! Events An ImageTracker is an AREventTarget . You can listen to the following events: targetfound A target has been found. Properties referenceImage: ReferenceImage . The reference image that is linked to the target. Example tracker . addEventListener ( 'targetfound' , event => { console . log ( 'Found target: ' + event . referenceImage . name ); }); targetlost A target has been lost. Properties referenceImage: ReferenceImage . The reference image that is linked to the target.","title":"ImageTracker"},{"location":"api/image-tracker/#imagetracker","text":"A tracker that tracks images in a video. Images are tracked using templates known as reference images .","title":"ImageTracker"},{"location":"api/image-tracker/#instantiation","text":"","title":"Instantiation"},{"location":"api/image-tracker/#artrackerimagetracker","text":"AR.Tracker.ImageTracker(): ImageTracker Instantiate an image tracker. Returns A new image tracker. Example const imageTracker = AR . Tracker . ImageTracker ();","title":"AR.Tracker.ImageTracker"},{"location":"api/image-tracker/#properties","text":"","title":"Properties"},{"location":"api/image-tracker/#type","text":"tracker.type: string, read-only The string \"image-tracker\" .","title":"type"},{"location":"api/image-tracker/#state","text":"tracker.state: string, read-only The current state of the tracker.","title":"state"},{"location":"api/image-tracker/#database","text":"tracker.database: ReferenceImageDatabase, read-only A database of reference images .","title":"database"},{"location":"api/image-tracker/#resolution","text":"tracker.resolution: Resolution The resolution adopted by the computer vision algorithms implemented in the tracker. Higher resolutions improve the tracking quality, but are computationally more expensive. Note that this resolution is different from, and should not be larger than, the resolution of the camera !","title":"resolution"},{"location":"api/image-tracker/#events","text":"An ImageTracker is an AREventTarget . You can listen to the following events:","title":"Events"},{"location":"api/image-tracker/#targetfound","text":"A target has been found. Properties referenceImage: ReferenceImage . The reference image that is linked to the target. Example tracker . addEventListener ( 'targetfound' , event => { console . log ( 'Found target: ' + event . referenceImage . name ); });","title":"targetfound"},{"location":"api/image-tracker/#targetlost","text":"A target has been lost. Properties referenceImage: ReferenceImage . The reference image that is linked to the target.","title":"targetlost"},{"location":"api/perspective-view/","text":"PerspectiveView A View that models a perspective projection. Properties aspect view.aspect: number, read-only Aspect ratio of the viewing frustum. fovx view.fovx: number, read-only Horizontal field-of-view of the viewing frustum, measured in radians. Since: 0.3.0 fovy view.fovy: number, read-only Vertical field-of-view of the viewing frustum, measured in radians. near view.near: number, read-only Distance of the near clipping plane of the viewing frustum to the Z = 0 plane in viewer space. far view.far: number, read-only Distance of the far clipping plane of the viewing frustum to the Z = 0 plane in viewer space.","title":"PerspectiveView"},{"location":"api/perspective-view/#perspectiveview","text":"A View that models a perspective projection.","title":"PerspectiveView"},{"location":"api/perspective-view/#properties","text":"","title":"Properties"},{"location":"api/perspective-view/#aspect","text":"view.aspect: number, read-only Aspect ratio of the viewing frustum.","title":"aspect"},{"location":"api/perspective-view/#fovx","text":"view.fovx: number, read-only Horizontal field-of-view of the viewing frustum, measured in radians. Since: 0.3.0","title":"fovx"},{"location":"api/perspective-view/#fovy","text":"view.fovy: number, read-only Vertical field-of-view of the viewing frustum, measured in radians.","title":"fovy"},{"location":"api/perspective-view/#near","text":"view.near: number, read-only Distance of the near clipping plane of the viewing frustum to the Z = 0 plane in viewer space.","title":"near"},{"location":"api/perspective-view/#far","text":"view.far: number, read-only Distance of the far clipping plane of the viewing frustum to the Z = 0 plane in viewer space.","title":"far"},{"location":"api/pointer-source/","text":"PointerSource A source of pointer -based input. It feeds a PointerTracker . Since: 0.4.0 Instantiation AR.Source.Pointer AR.Source.Pointer(): PointerSource Create a new PointerSource . Returns A new PointerSource . Example const pointerSource = AR . Source . Pointer ();","title":"PointerSource"},{"location":"api/pointer-source/#pointersource","text":"A source of pointer -based input. It feeds a PointerTracker . Since: 0.4.0","title":"PointerSource"},{"location":"api/pointer-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/pointer-source/#arsourcepointer","text":"AR.Source.Pointer(): PointerSource Create a new PointerSource . Returns A new PointerSource . Example const pointerSource = AR . Source . Pointer ();","title":"AR.Source.Pointer"},{"location":"api/pointer-tracker-result/","text":"PointerTrackerResult A result generated by a PointerTracker . Since: 0.4.0 Properties tracker result.tracker: PointerTracker, read-only A reference to the PointerTracker that generated this result. trackables result.trackables: TrackablePointer[], read-only An array of TrackablePointers .","title":"PointerTrackerResult"},{"location":"api/pointer-tracker-result/#pointertrackerresult","text":"A result generated by a PointerTracker . Since: 0.4.0","title":"PointerTrackerResult"},{"location":"api/pointer-tracker-result/#properties","text":"","title":"Properties"},{"location":"api/pointer-tracker-result/#tracker","text":"result.tracker: PointerTracker, read-only A reference to the PointerTracker that generated this result.","title":"tracker"},{"location":"api/pointer-tracker-result/#trackables","text":"result.trackables: TrackablePointer[], read-only An array of TrackablePointers .","title":"trackables"},{"location":"api/pointer-tracker/","text":"PointerTracker A tracker of pointers . It consumes data from a PointerSource and produces PointerTrackerResults . Since: 0.4.0 Instantiation AR.Tracker.Pointer AR.Tracker.Pointer(): PointerTracker Create a new PointerTracker . Returns A new PointerTracker . Example const pointerTracker = AR . Tracker . Pointer (); Properties type tracker.type: string, read-only The string \"pointer-tracker\" .","title":"PointerTracker"},{"location":"api/pointer-tracker/#pointertracker","text":"A tracker of pointers . It consumes data from a PointerSource and produces PointerTrackerResults . Since: 0.4.0","title":"PointerTracker"},{"location":"api/pointer-tracker/#instantiation","text":"","title":"Instantiation"},{"location":"api/pointer-tracker/#artrackerpointer","text":"AR.Tracker.Pointer(): PointerTracker Create a new PointerTracker . Returns A new PointerTracker . Example const pointerTracker = AR . Tracker . Pointer ();","title":"AR.Tracker.Pointer"},{"location":"api/pointer-tracker/#properties","text":"","title":"Properties"},{"location":"api/pointer-tracker/#type","text":"tracker.type: string, read-only The string \"pointer-tracker\" .","title":"type"},{"location":"api/pose/","text":"Pose A pose represents a position and an orientation in 3D space. Properties transform pose.transform: Transform, read-only The underlying transform .","title":"Pose"},{"location":"api/pose/#pose","text":"A pose represents a position and an orientation in 3D space.","title":"Pose"},{"location":"api/pose/#properties","text":"","title":"Properties"},{"location":"api/pose/#transform","text":"pose.transform: Transform, read-only The underlying transform .","title":"transform"},{"location":"api/quaternion/","text":"Quaternion A number system used in encantar.js to represent rotations in 3D space. Since: 0.4.0 Properties x quaternion.x: number, read-only The x coordinate of the quaternion (imaginary). y quaternion.y: number, read-only The y coordinate of the quaternion (imaginary). z quaternion.z: number, read-only The z coordinate of the quaternion (imaginary). w quaternion.w: number, read-only The w coordinate of the quaternion (real). Methods length quaternion.length(): number Compute the magnitude of the quaternion. Returns The magnitude of the quaternion. equals quaternion.equals(q: Quaternion): boolean Check if this and q have the same coordinates. Arguments q: Quaternion . A quaternion. Returns true if this and q have the same coordinates. toString quaternion.toString(): string Generate a string representation of the quaternion. Returns A string representation of the quaternion.","title":"Quaternion"},{"location":"api/quaternion/#quaternion","text":"A number system used in encantar.js to represent rotations in 3D space. Since: 0.4.0","title":"Quaternion"},{"location":"api/quaternion/#properties","text":"","title":"Properties"},{"location":"api/quaternion/#x","text":"quaternion.x: number, read-only The x coordinate of the quaternion (imaginary).","title":"x"},{"location":"api/quaternion/#y","text":"quaternion.y: number, read-only The y coordinate of the quaternion (imaginary).","title":"y"},{"location":"api/quaternion/#z","text":"quaternion.z: number, read-only The z coordinate of the quaternion (imaginary).","title":"z"},{"location":"api/quaternion/#w","text":"quaternion.w: number, read-only The w coordinate of the quaternion (real).","title":"w"},{"location":"api/quaternion/#methods","text":"","title":"Methods"},{"location":"api/quaternion/#length","text":"quaternion.length(): number Compute the magnitude of the quaternion. Returns The magnitude of the quaternion.","title":"length"},{"location":"api/quaternion/#equals","text":"quaternion.equals(q: Quaternion): boolean Check if this and q have the same coordinates. Arguments q: Quaternion . A quaternion. Returns true if this and q have the same coordinates.","title":"equals"},{"location":"api/quaternion/#tostring","text":"quaternion.toString(): string Generate a string representation of the quaternion. Returns A string representation of the quaternion.","title":"toString"},{"location":"api/ray/","text":"Ray A ray with origin and direction. Since: 0.4.0 Properties origin ray.origin: Vector3 The origin point of the ray. direction ray.direction: Vector3 The direction of the ray, a unit vector.","title":"Ray"},{"location":"api/ray/#ray","text":"A ray with origin and direction. Since: 0.4.0","title":"Ray"},{"location":"api/ray/#properties","text":"","title":"Properties"},{"location":"api/ray/#origin","text":"ray.origin: Vector3 The origin point of the ray.","title":"origin"},{"location":"api/ray/#direction","text":"ray.direction: Vector3 The direction of the ray, a unit vector.","title":"direction"},{"location":"api/reference-image-database/","text":"ReferenceImageDatabase A database of reference images that belongs to an Image Tracker . Properties count database.count: number, read-only The number of reference images stored in this database. capacity database.capacity: number The maximum number of reference images that can be stored in this database. Note: this property is writable since version 0.3.0 (experimental). Methods add database.add(referenceImages: ReferenceImage[]): SpeedyPromise<void> Add reference image(s) to the database. Arguments referenceImages: ReferenceImage[] . The reference image(s) you want to add. Returns A promise that resolves as soon as the provided reference images are loaded and added to the database. Example const referenceImages = [{ name : 'my-first-image' , image : document . getElementById ( 'my-first-image' ) }, { name : 'my-second-image' , image : document . getElementById ( 'my-second-image' ) }]; tracker . database . add ( referenceImages ). then (() => { console . log ( 'The images have been added to the database' ); }); @@iterator database[Symbol.iterator](): Iterator<ReferenceImage> This is used to iterate over the reference images stored in the database. Returns An iterator. Example for ( const referenceImage of tracker . database ) { console . log ( referenceImage . name ); }","title":"ReferenceImageDatabase"},{"location":"api/reference-image-database/#referenceimagedatabase","text":"A database of reference images that belongs to an Image Tracker .","title":"ReferenceImageDatabase"},{"location":"api/reference-image-database/#properties","text":"","title":"Properties"},{"location":"api/reference-image-database/#count","text":"database.count: number, read-only The number of reference images stored in this database.","title":"count"},{"location":"api/reference-image-database/#capacity","text":"database.capacity: number The maximum number of reference images that can be stored in this database. Note: this property is writable since version 0.3.0 (experimental).","title":"capacity"},{"location":"api/reference-image-database/#methods","text":"","title":"Methods"},{"location":"api/reference-image-database/#add","text":"database.add(referenceImages: ReferenceImage[]): SpeedyPromise<void> Add reference image(s) to the database. Arguments referenceImages: ReferenceImage[] . The reference image(s) you want to add. Returns A promise that resolves as soon as the provided reference images are loaded and added to the database. Example const referenceImages = [{ name : 'my-first-image' , image : document . getElementById ( 'my-first-image' ) }, { name : 'my-second-image' , image : document . getElementById ( 'my-second-image' ) }]; tracker . database . add ( referenceImages ). then (() => { console . log ( 'The images have been added to the database' ); });","title":"add"},{"location":"api/reference-image-database/#iterator","text":"database[Symbol.iterator](): Iterator<ReferenceImage> This is used to iterate over the reference images stored in the database. Returns An iterator. Example for ( const referenceImage of tracker . database ) { console . log ( referenceImage . name ); }","title":"@@iterator"},{"location":"api/reference-image/","text":"ReferenceImage An interface specifying an image template that is fed to an Image Tracker . Properties name referenceImage.name: string, read-only A name used to identify this reference image in a database . image referenceImage.image: HTMLImageElement | ImageBitmap | ImageData, read-only Image template with pixel data. Note: ImageData is acceptable since version 0.4.0.","title":"ReferenceImage"},{"location":"api/reference-image/#referenceimage","text":"An interface specifying an image template that is fed to an Image Tracker .","title":"ReferenceImage"},{"location":"api/reference-image/#properties","text":"","title":"Properties"},{"location":"api/reference-image/#name","text":"referenceImage.name: string, read-only A name used to identify this reference image in a database .","title":"name"},{"location":"api/reference-image/#image","text":"referenceImage.image: HTMLImageElement | ImageBitmap | ImageData, read-only Image template with pixel data. Note: ImageData is acceptable since version 0.4.0.","title":"image"},{"location":"api/resolution/","text":"Resolution A Resolution is a setting defined by a string. It is mapped to a size measured in pixels according to special rules. You may use it, for example, to change the resolution of a video captured by a webcam, to adjust the resolution of a video when it is processed by a tracker, or to set the rendering resolution of a virtual scene. The table below shows examples of how resolution strings are converted to pixels: Resolution Alias 16:9 16:10 4:3 Notes \"120p\" \"xs\" 214x120 192x120 160x120 \"144p\" \"xs+\" 256x144 230x144 192x144 \"240p\" \"sm\" 426x240 384x240 320x240 SD \"288p\" \"sm+\" 512x288 460x288 384x288 \"320p\" \"md\" 568x320 512x320 426x320 \"360p\" \"md+\" 640x360 576x360 480x360 SD \"480p\" \"lg\" 854x480 768x480 640x480 SD \"600p\" \"lg+\" 1066x600 960x600 800x600 \"720p\" \"xl\" 1280x720 1152x720 960x720 HD \"768p\" 1366x768 1228x768 1024x768 \"900p\" \"xl+\" 1600x900 1440x900 1200x900 \"960p\" 1706x960 1536x960 1280x960 \"1080p\" \"xxl\" 1920x1080 1728x1080 1440x1080 Full HD","title":"Resolution"},{"location":"api/resolution/#resolution","text":"A Resolution is a setting defined by a string. It is mapped to a size measured in pixels according to special rules. You may use it, for example, to change the resolution of a video captured by a webcam, to adjust the resolution of a video when it is processed by a tracker, or to set the rendering resolution of a virtual scene. The table below shows examples of how resolution strings are converted to pixels: Resolution Alias 16:9 16:10 4:3 Notes \"120p\" \"xs\" 214x120 192x120 160x120 \"144p\" \"xs+\" 256x144 230x144 192x144 \"240p\" \"sm\" 426x240 384x240 320x240 SD \"288p\" \"sm+\" 512x288 460x288 384x288 \"320p\" \"md\" 568x320 512x320 426x320 \"360p\" \"md+\" 640x360 576x360 480x360 SD \"480p\" \"lg\" 854x480 768x480 640x480 SD \"600p\" \"lg+\" 1066x600 960x600 800x600 \"720p\" \"xl\" 1280x720 1152x720 960x720 HD \"768p\" 1366x768 1228x768 1024x768 \"900p\" \"xl+\" 1600x900 1440x900 1200x900 \"960p\" 1706x960 1536x960 1280x960 \"1080p\" \"xxl\" 1920x1080 1728x1080 1440x1080 Full HD","title":"Resolution"},{"location":"api/session/","text":"Session A central component of a WebAR experience. Read the concepts for more information. Instantiation AR.startSession AR.startSession(options: object): SpeedyPromise<Session> Start a new session. Arguments options: object . Options object with the following keys: trackers: Tracker[] . The trackers to be attached to the session. sources: Source[] . The sources of data to be linked to the session. viewport: Viewport . The viewport to be linked to the session. mode: string, optional . Either \"immersive\" or \"inline\" . Defaults to \"immersive\" . gizmos: boolean, optional . Whether or not to display the gizmos . Defaults to false . stats: boolean, optional . Whether or not to display the stats panel. It's useful during development. Defaults to false . Returns A promise that resolves to a new Session object. Properties mode session.mode: string, read-only Session mode: either \"immersive\" or \"inline\" . ended session.ended: boolean, read-only Whether or not the session has been ended. See also: end . Since: 0.3.0 time session.time: TimeManager, read-only A reference to the TimeManager of this session. gizmos session.gizmos: Gizmos, read-only A reference to the Gizmos object. viewport session.viewport: Viewport, read-only A reference to the Viewport linked to this session. trackers session.trackers: Iterable<Tracker>, read-only The trackers that are attached to the session. Since: 0.3.0 sources session.sources: Iterable<Source>, read-only The sources of data that are linked to the session. Since: 0.3.0 Methods requestAnimationFrame session.requestAnimationFrame(callback: function): SessionRequestAnimationFrameHandle Schedules a call to the callback function, which is intended to update and render the virtual scene. Your callback function must itself call session.requestAnimationFrame() again in order to continue to update and render the virtual scene. Notes session.requestAnimationFrame() is analogous to window.requestAnimationFrame() , but they are not the same! The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser. This call will be ignored and an invalid handle will be returned if the session has been ended ( since 0.3.0 ). Previously, it would raise an exception. Arguments callback: function . A function that receives two parameters: time: DOMHighResTimeStamp . Elapsed time, in milliseconds, since an arbitrary reference. This parameter is kept to mimic web standards, but its usage is discouraged. Prefer using frame.session.time.elapsed and frame.session.time.delta instead. These are especially useful for creating animations. See also: TimeManager . frame: Frame . A Frame holding the data you need to create the augmented scene. Returns A handle. Example // // This is the animation loop: // function animate ( time , frame ) { // update and render the virtual scene // ... // repeat session . requestAnimationFrame ( animate ); } // start the animation loop session . requestAnimationFrame ( animate ); cancelAnimationFrame session.cancelAnimationFrame(handle: SessionRequestAnimationFrameHandle): void Cancels an animation frame request. Arguments handle: SessionRequestAnimationFrameHandle . A handle returned by session.requestAnimationFrame() . If the handle is invalid, this method does nothing. end session.end(): SpeedyPromise<void> Ends the session. Returns A promise that resolves as soon as the session is terminated. Events A session is an AREventTarget . You can listen to the following events: end The session has ended. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); });","title":"Session"},{"location":"api/session/#session","text":"A central component of a WebAR experience. Read the concepts for more information.","title":"Session"},{"location":"api/session/#instantiation","text":"","title":"Instantiation"},{"location":"api/session/#arstartsession","text":"AR.startSession(options: object): SpeedyPromise<Session> Start a new session. Arguments options: object . Options object with the following keys: trackers: Tracker[] . The trackers to be attached to the session. sources: Source[] . The sources of data to be linked to the session. viewport: Viewport . The viewport to be linked to the session. mode: string, optional . Either \"immersive\" or \"inline\" . Defaults to \"immersive\" . gizmos: boolean, optional . Whether or not to display the gizmos . Defaults to false . stats: boolean, optional . Whether or not to display the stats panel. It's useful during development. Defaults to false . Returns A promise that resolves to a new Session object.","title":"AR.startSession"},{"location":"api/session/#properties","text":"","title":"Properties"},{"location":"api/session/#mode","text":"session.mode: string, read-only Session mode: either \"immersive\" or \"inline\" .","title":"mode"},{"location":"api/session/#ended","text":"session.ended: boolean, read-only Whether or not the session has been ended. See also: end . Since: 0.3.0","title":"ended"},{"location":"api/session/#time","text":"session.time: TimeManager, read-only A reference to the TimeManager of this session.","title":"time"},{"location":"api/session/#gizmos","text":"session.gizmos: Gizmos, read-only A reference to the Gizmos object.","title":"gizmos"},{"location":"api/session/#viewport","text":"session.viewport: Viewport, read-only A reference to the Viewport linked to this session.","title":"viewport"},{"location":"api/session/#trackers","text":"session.trackers: Iterable<Tracker>, read-only The trackers that are attached to the session. Since: 0.3.0","title":"trackers"},{"location":"api/session/#sources","text":"session.sources: Iterable<Source>, read-only The sources of data that are linked to the session. Since: 0.3.0","title":"sources"},{"location":"api/session/#methods","text":"","title":"Methods"},{"location":"api/session/#requestanimationframe","text":"session.requestAnimationFrame(callback: function): SessionRequestAnimationFrameHandle Schedules a call to the callback function, which is intended to update and render the virtual scene. Your callback function must itself call session.requestAnimationFrame() again in order to continue to update and render the virtual scene. Notes session.requestAnimationFrame() is analogous to window.requestAnimationFrame() , but they are not the same! The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser. This call will be ignored and an invalid handle will be returned if the session has been ended ( since 0.3.0 ). Previously, it would raise an exception. Arguments callback: function . A function that receives two parameters: time: DOMHighResTimeStamp . Elapsed time, in milliseconds, since an arbitrary reference. This parameter is kept to mimic web standards, but its usage is discouraged. Prefer using frame.session.time.elapsed and frame.session.time.delta instead. These are especially useful for creating animations. See also: TimeManager . frame: Frame . A Frame holding the data you need to create the augmented scene. Returns A handle. Example // // This is the animation loop: // function animate ( time , frame ) { // update and render the virtual scene // ... // repeat session . requestAnimationFrame ( animate ); } // start the animation loop session . requestAnimationFrame ( animate );","title":"requestAnimationFrame"},{"location":"api/session/#cancelanimationframe","text":"session.cancelAnimationFrame(handle: SessionRequestAnimationFrameHandle): void Cancels an animation frame request. Arguments handle: SessionRequestAnimationFrameHandle . A handle returned by session.requestAnimationFrame() . If the handle is invalid, this method does nothing.","title":"cancelAnimationFrame"},{"location":"api/session/#end","text":"session.end(): SpeedyPromise<void> Ends the session. Returns A promise that resolves as soon as the session is terminated.","title":"end"},{"location":"api/session/#events","text":"A session is an AREventTarget . You can listen to the following events:","title":"Events"},{"location":"api/session/#end_1","text":"The session has ended. Example session . addEventListener ( 'end' , event => { console . log ( 'The session has ended.' ); });","title":"end"},{"location":"api/settings/","text":"Settings Engine settings. Properties powerPreference AR.Settings.powerPreference: string Power profile. One of the following: \"default\" , \"low-power\" , \"high-performance\" . Profile Description \"default\" Default settings. \"low-power\" Reduce performance in order to reduce power consumption. \"high-performance\" High performance mode.","title":"Settings"},{"location":"api/settings/#settings","text":"Engine settings.","title":"Settings"},{"location":"api/settings/#properties","text":"","title":"Properties"},{"location":"api/settings/#powerpreference","text":"AR.Settings.powerPreference: string Power profile. One of the following: \"default\" , \"low-power\" , \"high-performance\" . Profile Description \"default\" Default settings. \"low-power\" Reduce performance in order to reduce power consumption. \"high-performance\" High performance mode.","title":"powerPreference"},{"location":"api/source/","text":"Source An abstraction representing a source of data that is meant to be linked to a session . A video is a typical source of data. Sources of data feed the trackers . Refer to the concepts for more information.","title":"Source"},{"location":"api/source/#source","text":"An abstraction representing a source of data that is meant to be linked to a session . A video is a typical source of data. Sources of data feed the trackers . Refer to the concepts for more information.","title":"Source"},{"location":"api/speedy-matrix/","text":"SpeedyMatrix Speedy includes its own fast implementation of matrices. They are used extensively in encantar.js. Properties rows matrix.rows: number, read-only Number of rows of the matrix. columns matrix.columns: number, read-only Number of columns of the matrix. Methods read matrix.read(): number[] Read the entries of the matrix in column-major format . Returns The entries of the matrix in column-major format. Example /* Suppose that you are given this matrix: [ 1 4 7 ] matrix = [ 2 5 8 ] [ 3 6 9 ] Its entries in column-major format are: [1,2,3, 4,5,6, 7,8,9] */ const entries = matrix . read (); toString matrix.toString(): string Convert to string. Returns A human-readable representation of the matrix. Example console . log ( matrix . toString ());","title":"SpeedyMatrix"},{"location":"api/speedy-matrix/#speedymatrix","text":"Speedy includes its own fast implementation of matrices. They are used extensively in encantar.js.","title":"SpeedyMatrix"},{"location":"api/speedy-matrix/#properties","text":"","title":"Properties"},{"location":"api/speedy-matrix/#rows","text":"matrix.rows: number, read-only Number of rows of the matrix.","title":"rows"},{"location":"api/speedy-matrix/#columns","text":"matrix.columns: number, read-only Number of columns of the matrix.","title":"columns"},{"location":"api/speedy-matrix/#methods","text":"","title":"Methods"},{"location":"api/speedy-matrix/#read","text":"matrix.read(): number[] Read the entries of the matrix in column-major format . Returns The entries of the matrix in column-major format. Example /* Suppose that you are given this matrix: [ 1 4 7 ] matrix = [ 2 5 8 ] [ 3 6 9 ] Its entries in column-major format are: [1,2,3, 4,5,6, 7,8,9] */ const entries = matrix . read ();","title":"read"},{"location":"api/speedy-matrix/#tostring","text":"matrix.toString(): string Convert to string. Returns A human-readable representation of the matrix. Example console . log ( matrix . toString ());","title":"toString"},{"location":"api/speedy-promise/","text":"SpeedyPromise Speedy includes its own implementation of promises. A SpeedyPromise works just like a standard promise. SpeedyPromises are designed for real-time applications. There are some subtle differences behind the scenes, but you need not be concerned with those. Instantiation Speedy.Promise new Speedy.Promise(executor: function): SpeedyPromise Creates a new SpeedyPromise . This works just like the constructor of a standard promise. Arguments executor: function . A function that takes two functions as arguments: resolve: function . To be called when the promise is resolved. reject: function . To be called when the promise is rejected. Returns A new SpeedyPromise . Example function sleep ( ms ) { return new Speedy . Promise (( resolve , reject ) => { if ( ms >= 0 ) setTimeout ( resolve , ms ); else reject ( new Error ( 'Invalid time' )); }); } sleep ( 2000 ). then (() => { console . log ( '2 seconds have passed' ); }). catch ( error => { console . error ( error . message ); }). finally (() => { console . log ( 'Done!' ); });","title":"SpeedyPromise"},{"location":"api/speedy-promise/#speedypromise","text":"Speedy includes its own implementation of promises. A SpeedyPromise works just like a standard promise. SpeedyPromises are designed for real-time applications. There are some subtle differences behind the scenes, but you need not be concerned with those.","title":"SpeedyPromise"},{"location":"api/speedy-promise/#instantiation","text":"","title":"Instantiation"},{"location":"api/speedy-promise/#speedypromise_1","text":"new Speedy.Promise(executor: function): SpeedyPromise Creates a new SpeedyPromise . This works just like the constructor of a standard promise. Arguments executor: function . A function that takes two functions as arguments: resolve: function . To be called when the promise is resolved. reject: function . To be called when the promise is rejected. Returns A new SpeedyPromise . Example function sleep ( ms ) { return new Speedy . Promise (( resolve , reject ) => { if ( ms >= 0 ) setTimeout ( resolve , ms ); else reject ( new Error ( 'Invalid time' )); }); } sleep ( 2000 ). then (() => { console . log ( '2 seconds have passed' ); }). catch ( error => { console . error ( error . message ); }). finally (() => { console . log ( 'Done!' ); });","title":"Speedy.Promise"},{"location":"api/speedy-size/","text":"SpeedySize Represents the size of a rectangle. Properties width size.width: number, read-only Width of the rectangle. height size.height: number, read-only Height of the rectangle.","title":"SpeedySize"},{"location":"api/speedy-size/#speedysize","text":"Represents the size of a rectangle.","title":"SpeedySize"},{"location":"api/speedy-size/#properties","text":"","title":"Properties"},{"location":"api/speedy-size/#width","text":"size.width: number, read-only Width of the rectangle.","title":"width"},{"location":"api/speedy-size/#height","text":"size.height: number, read-only Height of the rectangle.","title":"height"},{"location":"api/speedy/","text":"Speedy encantar.js is built using Speedy Vision , a GPU-accelerated Computer Vision library which is another project of mine. Many features provided by Speedy Vision are very useful (e.g., matrices). I have decided to include some of them in parts of the encantar.js API for convenience. In this documentation, I provide a quick reference of some of the classes used in Speedy Vision. This reference is minimal. For a more complete reference, please visit the website of the project . Properties Speedy Speedy: Speedy, read-only The Speedy namespace is provided in global scope. It's also available as AR.Speedy .","title":"Speedy"},{"location":"api/speedy/#speedy","text":"encantar.js is built using Speedy Vision , a GPU-accelerated Computer Vision library which is another project of mine. Many features provided by Speedy Vision are very useful (e.g., matrices). I have decided to include some of them in parts of the encantar.js API for convenience. In this documentation, I provide a quick reference of some of the classes used in Speedy Vision. This reference is minimal. For a more complete reference, please visit the website of the project .","title":"Speedy"},{"location":"api/speedy/#properties","text":"","title":"Properties"},{"location":"api/speedy/#speedy_1","text":"Speedy: Speedy, read-only The Speedy namespace is provided in global scope. It's also available as AR.Speedy .","title":"Speedy"},{"location":"api/time-manager/","text":"TimeManager Time-related utilities. They are useful for animating virtual scenes. Properties elapsed time.elapsed: number, read-only Elapsed time since the start of the session, measured at the beginning of the current animation frame and given in seconds. delta time.delta: number, read-only Elapsed time between the current and the previous animation frame, given in seconds. Use this value to produce animations that are independent of the framerate. scale time.scale: number Time scale. Use it to accelerate, slowdown or pause the passage of time. Defaults to 1. unscaled time.unscaled: number, read-only Time scale independent seconds since the start of the session, measured at the beginning of the current animation frame.","title":"Time"},{"location":"api/time-manager/#timemanager","text":"Time-related utilities. They are useful for animating virtual scenes.","title":"TimeManager"},{"location":"api/time-manager/#properties","text":"","title":"Properties"},{"location":"api/time-manager/#elapsed","text":"time.elapsed: number, read-only Elapsed time since the start of the session, measured at the beginning of the current animation frame and given in seconds.","title":"elapsed"},{"location":"api/time-manager/#delta","text":"time.delta: number, read-only Elapsed time between the current and the previous animation frame, given in seconds. Use this value to produce animations that are independent of the framerate.","title":"delta"},{"location":"api/time-manager/#scale","text":"time.scale: number Time scale. Use it to accelerate, slowdown or pause the passage of time. Defaults to 1.","title":"scale"},{"location":"api/time-manager/#unscaled","text":"time.unscaled: number, read-only Time scale independent seconds since the start of the session, measured at the beginning of the current animation frame.","title":"unscaled"},{"location":"api/trackable-image/","text":"TrackableImage A trackable that represents an image target tracked by an Image Tracker . Properties pose trackable.pose: Pose, read-only The pose of the trackable. referenceImage trackable.referenceImage: ReferenceImage, read-only The reference image associated with the trackable.","title":"TrackableImage"},{"location":"api/trackable-image/#trackableimage","text":"A trackable that represents an image target tracked by an Image Tracker .","title":"TrackableImage"},{"location":"api/trackable-image/#properties","text":"","title":"Properties"},{"location":"api/trackable-image/#pose","text":"trackable.pose: Pose, read-only The pose of the trackable.","title":"pose"},{"location":"api/trackable-image/#referenceimage","text":"trackable.referenceImage: ReferenceImage, read-only The reference image associated with the trackable.","title":"referenceImage"},{"location":"api/trackable-pointer/","text":"TrackablePointer A trackable that represents a pointer tracked by a PointerTracker . A pointer is an abstraction that represents an instance of user input that targets one or more coordinates on a screen. For example, each point of contact between fingers and a multitouch screen generate a pointer. Devices such as a mouse and a pen/stylus also generate pointers. Pointers are positioned in the viewport . Their positions are given in normalized units, which range from -1 to +1. The center of the viewport is at (0,0). The top right corner is at (1,1). The bottom left corner is at (-1,-1). Since: 0.4.0 Properties id pointer.id: number, read-only A unique identifier assigned to this pointer. phase pointer.phase: string, read-only The phase of the pointer. It's one of the following strings: \"began\" : the tracking began in this frame (e.g., a finger has just touched the screen) \"stationary\" : the user did not move the pointer in this frame \"moved\" : the user moved the pointer in this frame \"ended\" : the tracking ended in this frame (e.g., a finger has just been lifted from the screen) \"canceled\" : the tracking was canceled in this frame (e.g., the page has just lost focus) position pointer.position: Vector2, read-only The current position of the pointer, given in normalized units. See also: Viewer.raycast , Viewport.convertToPixels . initialPosition pointer.initialPosition: Vector2, read-only The position of the pointer when its tracking began. deltaPosition pointer.deltaPosition: Vector2, read-only The difference between the position of the pointer in this and in the previous frame. velocity pointer.velocity: Vector2, read-only The current velocity of the pointer, given in normalized units per second. You can get the speed of motion by calculating the magnitude of this vector. elapsedTime pointer.elapsedTime: number, read-only The elapsed time, in seconds, since the tracking of this pointer began. isPrimary pointer.isPrimary: boolean, read-only Whether or not this is the primary pointer among all pointers of this type . A typical primary pointer is that of a finger that touches the screen when no other fingers are touching it. kind pointer.kind: string, read-only The type of device that originated this pointer. Typically \"touch\" , \"mouse\" or \"pen\" .","title":"TrackablePointer"},{"location":"api/trackable-pointer/#trackablepointer","text":"A trackable that represents a pointer tracked by a PointerTracker . A pointer is an abstraction that represents an instance of user input that targets one or more coordinates on a screen. For example, each point of contact between fingers and a multitouch screen generate a pointer. Devices such as a mouse and a pen/stylus also generate pointers. Pointers are positioned in the viewport . Their positions are given in normalized units, which range from -1 to +1. The center of the viewport is at (0,0). The top right corner is at (1,1). The bottom left corner is at (-1,-1). Since: 0.4.0","title":"TrackablePointer"},{"location":"api/trackable-pointer/#properties","text":"","title":"Properties"},{"location":"api/trackable-pointer/#id","text":"pointer.id: number, read-only A unique identifier assigned to this pointer.","title":"id"},{"location":"api/trackable-pointer/#phase","text":"pointer.phase: string, read-only The phase of the pointer. It's one of the following strings: \"began\" : the tracking began in this frame (e.g., a finger has just touched the screen) \"stationary\" : the user did not move the pointer in this frame \"moved\" : the user moved the pointer in this frame \"ended\" : the tracking ended in this frame (e.g., a finger has just been lifted from the screen) \"canceled\" : the tracking was canceled in this frame (e.g., the page has just lost focus)","title":"phase"},{"location":"api/trackable-pointer/#position","text":"pointer.position: Vector2, read-only The current position of the pointer, given in normalized units. See also: Viewer.raycast , Viewport.convertToPixels .","title":"position"},{"location":"api/trackable-pointer/#initialposition","text":"pointer.initialPosition: Vector2, read-only The position of the pointer when its tracking began.","title":"initialPosition"},{"location":"api/trackable-pointer/#deltaposition","text":"pointer.deltaPosition: Vector2, read-only The difference between the position of the pointer in this and in the previous frame.","title":"deltaPosition"},{"location":"api/trackable-pointer/#velocity","text":"pointer.velocity: Vector2, read-only The current velocity of the pointer, given in normalized units per second. You can get the speed of motion by calculating the magnitude of this vector.","title":"velocity"},{"location":"api/trackable-pointer/#elapsedtime","text":"pointer.elapsedTime: number, read-only The elapsed time, in seconds, since the tracking of this pointer began.","title":"elapsedTime"},{"location":"api/trackable-pointer/#isprimary","text":"pointer.isPrimary: boolean, read-only Whether or not this is the primary pointer among all pointers of this type . A typical primary pointer is that of a finger that touches the screen when no other fingers are touching it.","title":"isPrimary"},{"location":"api/trackable-pointer/#kind","text":"pointer.kind: string, read-only The type of device that originated this pointer. Typically \"touch\" , \"mouse\" or \"pen\" .","title":"kind"},{"location":"api/trackable/","text":"Trackable A Trackable is an interface that represents something that is tracked by a Tracker .","title":"Trackable"},{"location":"api/trackable/#trackable","text":"A Trackable is an interface that represents something that is tracked by a Tracker .","title":"Trackable"},{"location":"api/tracker-result/","text":"TrackerResult An interface that represents the result generated by a tracker at a specific time. It is part of a frame . Properties tracker result.tracker: Tracker, read-only A reference to the tracker that generated this result. trackables result.trackables: Trackable[], read-only An array of zero or more trackables .","title":"TrackerResult"},{"location":"api/tracker-result/#trackerresult","text":"An interface that represents the result generated by a tracker at a specific time. It is part of a frame .","title":"TrackerResult"},{"location":"api/tracker-result/#properties","text":"","title":"Properties"},{"location":"api/tracker-result/#tracker","text":"result.tracker: Tracker, read-only A reference to the tracker that generated this result.","title":"tracker"},{"location":"api/tracker-result/#trackables","text":"result.trackables: Trackable[], read-only An array of zero or more trackables .","title":"trackables"},{"location":"api/tracker/","text":"Tracker An interface that represents a generic tracker. Trackers analyze input data in some way and are meant to be attached to a session . Refer to the concepts for more information. An Image Tracker is an implementation of a tracker. Properties type tracker.type: string, read-only A string representing the type of the tracker.","title":"Tracker"},{"location":"api/tracker/#tracker","text":"An interface that represents a generic tracker. Trackers analyze input data in some way and are meant to be attached to a session . Refer to the concepts for more information. An Image Tracker is an implementation of a tracker.","title":"Tracker"},{"location":"api/tracker/#properties","text":"","title":"Properties"},{"location":"api/tracker/#type","text":"tracker.type: string, read-only A string representing the type of the tracker.","title":"type"},{"location":"api/transform/","text":"Transform A Transform represents a position, a rotation and a scale in 3D space. Properties matrix transform.matrix: SpeedyMatrix, read-only A 4x4 matrix encoding the transform. inverse transform.inverse: Transform, read-only The inverse transform. position transform.position: Vector3, read-only The 3D position encoded by the transform. Since: 0.4.0 orientation transform.orientation: Quaternion, read-only A unit quaternion describing the rotational component of the transform. Since: 0.4.0 scale transform.scale: Vector3, read-only The scale encoded by the transform. Since: 0.4.0 right transform.right: Vector3, read-only The unit right vector of the local space. Since: 0.4.0 up transform.up: Vector3, read-only The unit up vector of the local space. Since: 0.4.0 forward transform.forward: Vector3, read-only The unit forward vector of the local space. Since: 0.4.0","title":"Transform"},{"location":"api/transform/#transform","text":"A Transform represents a position, a rotation and a scale in 3D space.","title":"Transform"},{"location":"api/transform/#properties","text":"","title":"Properties"},{"location":"api/transform/#matrix","text":"transform.matrix: SpeedyMatrix, read-only A 4x4 matrix encoding the transform.","title":"matrix"},{"location":"api/transform/#inverse","text":"transform.inverse: Transform, read-only The inverse transform.","title":"inverse"},{"location":"api/transform/#position","text":"transform.position: Vector3, read-only The 3D position encoded by the transform. Since: 0.4.0","title":"position"},{"location":"api/transform/#orientation","text":"transform.orientation: Quaternion, read-only A unit quaternion describing the rotational component of the transform. Since: 0.4.0","title":"orientation"},{"location":"api/transform/#scale","text":"transform.scale: Vector3, read-only The scale encoded by the transform. Since: 0.4.0","title":"scale"},{"location":"api/transform/#right","text":"transform.right: Vector3, read-only The unit right vector of the local space. Since: 0.4.0","title":"right"},{"location":"api/transform/#up","text":"transform.up: Vector3, read-only The unit up vector of the local space. Since: 0.4.0","title":"up"},{"location":"api/transform/#forward","text":"transform.forward: Vector3, read-only The unit forward vector of the local space. Since: 0.4.0","title":"forward"},{"location":"api/vector2/","text":"Vector2 A vector in 2D space. Since: 0.4.0 Instantiation AR.Vector2 AR.Vector2(x: number, y: number): Vector2 Create a new vector with the provided coordinates. Arguments x: number . x coordinate. y: number . y coordinate. Returns A new vector. Example const zero = AR . Vector2 ( 0 , 0 ); Properties x vector.x: number, read-only The x coordinate of the vector. y vector.y: number, read-only The y coordinate of the vector. Methods length vector.length(): number Compute the magnitude of the vector. Returns The magnitude of the vector. dot vector.dot(v: Vector2): number Compute the dot product of this and v . Arguments v: Vector2 . A vector. Returns The dot product of the vectors. distanceTo vector.distanceTo(v: Vector2): number Compute the distance between points this and v . Arguments v: Vector2 . A vector / point. Returns The distance between the points. directionTo vector.directionTo(v: Vector2): Vector2 Compute a unit vector pointing to v from this . Arguments v: Vector2 . A vector. Returns A new unit vector pointing to v from this . equals vector.equals(v: Vector2): boolean Check if this and v have the same coordinates. Arguments v: Vector2 . A vector. Returns true if this and v have the same coordinates. toString vector.toString(): string Generate a string representation of the vector. Returns A string representation of the vector.","title":"Vector2"},{"location":"api/vector2/#vector2","text":"A vector in 2D space. Since: 0.4.0","title":"Vector2"},{"location":"api/vector2/#instantiation","text":"","title":"Instantiation"},{"location":"api/vector2/#arvector2","text":"AR.Vector2(x: number, y: number): Vector2 Create a new vector with the provided coordinates. Arguments x: number . x coordinate. y: number . y coordinate. Returns A new vector. Example const zero = AR . Vector2 ( 0 , 0 );","title":"AR.Vector2"},{"location":"api/vector2/#properties","text":"","title":"Properties"},{"location":"api/vector2/#x","text":"vector.x: number, read-only The x coordinate of the vector.","title":"x"},{"location":"api/vector2/#y","text":"vector.y: number, read-only The y coordinate of the vector.","title":"y"},{"location":"api/vector2/#methods","text":"","title":"Methods"},{"location":"api/vector2/#length","text":"vector.length(): number Compute the magnitude of the vector. Returns The magnitude of the vector.","title":"length"},{"location":"api/vector2/#dot","text":"vector.dot(v: Vector2): number Compute the dot product of this and v . Arguments v: Vector2 . A vector. Returns The dot product of the vectors.","title":"dot"},{"location":"api/vector2/#distanceto","text":"vector.distanceTo(v: Vector2): number Compute the distance between points this and v . Arguments v: Vector2 . A vector / point. Returns The distance between the points.","title":"distanceTo"},{"location":"api/vector2/#directionto","text":"vector.directionTo(v: Vector2): Vector2 Compute a unit vector pointing to v from this . Arguments v: Vector2 . A vector. Returns A new unit vector pointing to v from this .","title":"directionTo"},{"location":"api/vector2/#equals","text":"vector.equals(v: Vector2): boolean Check if this and v have the same coordinates. Arguments v: Vector2 . A vector. Returns true if this and v have the same coordinates.","title":"equals"},{"location":"api/vector2/#tostring","text":"vector.toString(): string Generate a string representation of the vector. Returns A string representation of the vector.","title":"toString"},{"location":"api/vector3/","text":"Vector3 A vector in 3D space. Since: 0.4.0 Instantiation AR.Vector3 AR.Vector3(x: number, y: number, z: number): Vector3 Create a new vector with the provided coordinates. Arguments x: number . x coordinate. y: number . y coordinate. z: number . z coordinate. Returns A new vector. Example const zero = AR . Vector3 ( 0 , 0 , 0 ); Properties x vector.x: number, read-only The x coordinate of the vector. y vector.y: number, read-only The y coordinate of the vector. z vector.z: number, read-only The z coordinate of the vector. Methods length vector.length(): number Compute the magnitude of the vector. Returns The magnitude of the vector. dot vector.dot(v: Vector3): number Compute the dot product of this and v . Arguments v: Vector3 . A vector. Returns The dot product of the vectors. distanceTo vector.distanceTo(v: Vector3): number Compute the distance between points this and v . Arguments v: Vector3 . A vector / point. Returns The distance between the points. directionTo vector.directionTo(v: Vector3): Vector3 Compute a unit vector pointing to v from this . Arguments v: Vector3 . A vector. Returns A new unit vector pointing to v from this . cross vector.cross(v: Vector3): Vector3 Compute the cross product of this and v . Arguments v: Vector3 . A vector. Returns The cross product this x v . equals vector.equals(v: Vector3): boolean Check if this and v have the same coordinates. Arguments v: Vector3 . A vector. Returns true if this and v have the same coordinates. toString vector.toString(): string Generate a string representation of the vector. Returns A string representation of the vector.","title":"Vector3"},{"location":"api/vector3/#vector3","text":"A vector in 3D space. Since: 0.4.0","title":"Vector3"},{"location":"api/vector3/#instantiation","text":"","title":"Instantiation"},{"location":"api/vector3/#arvector3","text":"AR.Vector3(x: number, y: number, z: number): Vector3 Create a new vector with the provided coordinates. Arguments x: number . x coordinate. y: number . y coordinate. z: number . z coordinate. Returns A new vector. Example const zero = AR . Vector3 ( 0 , 0 , 0 );","title":"AR.Vector3"},{"location":"api/vector3/#properties","text":"","title":"Properties"},{"location":"api/vector3/#x","text":"vector.x: number, read-only The x coordinate of the vector.","title":"x"},{"location":"api/vector3/#y","text":"vector.y: number, read-only The y coordinate of the vector.","title":"y"},{"location":"api/vector3/#z","text":"vector.z: number, read-only The z coordinate of the vector.","title":"z"},{"location":"api/vector3/#methods","text":"","title":"Methods"},{"location":"api/vector3/#length","text":"vector.length(): number Compute the magnitude of the vector. Returns The magnitude of the vector.","title":"length"},{"location":"api/vector3/#dot","text":"vector.dot(v: Vector3): number Compute the dot product of this and v . Arguments v: Vector3 . A vector. Returns The dot product of the vectors.","title":"dot"},{"location":"api/vector3/#distanceto","text":"vector.distanceTo(v: Vector3): number Compute the distance between points this and v . Arguments v: Vector3 . A vector / point. Returns The distance between the points.","title":"distanceTo"},{"location":"api/vector3/#directionto","text":"vector.directionTo(v: Vector3): Vector3 Compute a unit vector pointing to v from this . Arguments v: Vector3 . A vector. Returns A new unit vector pointing to v from this .","title":"directionTo"},{"location":"api/vector3/#cross","text":"vector.cross(v: Vector3): Vector3 Compute the cross product of this and v . Arguments v: Vector3 . A vector. Returns The cross product this x v .","title":"cross"},{"location":"api/vector3/#equals","text":"vector.equals(v: Vector3): boolean Check if this and v have the same coordinates. Arguments v: Vector3 . A vector. Returns true if this and v have the same coordinates.","title":"equals"},{"location":"api/vector3/#tostring","text":"vector.toString(): string Generate a string representation of the vector. Returns A string representation of the vector.","title":"toString"},{"location":"api/video-source/","text":"VideoSource A source of data linked to a <video> element. Instantiation AR.Source.Video AR.Source.Video(video: HTMLVideoElement): VideoSource Create a new source of data linked to the provided video . Arguments video: HTMLVideoElement . A <video> element. Returns A new source of data.","title":"VideoSource"},{"location":"api/video-source/#videosource","text":"A source of data linked to a <video> element.","title":"VideoSource"},{"location":"api/video-source/#instantiation","text":"","title":"Instantiation"},{"location":"api/video-source/#arsourcevideo","text":"AR.Source.Video(video: HTMLVideoElement): VideoSource Create a new source of data linked to the provided video . Arguments video: HTMLVideoElement . A <video> element. Returns A new source of data.","title":"AR.Source.Video"},{"location":"api/view/","text":"View An interface that represents a view of the 3D world at a moment in time. A PerspectiveView is an implementation of a View. Properties projectionMatrix view.projectionMatrix: SpeedyMatrix, read-only A 4x4 matrix that projects viewer space onto clip space.","title":"View"},{"location":"api/view/#view","text":"An interface that represents a view of the 3D world at a moment in time. A PerspectiveView is an implementation of a View.","title":"View"},{"location":"api/view/#properties","text":"","title":"Properties"},{"location":"api/view/#projectionmatrix","text":"view.projectionMatrix: SpeedyMatrix, read-only A 4x4 matrix that projects viewer space onto clip space.","title":"projectionMatrix"},{"location":"api/viewer-pose/","text":"ViewerPose The pose of a Viewer . Properties viewMatrix pose.viewMatrix: SpeedyMatrix, read-only A 4x4 matrix that converts points from world space to viewer space.","title":"ViewerPose"},{"location":"api/viewer-pose/#viewerpose","text":"The pose of a Viewer .","title":"ViewerPose"},{"location":"api/viewer-pose/#properties","text":"","title":"Properties"},{"location":"api/viewer-pose/#viewmatrix","text":"pose.viewMatrix: SpeedyMatrix, read-only A 4x4 matrix that converts points from world space to viewer space.","title":"viewMatrix"},{"location":"api/viewer/","text":"Viewer A virtual camera in 3D world space. Properties pose viewer.pose: ViewerPose, read-only The pose of the viewer. view viewer.view: View, read-only A view of the viewer (monoscopic rendering). Methods convertToViewerSpace viewer.convertToViewerSpace(pose: Pose): Pose Convert a pose from world space to viewer space. Arguments pose: Pose . A pose in world space. Returns The input pose converted to viewer space. Example const modelViewMatrix = viewer . convertToViewerSpace ( pose ). transform . matrix ; raycast viewer.raycast(position: Vector2): Ray Cast a ray from a point in the image space associated with this viewer. Since: 0.4.0 Arguments position: Vector2 . A point in image space, given in normalized units . Returns A ray in world space that corresponds to the given point. forwardRay viewer.forwardRay(): Ray Compute a ray in the forward direction from the viewer. Since: 0.4.0 Returns A new ray in world space.","title":"Viewer"},{"location":"api/viewer/#viewer","text":"A virtual camera in 3D world space.","title":"Viewer"},{"location":"api/viewer/#properties","text":"","title":"Properties"},{"location":"api/viewer/#pose","text":"viewer.pose: ViewerPose, read-only The pose of the viewer.","title":"pose"},{"location":"api/viewer/#view","text":"viewer.view: View, read-only A view of the viewer (monoscopic rendering).","title":"view"},{"location":"api/viewer/#methods","text":"","title":"Methods"},{"location":"api/viewer/#converttoviewerspace","text":"viewer.convertToViewerSpace(pose: Pose): Pose Convert a pose from world space to viewer space. Arguments pose: Pose . A pose in world space. Returns The input pose converted to viewer space. Example const modelViewMatrix = viewer . convertToViewerSpace ( pose ). transform . matrix ;","title":"convertToViewerSpace"},{"location":"api/viewer/#raycast","text":"viewer.raycast(position: Vector2): Ray Cast a ray from a point in the image space associated with this viewer. Since: 0.4.0 Arguments position: Vector2 . A point in image space, given in normalized units . Returns A ray in world space that corresponds to the given point.","title":"raycast"},{"location":"api/viewer/#forwardray","text":"viewer.forwardRay(): Ray Compute a ray in the forward direction from the viewer. Since: 0.4.0 Returns A new ray in world space.","title":"forwardRay"},{"location":"api/viewport/","text":"Viewport The viewport is the area of the web page in which the augmented scene is displayed. Instantiation AR.Viewport AR.Viewport(settings: object): Viewport Create a new viewport with the specified settings . Arguments settings: object . An object with the following keys: container: HTMLDivElement . A <div> that will contain the augmented scene. hudContainer: HTMLDivElement, optional . An overlay that will be displayed in front of the augmented scene. It must be a direct child of container in the DOM tree. resolution: Resolution, optional . The resolution of the virtual scene. canvas: HTMLCanvasElement, optional . An existing canvas on which the virtual scene will be drawn. The engine automatically creates a canvas. You should only specify an existing canvas if you must. Experimental. style: string, optional . The viewport style . Since: 0.3.0 fullscreenUI: boolean, optional . Whether or not to include, as a convenience, the built-in fullscreen button on platforms in which the fullscreen mode is available . Defaults to true . Since: 0.3.0 Returns A new viewport. Example const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), resolution : 'lg' }); Properties container viewport.container: HTMLDivElement, read-only The container of the viewport. hud viewport.hud: HUD, read-only The HUD . resolution viewport.resolution: Resolution, read-only The resolution of the virtual scene. virtualSize viewport.virtualSize: SpeedySize, read-only The size in pixels that matches the resolution of the virtual scene. canvas viewport.canvas: HTMLCanvasElement, read-only The <canvas> on which the virtual scene is drawn. style viewport.style: string, read-only The style determines the way the viewport appears on the screen. Different styles are applicable to different session modes . The following are valid styles: \"best-fit\" : an immersive viewport that is scaled in a way that covers the page while preserving the aspect ratio of the augmented scene. \"stretch\" : an immersive viewport that is scaled in a way that covers the entire page, stretching the augmented scene if necessary. \"inline\" : an inline viewport that follows the typical flow of a web page. The default style is \"best-fit\" in immersive mode, or \"inline\" in inline mode. Since: 0.3.0 fullscreen viewport.fullscreen: boolean, read-only Whether or not the viewport container is being displayed in fullscreen mode. See also: requestFullscreen . Since: 0.3.0 fullscreenAvailable viewport.fullscreenAvailable: boolean, read-only Used to check the availability of the fullscreen mode on the current platform and page. Since: 0.3.0 Methods requestFullscreen viewport.requestFullscreen(): SpeedyPromise<void> Make a request to the user agent so that the viewport container is displayed in fullscreen mode. The user must interact with the page (e.g., tap on a button) in order to comply with browser policies , otherwise the request will not succeed. iPhone support At the time of this writing, the fullscreen mode is not supported on iPhone . An alternative way to create a fullscreen experience is to set the viewport style to \"stretch\" in a web app . Since: 0.3.0 Returns A promise that is resolved when the fullscreen mode is activated, or rejected on error. Example function toggleFullscreen () { if ( ! viewport . fullscreen ) { viewport . requestFullscreen (). catch ( err => { alert ( `Can't enable fullscreen mode. ` + err . toString ()); }); } else { viewport . exitFullscreen (); } } // require user interaction button . addEventListener ( 'click' , toggleFullscreen ); exitFullscreen viewport.exitFullscreen(): SpeedyPromise<void> Exit fullscreen mode. Since: 0.3.0 Returns A promise that is resolved once the fullscreen mode is no longer active, or rejected on error. The promise will be rejected if the method is called when not in fullscreen mode. convertToPixels viewport.convertToPixels(position: Vector2): Vector2 Convert a position given in normalized units to a corresponding pixel position in canvas space. Normalized units range from -1 to +1. The center of the canvas is at (0,0). The top right corner is at (1,1). The bottom left corner is at (-1,-1). Since: 0.4.0 Arguments position: Vector2 . A position in normalized units. Returns An equivalent pixel position in canvas space. Events A viewport is an AREventTarget . You can listen to the following events: resize The viewport has been resized. This will happen when the user resizes the window of the web browser or when the mobile device is flipped. Example viewport . addEventListener ( 'resize' , () => { console . log ( 'The viewport has been resized.' ); }); fullscreenchange The viewport has been switched to or out of fullscreen mode. Since: 0.3.0 Example viewport . addEventListener ( 'fullscreenchange' , () => { if ( viewport . fullscreen ) console . log ( 'Switched to fullscreen mode' ); else console . log ( 'Switched out of fullscreen mode' ); });","title":"Viewport"},{"location":"api/viewport/#viewport","text":"The viewport is the area of the web page in which the augmented scene is displayed.","title":"Viewport"},{"location":"api/viewport/#instantiation","text":"","title":"Instantiation"},{"location":"api/viewport/#arviewport","text":"AR.Viewport(settings: object): Viewport Create a new viewport with the specified settings . Arguments settings: object . An object with the following keys: container: HTMLDivElement . A <div> that will contain the augmented scene. hudContainer: HTMLDivElement, optional . An overlay that will be displayed in front of the augmented scene. It must be a direct child of container in the DOM tree. resolution: Resolution, optional . The resolution of the virtual scene. canvas: HTMLCanvasElement, optional . An existing canvas on which the virtual scene will be drawn. The engine automatically creates a canvas. You should only specify an existing canvas if you must. Experimental. style: string, optional . The viewport style . Since: 0.3.0 fullscreenUI: boolean, optional . Whether or not to include, as a convenience, the built-in fullscreen button on platforms in which the fullscreen mode is available . Defaults to true . Since: 0.3.0 Returns A new viewport. Example const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), resolution : 'lg' });","title":"AR.Viewport"},{"location":"api/viewport/#properties","text":"","title":"Properties"},{"location":"api/viewport/#container","text":"viewport.container: HTMLDivElement, read-only The container of the viewport.","title":"container"},{"location":"api/viewport/#hud","text":"viewport.hud: HUD, read-only The HUD .","title":"hud"},{"location":"api/viewport/#resolution","text":"viewport.resolution: Resolution, read-only The resolution of the virtual scene.","title":"resolution"},{"location":"api/viewport/#virtualsize","text":"viewport.virtualSize: SpeedySize, read-only The size in pixels that matches the resolution of the virtual scene.","title":"virtualSize"},{"location":"api/viewport/#canvas","text":"viewport.canvas: HTMLCanvasElement, read-only The <canvas> on which the virtual scene is drawn.","title":"canvas"},{"location":"api/viewport/#style","text":"viewport.style: string, read-only The style determines the way the viewport appears on the screen. Different styles are applicable to different session modes . The following are valid styles: \"best-fit\" : an immersive viewport that is scaled in a way that covers the page while preserving the aspect ratio of the augmented scene. \"stretch\" : an immersive viewport that is scaled in a way that covers the entire page, stretching the augmented scene if necessary. \"inline\" : an inline viewport that follows the typical flow of a web page. The default style is \"best-fit\" in immersive mode, or \"inline\" in inline mode. Since: 0.3.0","title":"style"},{"location":"api/viewport/#fullscreen","text":"viewport.fullscreen: boolean, read-only Whether or not the viewport container is being displayed in fullscreen mode. See also: requestFullscreen . Since: 0.3.0","title":"fullscreen"},{"location":"api/viewport/#fullscreenavailable","text":"viewport.fullscreenAvailable: boolean, read-only Used to check the availability of the fullscreen mode on the current platform and page. Since: 0.3.0","title":"fullscreenAvailable"},{"location":"api/viewport/#methods","text":"","title":"Methods"},{"location":"api/viewport/#requestfullscreen","text":"viewport.requestFullscreen(): SpeedyPromise<void> Make a request to the user agent so that the viewport container is displayed in fullscreen mode. The user must interact with the page (e.g., tap on a button) in order to comply with browser policies , otherwise the request will not succeed. iPhone support At the time of this writing, the fullscreen mode is not supported on iPhone . An alternative way to create a fullscreen experience is to set the viewport style to \"stretch\" in a web app . Since: 0.3.0 Returns A promise that is resolved when the fullscreen mode is activated, or rejected on error. Example function toggleFullscreen () { if ( ! viewport . fullscreen ) { viewport . requestFullscreen (). catch ( err => { alert ( `Can't enable fullscreen mode. ` + err . toString ()); }); } else { viewport . exitFullscreen (); } } // require user interaction button . addEventListener ( 'click' , toggleFullscreen );","title":"requestFullscreen"},{"location":"api/viewport/#exitfullscreen","text":"viewport.exitFullscreen(): SpeedyPromise<void> Exit fullscreen mode. Since: 0.3.0 Returns A promise that is resolved once the fullscreen mode is no longer active, or rejected on error. The promise will be rejected if the method is called when not in fullscreen mode.","title":"exitFullscreen"},{"location":"api/viewport/#converttopixels","text":"viewport.convertToPixels(position: Vector2): Vector2 Convert a position given in normalized units to a corresponding pixel position in canvas space. Normalized units range from -1 to +1. The center of the canvas is at (0,0). The top right corner is at (1,1). The bottom left corner is at (-1,-1). Since: 0.4.0 Arguments position: Vector2 . A position in normalized units. Returns An equivalent pixel position in canvas space.","title":"convertToPixels"},{"location":"api/viewport/#events","text":"A viewport is an AREventTarget . You can listen to the following events:","title":"Events"},{"location":"api/viewport/#resize","text":"The viewport has been resized. This will happen when the user resizes the window of the web browser or when the mobile device is flipped. Example viewport . addEventListener ( 'resize' , () => { console . log ( 'The viewport has been resized.' ); });","title":"resize"},{"location":"api/viewport/#fullscreenchange","text":"The viewport has been switched to or out of fullscreen mode. Since: 0.3.0 Example viewport . addEventListener ( 'fullscreenchange' , () => { if ( viewport . fullscreen ) console . log ( 'Switched to fullscreen mode' ); else console . log ( 'Switched out of fullscreen mode' ); });","title":"fullscreenchange"},{"location":"getting-started/","text":"Welcome to encantar.js! encantar.js is the Augmented Reality engine that will enchant you! Demos Download Learn Sponsor Features Image tracking : track detailed images such as book covers, cartoons and photos. No need of manual training! Pointer tracking : create interactive experiences based on touch and mouse input with an easy-to-use API. Plugins : use encantar.js with the 3D framework of your choice, including A-Frame, Babylon.js, Three.js and more! Key points AR everywhere : it runs on Android, on iOS, and even on Desktop computers! Only a modern web browser is required. Easy to use : just load a static HTML page! No need of heavy app downloads, API keys, subscriptions or server-side stuff. Device-agnostic : no need of AR-capable devices! encantar.js is AR built from scratch with computer vision! Fast & powerful : encantar.js is GPU-accelerated. It uses WebGL2 and WebAssembly for turbocharged performance. Open source : personal, professional and educational uses are allowed. Browser compatibility encantar.js is compatible with all major web browsers: Chrome Edge Firefox Opera Safari* * use Safari 15.2 or later. encantar.js requires WebGL2 and WebAssembly, which are widely supported. About encantar.js is developed by Alexandre Martins and released under the LGPL . It is based on Speedy Vision .","title":"Welcome"},{"location":"getting-started/#welcome-to-encantarjs","text":"encantar.js is the Augmented Reality engine that will enchant you! Demos Download Learn Sponsor","title":"Welcome to encantar.js!"},{"location":"getting-started/#features","text":"Image tracking : track detailed images such as book covers, cartoons and photos. No need of manual training! Pointer tracking : create interactive experiences based on touch and mouse input with an easy-to-use API. Plugins : use encantar.js with the 3D framework of your choice, including A-Frame, Babylon.js, Three.js and more!","title":"Features"},{"location":"getting-started/#key-points","text":"AR everywhere : it runs on Android, on iOS, and even on Desktop computers! Only a modern web browser is required. Easy to use : just load a static HTML page! No need of heavy app downloads, API keys, subscriptions or server-side stuff. Device-agnostic : no need of AR-capable devices! encantar.js is AR built from scratch with computer vision! Fast & powerful : encantar.js is GPU-accelerated. It uses WebGL2 and WebAssembly for turbocharged performance. Open source : personal, professional and educational uses are allowed.","title":"Key points"},{"location":"getting-started/#browser-compatibility","text":"encantar.js is compatible with all major web browsers: Chrome Edge Firefox Opera Safari* * use Safari 15.2 or later. encantar.js requires WebGL2 and WebAssembly, which are widely supported.","title":"Browser compatibility"},{"location":"getting-started/#about","text":"encantar.js is developed by Alexandre Martins and released under the LGPL . It is based on Speedy Vision .","title":"About"},{"location":"getting-started/activate-your-webcam/","text":"Activate your webcam In this section we're going to learn how to use your webcam to capture the video. We're also going to polish our work and make it presentable to users. Change the source of data Instead of using a video file, we're going to use your webcam. We simply need to change the source of data and instruct encantar.js to use your webcam. We'll do it with one new line of code! ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); //const video = document.getElementById('my-video'); // comment this line //const source = AR.Source.Video(video); // comment this line const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Let's also comment (or remove) the <video> tag from the HTML file - we no longer need it: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > Open http://localhost:8000 and... ta-da! The web browser will ask for your permission to access the camera. Have fun. Before using a webcam Pay attention to the following: Low-quality cameras should be avoided. A camera of a typical smartphone is probably good enough. Don't move the camera / the target image too quickly, as quick movements produce motion blur. Ensure good lighting conditions (see below). Check your physical scene Good lighting conditions are important for a good user experience. Even though the encantar.js can handle various lighting conditions, you should get your physical scene appropriately illuminated. When developing your own WebAR experiences, ask yourself: Will my users experience AR indoors? If so, make sure that the room is sufficiently illuminated. Will my users experience AR outdoors? In this case, make sure that users interact with your AR experience during the day, or have that interaction happen in a place with sufficient artificial lighting. When printing your reference images, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. If you're using a screen to display the reference image, make sure to adjust the brightness. Too much brightness causes overexposure and loss of detail, leading to tracking difficulties. Not enough brightness is also undesirable, because it makes the reference image look too dark in the video. Screen reflections are also undesirable. Use HTTPS When distributing your WebAR experiences over the internet, make sure to use HTTPS. Web browsers will only allow access to the webcam in secure contexts. Here is the reference image in case you need it again: Reference Image Create a scan gimmick Let's polish our work. When the tracker is scanning the physical scene, we'll display a visual cue suggesting the user to frame the target image. I'll call that a scan gimmick. Save the image below as scan.png : Scan gimmick In order to display that scan gimmick, we need to create a HUD ( Heads-Up Display ). A HUD is an overlay used to display 2D content in front of the augmented scene. It's part of the viewport. Modify index.html and ar-demo.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; } # scan { width : 100 % ; height : 100 % ; object-fit : contain ; opacity : 0.75 ; } </ style > </ head > < body > < div id = \"ar-viewport\" > < div id = \"ar-hud\" hidden > < img id = \"scan\" src = \"scan.png\" draggable = \"false\" > </ div > </ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = AR.Source.Video(video); const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Open http://localhost:8000 . Now you can see the scan gimmick being displayed... all the time?! Configure the scan gimmick The scan gimmick should only be displayed when the tracker is scanning the physical scene. We should hide it as soon as a target image is recognized. If the tracking is lost, then we need to display it again because we're back in scanning mode. A simple way to know whether or not we're tracking a target image is to use events. We're going to add two event listeners to our tracker. If a targetfound event happens, we hide the scan gimmick. If a targetlost event happens, we show the scan gimmick again. ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = AR.Source.Video(video); const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; }); return session ; } Hide the gizmos Let's polish our work even more by hiding the gizmos. You may just set gizmos to false in AR.startSession() and there will be no more gizmos. Do the same to hide the stats panel. Let me show you a different approach. Instead of getting rid of the gizmos completely, we're going to hide them partially. They will be displayed when the tracker is scanning the physical scene, but not when the physical scene is being augmented. That's easy to do with the event listeners we have just set up: ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = AR.Source.Video(video); const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; session . gizmos . visible = false ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; session . gizmos . visible = true ; }); return session ; } Open http://localhost:8000 again. Now we're ready to create the augmented scene!","title":"Activate your webcam"},{"location":"getting-started/activate-your-webcam/#activate-your-webcam","text":"In this section we're going to learn how to use your webcam to capture the video. We're also going to polish our work and make it presentable to users.","title":"Activate your webcam"},{"location":"getting-started/activate-your-webcam/#change-the-source-of-data","text":"Instead of using a video file, we're going to use your webcam. We simply need to change the source of data and instruct encantar.js to use your webcam. We'll do it with one new line of code! ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); //const video = document.getElementById('my-video'); // comment this line //const source = AR.Source.Video(video); // comment this line const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Let's also comment (or remove) the <video> tag from the HTML file - we no longer need it: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > Open http://localhost:8000 and... ta-da! The web browser will ask for your permission to access the camera. Have fun. Before using a webcam Pay attention to the following: Low-quality cameras should be avoided. A camera of a typical smartphone is probably good enough. Don't move the camera / the target image too quickly, as quick movements produce motion blur. Ensure good lighting conditions (see below). Check your physical scene Good lighting conditions are important for a good user experience. Even though the encantar.js can handle various lighting conditions, you should get your physical scene appropriately illuminated. When developing your own WebAR experiences, ask yourself: Will my users experience AR indoors? If so, make sure that the room is sufficiently illuminated. Will my users experience AR outdoors? In this case, make sure that users interact with your AR experience during the day, or have that interaction happen in a place with sufficient artificial lighting. When printing your reference images, avoid shiny materials (e.g., glossy paper). They may generate artifacts in the image and interfere with the tracking. Prefer non-reflective materials. If you're using a screen to display the reference image, make sure to adjust the brightness. Too much brightness causes overexposure and loss of detail, leading to tracking difficulties. Not enough brightness is also undesirable, because it makes the reference image look too dark in the video. Screen reflections are also undesirable. Use HTTPS When distributing your WebAR experiences over the internet, make sure to use HTTPS. Web browsers will only allow access to the webcam in secure contexts. Here is the reference image in case you need it again: Reference Image","title":"Change the source of data"},{"location":"getting-started/activate-your-webcam/#create-a-scan-gimmick","text":"Let's polish our work. When the tracker is scanning the physical scene, we'll display a visual cue suggesting the user to frame the target image. I'll call that a scan gimmick. Save the image below as scan.png : Scan gimmick In order to display that scan gimmick, we need to create a HUD ( Heads-Up Display ). A HUD is an overlay used to display 2D content in front of the augmented scene. It's part of the viewport. Modify index.html and ar-demo.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; } # scan { width : 100 % ; height : 100 % ; object-fit : contain ; opacity : 0.75 ; } </ style > </ head > < body > < div id = \"ar-viewport\" > < div id = \"ar-hud\" hidden > < img id = \"scan\" src = \"scan.png\" draggable = \"false\" > </ div > </ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > <!-- <video id=\"my-video\" hidden muted loop playsinline autoplay> <source src=\"my-video.webm\" type=\"video/webm\" /> <source src=\"my-video.mp4\" type=\"video/mp4\" /> </video> --> </ body > </ html > ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = AR.Source.Video(video); const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Open http://localhost:8000 . Now you can see the scan gimmick being displayed... all the time?!","title":"Create a scan gimmick"},{"location":"getting-started/activate-your-webcam/#configure-the-scan-gimmick","text":"The scan gimmick should only be displayed when the tracker is scanning the physical scene. We should hide it as soon as a target image is recognized. If the tracking is lost, then we need to display it again because we're back in scanning mode. A simple way to know whether or not we're tracking a target image is to use events. We're going to add two event listeners to our tracker. If a targetfound event happens, we hide the scan gimmick. If a targetlost event happens, we show the scan gimmick again. ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = AR.Source.Video(video); const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; }); return session ; }","title":"Configure the scan gimmick"},{"location":"getting-started/activate-your-webcam/#hide-the-gizmos","text":"Let's polish our work even more by hiding the gizmos. You may just set gizmos to false in AR.startSession() and there will be no more gizmos. Do the same to hide the stats panel. Let me show you a different approach. Instead of getting rid of the gizmos completely, we're going to hide them partially. They will be displayed when the tracker is scanning the physical scene, but not when the physical scene is being augmented. That's easy to do with the event listeners we have just set up: ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ), hudContainer : document . getElementById ( 'ar-hud' ) }); //const video = document.getElementById('my-video'); //const source = AR.Source.Video(video); const source = AR . Source . Camera (); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); const scan = document . getElementById ( 'scan' ); tracker . addEventListener ( 'targetfound' , event => { scan . hidden = true ; session . gizmos . visible = false ; }); tracker . addEventListener ( 'targetlost' , event => { scan . hidden = false ; session . gizmos . visible = true ; }); return session ; } Open http://localhost:8000 again. Now we're ready to create the augmented scene!","title":"Hide the gizmos"},{"location":"getting-started/augment-the-scene/","text":"Augment the scene We're already tracking an image of the physical world. The next step is to augment it with computer graphics. You'll use a different technology to render the graphics. Pick a 3D rendering technology encantar.js is not a 3D rendering technology. It is an Augmented Reality technology that provides the data you need in order to augment your physical scenes. There are free and open-source 3D rendering technologies that you can use with encantar.js. Popular solutions include: A-Frame , Babylon.js and Three.js . You can also use other solutions. encantar.js lets you pick any web-based 3D rendering technology. Once you pick a 3D rendering technology, you need to integrate it with encantar.js. There is a code that is responsible for that integration. I call it a plugin . Among other things, a plugin transports the tracking results from encantar.js to the 3D rendering technology of your choice. Use a plugin Writing a plugin is a task of moderate complexity. It requires dealing with maths and with some idiosyncrasies of the 3D rendering technologies in order to make sure that it all works as intended. I provide easy-to-use plugins that work with different 3D rendering technologies, so that you don't need to deal with that complexity yourself. Plugins are shipped as JavaScript (.js) files. You just need to add a plugin to your web page, and then the integration will be done for you! Get the plugins in the demos Create the virtual scene You will create the virtual scene using the 3D rendering technology of your choice. As soon as you combine it with an appropriate plugin, the physical scene will be automagically augmented with the virtual scene, thus creating the augmented scene. Let me tell you more about the 3D rendering technologies I just mentioned. A-Frame A-Frame is an open-source framework used to build virtual reality (VR) experiences for the web. When you combine it with encantar.js, you become able to use it to create AR experiences too - without the need of special hardware or software. A-Frame is built on top of Three.js and extends it in powerful ways. It introduces a HTML-based declarative approach for scene graphs , empowering them with the Entity-Component-System , a software pattern commonly used in game development. A-Frame is easy for beginners and pleasing for experts. In many cases, writing JavaScript code is not needed. Babylon.js Babylon.js is a powerful open-source game and 3D rendering engine for the web. It includes pretty much all features you commonly find in 3D rendering engines (scene graphs, lights, materials, meshes, animations, etc.), plus systems that are specific to game engines (audio engine, collision system, physics system, support for sprites, etc.), plus all kinds of sophisticated features for various applications. Babylon.js has an amazing documentation with plenty of learning resources, as well as a helpful and friendly community! Even though it can be used by beginners, it's recommended to have experience with JavaScript before creating projects with it. Three.js Three.js is a popular open-source JavaScript library used to render 3D graphics in web browsers. It has many features, including: scene graphs, cameras, animations, lights, materials, loading of 3D models, mathematical utilities, special effects, and more. It has an active and vibrant community. Many community-made extensions are available. Using Three.js requires more JavaScript experience than using A-Frame in most cases, but it's also a great choice if you're comfortable with coding. Compared to A-Frame, Three.js offers you additional freedom on how you can organize your code, because it's a library, not a framework.","title":"Augment the scene"},{"location":"getting-started/augment-the-scene/#augment-the-scene","text":"We're already tracking an image of the physical world. The next step is to augment it with computer graphics. You'll use a different technology to render the graphics.","title":"Augment the scene"},{"location":"getting-started/augment-the-scene/#pick-a-3d-rendering-technology","text":"encantar.js is not a 3D rendering technology. It is an Augmented Reality technology that provides the data you need in order to augment your physical scenes. There are free and open-source 3D rendering technologies that you can use with encantar.js. Popular solutions include: A-Frame , Babylon.js and Three.js . You can also use other solutions. encantar.js lets you pick any web-based 3D rendering technology. Once you pick a 3D rendering technology, you need to integrate it with encantar.js. There is a code that is responsible for that integration. I call it a plugin . Among other things, a plugin transports the tracking results from encantar.js to the 3D rendering technology of your choice.","title":"Pick a 3D rendering technology"},{"location":"getting-started/augment-the-scene/#use-a-plugin","text":"Writing a plugin is a task of moderate complexity. It requires dealing with maths and with some idiosyncrasies of the 3D rendering technologies in order to make sure that it all works as intended. I provide easy-to-use plugins that work with different 3D rendering technologies, so that you don't need to deal with that complexity yourself. Plugins are shipped as JavaScript (.js) files. You just need to add a plugin to your web page, and then the integration will be done for you! Get the plugins in the demos","title":"Use a plugin"},{"location":"getting-started/augment-the-scene/#create-the-virtual-scene","text":"You will create the virtual scene using the 3D rendering technology of your choice. As soon as you combine it with an appropriate plugin, the physical scene will be automagically augmented with the virtual scene, thus creating the augmented scene. Let me tell you more about the 3D rendering technologies I just mentioned.","title":"Create the virtual scene"},{"location":"getting-started/augment-the-scene/#a-frame","text":"A-Frame is an open-source framework used to build virtual reality (VR) experiences for the web. When you combine it with encantar.js, you become able to use it to create AR experiences too - without the need of special hardware or software. A-Frame is built on top of Three.js and extends it in powerful ways. It introduces a HTML-based declarative approach for scene graphs , empowering them with the Entity-Component-System , a software pattern commonly used in game development. A-Frame is easy for beginners and pleasing for experts. In many cases, writing JavaScript code is not needed.","title":"A-Frame"},{"location":"getting-started/augment-the-scene/#babylonjs","text":"Babylon.js is a powerful open-source game and 3D rendering engine for the web. It includes pretty much all features you commonly find in 3D rendering engines (scene graphs, lights, materials, meshes, animations, etc.), plus systems that are specific to game engines (audio engine, collision system, physics system, support for sprites, etc.), plus all kinds of sophisticated features for various applications. Babylon.js has an amazing documentation with plenty of learning resources, as well as a helpful and friendly community! Even though it can be used by beginners, it's recommended to have experience with JavaScript before creating projects with it.","title":"Babylon.js"},{"location":"getting-started/augment-the-scene/#threejs","text":"Three.js is a popular open-source JavaScript library used to render 3D graphics in web browsers. It has many features, including: scene graphs, cameras, animations, lights, materials, loading of 3D models, mathematical utilities, special effects, and more. It has an active and vibrant community. Many community-made extensions are available. Using Three.js requires more JavaScript experience than using A-Frame in most cases, but it's also a great choice if you're comfortable with coding. Compared to A-Frame, Three.js offers you additional freedom on how you can organize your code, because it's a library, not a framework.","title":"Three.js"},{"location":"getting-started/concepts/","text":"Concepts Before diving into AR with you, I need to introduce a few concepts. Please take the time to read them all. Feel free to come back to this page at any time. What is Augmented Reality? Let me clarify what I mean by terms such as Augmented Reality and WebAR: Augmented Reality is the augmentation of physical reality with virtual elements. We typically augment physical reality with imagery generated by computer graphics. In this context, the word augment means: to blend the physical and the virtual imagery in a visually correlated manner. 1.1. AR is an abbreviation of Augmented Reality. An Augmented Reality experience , or Augmented Reality program, is a computer program designed to let users experience Augmented Reality 1 . Augmented Reality experiences come in different shapes. Some are designed for smartphones and tablets, others for special headsets, and so on. WebAR is a set of technologies used to create Augmented Reality experiences that run in web browsers. WebAR makes it easy for users to experience AR, because they can have immediate access to the AR experiences. All they have to do is open a web page. They are not tied to specific platforms and they also don't need to download apps. A WebAR experience is an Augmented Reality experience developed using WebAR technology. encantar.js is a WebAR technology. I also call it a WebAR engine. Lots of computations have to be performed behind the scenes in order to make an Augmented Reality experience possible. encantar.js uses the GPU 2 to accelerate many of those computations. In fact, the GPU and the CPU 3 are used together. This approach improves the performance of the WebAR experience and ultimately leads to a better user experience. You can use encantar.js to create amazing WebAR experiences! Basic concepts Let me explain some concepts that you'll see over and over again when developing WebAR experiences with encantar.js: An Augmented Reality experience augments a physical scene with a virtual scene. 1.1. The physical scene is a scene of the physical world. 1.2. The virtual scene is a scene generated by computer graphics. 1.3. The augmented scene is the physical scene augmented with the virtual scene. A session is a central component of a WebAR experience. It handles the main loop . The main loop performs two central tasks: it analyzes the input data and then passes the result of that analysis to the user callback. 2.1. The user callback is a function that updates and renders the virtual scene. It is repeated in an animation loop , which is part of the main loop. 2.2. The main loop is repeated until the session ends. The session ends when the user closes the web page, or by deliberate command. A session has one or more sources of data linked to it. A typical source of data is a video stream. Such a stream usually comes from a webcam or from a video file. 3.1. A source of data produces input data . A tracker is a subsystem of the WebAR engine that analyzes input data in some way. Trackers are meant to be attached to a session. Example: an image tracker is a type of tracker. If we attach an image tracker to a session, then we will be able to track images in that session. The user callback receives a frame . A frame is an object that holds data for rendering the virtual scene in a way that is consistent with the input data at a particular moment in time. Simply put, frames help us augment the physical scene with the virtual scene. 5.1. The data held by a frame is computed by the trackers that are attached to the session. A session is linked to a viewport . The viewport is the area in which we'll display the augmented scene. It's represented by a container defined by a suitable HTML element, typically a <div> . A session has a mode . The mode can be either immersive or inline. 7.1. In immersive mode, the augmented scene is displayed in such a way that it occupies the entire area of the screen in which the web page is shown. Think of it as a kind of fullscreen. The immersive mode is what is typically wanted. 7.2. In inline mode, the augmented scene is displayed in a way that is consistent with the typical flow of a web page. We can display the augmented scene in a web page such as this one - in the middle of text, links and other elements. Now let's create our first WebAR experience! It gets to be fun, I promise! An experience of AR is an event in consciousness in which AR is experienced. I sometimes use the latter definition. \u21a9 Graphics Processing Unit \u21a9 Central Processing Unit \u21a9","title":"Concepts"},{"location":"getting-started/concepts/#concepts","text":"Before diving into AR with you, I need to introduce a few concepts. Please take the time to read them all. Feel free to come back to this page at any time.","title":"Concepts"},{"location":"getting-started/concepts/#what-is-augmented-reality","text":"Let me clarify what I mean by terms such as Augmented Reality and WebAR: Augmented Reality is the augmentation of physical reality with virtual elements. We typically augment physical reality with imagery generated by computer graphics. In this context, the word augment means: to blend the physical and the virtual imagery in a visually correlated manner. 1.1. AR is an abbreviation of Augmented Reality. An Augmented Reality experience , or Augmented Reality program, is a computer program designed to let users experience Augmented Reality 1 . Augmented Reality experiences come in different shapes. Some are designed for smartphones and tablets, others for special headsets, and so on. WebAR is a set of technologies used to create Augmented Reality experiences that run in web browsers. WebAR makes it easy for users to experience AR, because they can have immediate access to the AR experiences. All they have to do is open a web page. They are not tied to specific platforms and they also don't need to download apps. A WebAR experience is an Augmented Reality experience developed using WebAR technology. encantar.js is a WebAR technology. I also call it a WebAR engine. Lots of computations have to be performed behind the scenes in order to make an Augmented Reality experience possible. encantar.js uses the GPU 2 to accelerate many of those computations. In fact, the GPU and the CPU 3 are used together. This approach improves the performance of the WebAR experience and ultimately leads to a better user experience. You can use encantar.js to create amazing WebAR experiences!","title":"What is Augmented Reality?"},{"location":"getting-started/concepts/#basic-concepts","text":"Let me explain some concepts that you'll see over and over again when developing WebAR experiences with encantar.js: An Augmented Reality experience augments a physical scene with a virtual scene. 1.1. The physical scene is a scene of the physical world. 1.2. The virtual scene is a scene generated by computer graphics. 1.3. The augmented scene is the physical scene augmented with the virtual scene. A session is a central component of a WebAR experience. It handles the main loop . The main loop performs two central tasks: it analyzes the input data and then passes the result of that analysis to the user callback. 2.1. The user callback is a function that updates and renders the virtual scene. It is repeated in an animation loop , which is part of the main loop. 2.2. The main loop is repeated until the session ends. The session ends when the user closes the web page, or by deliberate command. A session has one or more sources of data linked to it. A typical source of data is a video stream. Such a stream usually comes from a webcam or from a video file. 3.1. A source of data produces input data . A tracker is a subsystem of the WebAR engine that analyzes input data in some way. Trackers are meant to be attached to a session. Example: an image tracker is a type of tracker. If we attach an image tracker to a session, then we will be able to track images in that session. The user callback receives a frame . A frame is an object that holds data for rendering the virtual scene in a way that is consistent with the input data at a particular moment in time. Simply put, frames help us augment the physical scene with the virtual scene. 5.1. The data held by a frame is computed by the trackers that are attached to the session. A session is linked to a viewport . The viewport is the area in which we'll display the augmented scene. It's represented by a container defined by a suitable HTML element, typically a <div> . A session has a mode . The mode can be either immersive or inline. 7.1. In immersive mode, the augmented scene is displayed in such a way that it occupies the entire area of the screen in which the web page is shown. Think of it as a kind of fullscreen. The immersive mode is what is typically wanted. 7.2. In inline mode, the augmented scene is displayed in a way that is consistent with the typical flow of a web page. We can display the augmented scene in a web page such as this one - in the middle of text, links and other elements. Now let's create our first WebAR experience! It gets to be fun, I promise! An experience of AR is an event in consciousness in which AR is experienced. I sometimes use the latter definition. \u21a9 Graphics Processing Unit \u21a9 Central Processing Unit \u21a9","title":"Basic concepts"},{"location":"getting-started/guidelines-for-images/","text":"Guidelines for Reference Images Some images are more suitable for tracking than others. For best results, pick images that are distinct, asymmetric and detailed. Let me show you some examples. Distinct A distinct image has distinguishable areas - quite unlike a repetitive pattern! Distinct Not distinct Asymmetric Asymmetric images help the engine determine their orientation. When evaluating symmetry, you must not take colors into account. Asymmetric Symmetric Detailed A detailed image has lots of details with sufficient contrast. There's not much blank space! Detailed Not detailed Other considerations Aspect ratio Prefer images whose aspect ratio (the ratio width \u00f7 height ) is somewhere between the aspect ratio of the target device (16:9 is a common aspect ratio) and 1:1 (a square). It's okay to use landscape or portrait mode - the engine will make the necessary adjustments. Resolution Using a Ultra HD image is of no benefit, because the engine will downscale it. A tiny image isn't desirable either, because some details may be lost and the engine will likely have to upscale it. Use an image that has its details preserved. It's even better if that image can be loaded quickly! Physical materials When printing your images, keep the following in mind: Prefer non-reflective materials. Avoid shiny materials such as glossy paper. Reflections may generate artifacts in the video and interfere with the tracking. Materials should be rigid. Don't use something that can be distorted too easily. Use quality materials. Brightness on screens If you're using a screen to display your images, make sure to adjust its brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable. Test it! In addition to the guidelines presented above, you should always experiment with your images and make sure it all works as intended. Keep in mind that proper lighting of the physical environment is also very important!","title":"Guidelines for Images"},{"location":"getting-started/guidelines-for-images/#guidelines-for-reference-images","text":"Some images are more suitable for tracking than others. For best results, pick images that are distinct, asymmetric and detailed. Let me show you some examples.","title":"Guidelines for Reference Images"},{"location":"getting-started/guidelines-for-images/#distinct","text":"A distinct image has distinguishable areas - quite unlike a repetitive pattern! Distinct Not distinct","title":"Distinct"},{"location":"getting-started/guidelines-for-images/#asymmetric","text":"Asymmetric images help the engine determine their orientation. When evaluating symmetry, you must not take colors into account. Asymmetric Symmetric","title":"Asymmetric"},{"location":"getting-started/guidelines-for-images/#detailed","text":"A detailed image has lots of details with sufficient contrast. There's not much blank space! Detailed Not detailed","title":"Detailed"},{"location":"getting-started/guidelines-for-images/#other-considerations","text":"","title":"Other considerations"},{"location":"getting-started/guidelines-for-images/#aspect-ratio","text":"Prefer images whose aspect ratio (the ratio width \u00f7 height ) is somewhere between the aspect ratio of the target device (16:9 is a common aspect ratio) and 1:1 (a square). It's okay to use landscape or portrait mode - the engine will make the necessary adjustments.","title":"Aspect ratio"},{"location":"getting-started/guidelines-for-images/#resolution","text":"Using a Ultra HD image is of no benefit, because the engine will downscale it. A tiny image isn't desirable either, because some details may be lost and the engine will likely have to upscale it. Use an image that has its details preserved. It's even better if that image can be loaded quickly!","title":"Resolution"},{"location":"getting-started/guidelines-for-images/#physical-materials","text":"When printing your images, keep the following in mind: Prefer non-reflective materials. Avoid shiny materials such as glossy paper. Reflections may generate artifacts in the video and interfere with the tracking. Materials should be rigid. Don't use something that can be distorted too easily. Use quality materials.","title":"Physical materials"},{"location":"getting-started/guidelines-for-images/#brightness-on-screens","text":"If you're using a screen to display your images, make sure to adjust its brightness. If the screen is too bright (too dark), it will cause overexposure (underexposure) in the video and tracking difficulties - details of the images will be lost. Screen reflections are also undesirable.","title":"Brightness on screens"},{"location":"getting-started/guidelines-for-images/#test-it","text":"In addition to the guidelines presented above, you should always experiment with your images and make sure it all works as intended. Keep in mind that proper lighting of the physical environment is also very important!","title":"Test it!"},{"location":"getting-started/introduction/","text":"Introduction Augmented Reality (AR) has applications in many fields, including: games, marketing, commerce, education, arts, tourism, sports, healthcare, and so on. AR brings many exciting possibilities for creative projects - and you too can get into it! Traditionally, users were required to download (sometimes large) apps to experience AR. That was an obstacle for adoption. What if we dropped the need for apps and just required a web browser instead? Users already have web browsers! That's where WebAR comes in. Image Tracking What to expect from this guide We will create together an Augmented Reality experience that runs in web browsers. You will learn how to track an image in real-time. This is a common use case of Augmented Reality technology. No matter if you are a beginner, an expert, or somewhere in-between, set yourself at ease: this step-by-step guide can be followed by you. Get started In a hurry? This guide is an in-depth introduction to web-based Augmented Reality with encantar.js. If you're in a hurry, skip straight to the Demos section.","title":"Introduction"},{"location":"getting-started/introduction/#introduction","text":"Augmented Reality (AR) has applications in many fields, including: games, marketing, commerce, education, arts, tourism, sports, healthcare, and so on. AR brings many exciting possibilities for creative projects - and you too can get into it! Traditionally, users were required to download (sometimes large) apps to experience AR. That was an obstacle for adoption. What if we dropped the need for apps and just required a web browser instead? Users already have web browsers! That's where WebAR comes in. Image Tracking","title":"Introduction"},{"location":"getting-started/introduction/#what-to-expect-from-this-guide","text":"We will create together an Augmented Reality experience that runs in web browsers. You will learn how to track an image in real-time. This is a common use case of Augmented Reality technology. No matter if you are a beginner, an expert, or somewhere in-between, set yourself at ease: this step-by-step guide can be followed by you. Get started In a hurry? This guide is an in-depth introduction to web-based Augmented Reality with encantar.js. If you're in a hurry, skip straight to the Demos section.","title":"What to expect from this guide"},{"location":"getting-started/next-steps/","text":"Next steps Congratulations! You have created your first WebAR experience with encantar.js! Let me tell you some of the steps you can take from now on. Change the power preference Image tracking is no trivial task: lots of computations are being performed behind the scenes. The WebAR engine prioritizes processing performance over power consumption by default. You may reduce power consumption by reducing processing performance. This is simple to do: just set AR.Settings.powerPreference to \"low-power\" . ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } AR . Settings . powerPreference = 'low-power' ; // OPTIONAL const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); // ... } When you enable low-power mode, the WebAR engine will target a framerate of 30. In many cases, this is still acceptable for a good user experience. I suggest you test both ways! I emphasize that you are not required to enable low-power mode. Enable it if power consumption is an issue for you. If it isn't, you may also experiment with the \"high-performance\" mode. When should I use low-power mode? If you're targeting mobile devices, test your WebAR experiences with low-power mode. If you decide that the lower framerate is still acceptable, keep the low-power mode in order to save battery life. Add multiple virtual scenes You can add multiple reference images to the reference image database. Each of those images can correspond to a different virtual scene. The virtual scene that shows up depends on the target image that is being tracked. Explore the API to see how you can have multiple virtual scenes in a single web page. Don't go overboard with this, though: the web page should load fast. Too much content may impact loading times. Keep your media files small and load your content asynchronously if possible. Publish your WebAR experiences So far we've just created a static HTML page. The next step is to make your page available on the web. Your pages must be served over HTTPS - that's important for webcam access! Tip: use a QR code If you intend to print your reference images, consider adding a QR code nearby. The QR code should point to your web page. Users can then just scan your QR code to open your WebAR experience. Easy! Use the minified code When deploying your WebAR experiences, make sure to include the minified encantar.min.js file instead of the regular encantar.js . The latter is suitable for development. The former, for production. Support my work If you came this far in the guide, WebAR probably excites you. It is definitely something you want. I know, it is awesome! The possibilities are endless. Even better than getting your creative juices boiling with enthusiasm is the feeling of joy I have for sharing this work with you. I develop encantar.js independently. Creating this WebAR engine required a lot of time, effort, skill and specialized knowledge. Please support my work today, so that I can make it even more awesome! Support my work","title":"Next steps"},{"location":"getting-started/next-steps/#next-steps","text":"Congratulations! You have created your first WebAR experience with encantar.js! Let me tell you some of the steps you can take from now on.","title":"Next steps"},{"location":"getting-started/next-steps/#change-the-power-preference","text":"Image tracking is no trivial task: lots of computations are being performed behind the scenes. The WebAR engine prioritizes processing performance over power consumption by default. You may reduce power consumption by reducing processing performance. This is simple to do: just set AR.Settings.powerPreference to \"low-power\" . ar-demo.js async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } AR . Settings . powerPreference = 'low-power' ; // OPTIONAL const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); // ... } When you enable low-power mode, the WebAR engine will target a framerate of 30. In many cases, this is still acceptable for a good user experience. I suggest you test both ways! I emphasize that you are not required to enable low-power mode. Enable it if power consumption is an issue for you. If it isn't, you may also experiment with the \"high-performance\" mode. When should I use low-power mode? If you're targeting mobile devices, test your WebAR experiences with low-power mode. If you decide that the lower framerate is still acceptable, keep the low-power mode in order to save battery life.","title":"Change the power preference"},{"location":"getting-started/next-steps/#add-multiple-virtual-scenes","text":"You can add multiple reference images to the reference image database. Each of those images can correspond to a different virtual scene. The virtual scene that shows up depends on the target image that is being tracked. Explore the API to see how you can have multiple virtual scenes in a single web page. Don't go overboard with this, though: the web page should load fast. Too much content may impact loading times. Keep your media files small and load your content asynchronously if possible.","title":"Add multiple virtual scenes"},{"location":"getting-started/next-steps/#publish-your-webar-experiences","text":"So far we've just created a static HTML page. The next step is to make your page available on the web. Your pages must be served over HTTPS - that's important for webcam access! Tip: use a QR code If you intend to print your reference images, consider adding a QR code nearby. The QR code should point to your web page. Users can then just scan your QR code to open your WebAR experience. Easy! Use the minified code When deploying your WebAR experiences, make sure to include the minified encantar.min.js file instead of the regular encantar.js . The latter is suitable for development. The former, for production.","title":"Publish your WebAR experiences"},{"location":"getting-started/next-steps/#support-my-work","text":"If you came this far in the guide, WebAR probably excites you. It is definitely something you want. I know, it is awesome! The possibilities are endless. Even better than getting your creative juices boiling with enthusiasm is the feeling of joy I have for sharing this work with you. I develop encantar.js independently. Creating this WebAR engine required a lot of time, effort, skill and specialized knowledge. Please support my work today, so that I can make it even more awesome! Support my work","title":"Support my work"},{"location":"getting-started/set-up-a-web-server/","text":"Set up a web server Let's prepare our local environment in order to create our first WebAR experience. Create a file structure Let's create a file structure for our AR experience: Create a new folder called ar-demo in your filesystem Download the latest release of encantar.js and extract dist/encantar.js to ar-demo/ Create a new empty file called index.html and store it in ar-demo/ You will have the following file structure: ar-demo/ \u251c\u2500\u2500 index.html \u2514\u2500\u2500 encantar.js Add boilerplate code Use the code editor of your choice to write the following content to index.html : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > </ body > </ html > Set up a local web server Let's set up a local web server in your machine for development purposes. I'll be showing you an easy-to-follow approach. Feel free to use a different approach if you're an experienced web developer. Graphical interface Command line This is an easy solution that works on Windows, Linux and macOS: Download and run Servez , a simple web server for local web development. In Folder to Serve , specify the ar-demo folder we have just created. Change the Port to 8000 . Click on Start to launch the local web server. Click on Launch Browser to open a web browser at http://localhost:8000 . Setting up Servez: make sure that Folder to Serve points to the correct path in your filesystem! If you're familiar with the command line, you can use programs such as python , node or php to launch a local web server. Navigate to the ar-demo directory and then run: Python 2 Python 3 Node.js PHP python -m SimpleHTTPServer 8000 python3 -m http.server 8000 npx http-server -p 8000 php -S localhost:8000 Next, open your web browser and go to http://localhost:8000 . You should see a blue screen in your web browser: If you see that blue screen, you're ready to proceed. If not, review your settings. Why port 8000? Port 8000 is commonly used in web development. Although you may use a different port, I suggest that you stick to this convention throughout this guide.","title":"Set up a web server"},{"location":"getting-started/set-up-a-web-server/#set-up-a-web-server","text":"Let's prepare our local environment in order to create our first WebAR experience.","title":"Set up a web server"},{"location":"getting-started/set-up-a-web-server/#create-a-file-structure","text":"Let's create a file structure for our AR experience: Create a new folder called ar-demo in your filesystem Download the latest release of encantar.js and extract dist/encantar.js to ar-demo/ Create a new empty file called index.html and store it in ar-demo/ You will have the following file structure: ar-demo/ \u251c\u2500\u2500 index.html \u2514\u2500\u2500 encantar.js","title":"Create a file structure"},{"location":"getting-started/set-up-a-web-server/#add-boilerplate-code","text":"Use the code editor of your choice to write the following content to index.html : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > </ body > </ html >","title":"Add boilerplate code"},{"location":"getting-started/set-up-a-web-server/#set-up-a-local-web-server","text":"Let's set up a local web server in your machine for development purposes. I'll be showing you an easy-to-follow approach. Feel free to use a different approach if you're an experienced web developer. Graphical interface Command line This is an easy solution that works on Windows, Linux and macOS: Download and run Servez , a simple web server for local web development. In Folder to Serve , specify the ar-demo folder we have just created. Change the Port to 8000 . Click on Start to launch the local web server. Click on Launch Browser to open a web browser at http://localhost:8000 . Setting up Servez: make sure that Folder to Serve points to the correct path in your filesystem! If you're familiar with the command line, you can use programs such as python , node or php to launch a local web server. Navigate to the ar-demo directory and then run: Python 2 Python 3 Node.js PHP python -m SimpleHTTPServer 8000 python3 -m http.server 8000 npx http-server -p 8000 php -S localhost:8000 Next, open your web browser and go to http://localhost:8000 . You should see a blue screen in your web browser: If you see that blue screen, you're ready to proceed. If not, review your settings. Why port 8000? Port 8000 is commonly used in web development. Although you may use a different port, I suggest that you stick to this convention throughout this guide.","title":"Set up a local web server"},{"location":"getting-started/set-up-the-session/","text":"Set up the session Now we're going to track our reference image for the first time! Create the viewport We begin by creating the viewport. Remember that the viewport is the area in which we'll display the augmented scene. Add the following to index.html and to ar-demo.js : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); } catch ( error ) { alert ( error . message ); } }; Create the source of data Let's set up our source of data. We get the HTMLVideoElement corresponding to the test video and then we use it to instantiate a video source of data. Write the following to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = AR . Source . Video ( video ); } catch ( error ) { alert ( error . message ); } }; Start the session The session is a central component of a WebAR experience. The AR namespace has a very special method called startSession . It receives a settings dictionary that lets us configure the new session in different ways. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = AR . Source . Video ( video ); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); } catch ( error ) { alert ( error . message ); } }; Most of the settings passed to startSession correspond directly to the concepts we saw earlier. We're starting a new session in immersive mode, with the tracker, source of data and viewport that we have just configured. Let me explain what stats and gizmos mean: When you set stats: true , you're asking the engine to display a stats panel that shows useful data such as the current framerate. This is useful when developing WebAR experiences, but you should disable it in production. The option gizmos: true enables the gizmos. Gizmos are visual artifacts that help you visualize the current state of the tracker. They too are useful in development. In production, you may disable them or enable them partially (more on that later). Open http://localhost:8000 . You should see the tracking in action. Even though there is no virtual scene yet, the gizmos will show you the image being tracked. Image tracking in action! The code I have just presented is, in essence, what you need to start a session. I'm going to move it to a new function called startARSession for convenience: ar-demo.js window . onload = async function () { try { const session = await startARSession (); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = AR . Source . Video ( video ); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Now all you have to do to start a new session is call startARSession() ! Start the animation loop The animation loop repeatedly calls the user callback. The user callback is a function responsible for updating and rendering the virtual scene. We have no virtual scene at the moment, but we'll already set up that function. Let's call session.requestAnimationFrame() and pass the user callback as an argument: ar-demo.js window . onload = async function () { try { const session = await startARSession (); function animate ( time , frame ) { session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate ); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { // ... } Calling session.requestAnimationFrame() inside the user callback, animate() in this example, makes it loop until the session ends. Making that call outside the user callback starts the loop. requestAnimationFrame Note that session.requestAnimationFrame() is different from window.requestAnimationFrame() . The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser.","title":"Set up the session"},{"location":"getting-started/set-up-the-session/#set-up-the-session","text":"Now we're going to track our reference image for the first time!","title":"Set up the session"},{"location":"getting-started/set-up-the-session/#create-the-viewport","text":"We begin by creating the viewport. Remember that the viewport is the area in which we'll display the augmented scene. Add the following to index.html and to ar-demo.js : index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < div id = \"ar-viewport\" ></ div > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); } catch ( error ) { alert ( error . message ); } };","title":"Create the viewport"},{"location":"getting-started/set-up-the-session/#create-the-source-of-data","text":"Let's set up our source of data. We get the HTMLVideoElement corresponding to the test video and then we use it to instantiate a video source of data. Write the following to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = AR . Source . Video ( video ); } catch ( error ) { alert ( error . message ); } };","title":"Create the source of data"},{"location":"getting-started/set-up-the-session/#start-the-session","text":"The session is a central component of a WebAR experience. The AR namespace has a very special method called startSession . It receives a settings dictionary that lets us configure the new session in different ways. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = AR . Source . Video ( video ); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); } catch ( error ) { alert ( error . message ); } }; Most of the settings passed to startSession correspond directly to the concepts we saw earlier. We're starting a new session in immersive mode, with the tracker, source of data and viewport that we have just configured. Let me explain what stats and gizmos mean: When you set stats: true , you're asking the engine to display a stats panel that shows useful data such as the current framerate. This is useful when developing WebAR experiences, but you should disable it in production. The option gizmos: true enables the gizmos. Gizmos are visual artifacts that help you visualize the current state of the tracker. They too are useful in development. In production, you may disable them or enable them partially (more on that later). Open http://localhost:8000 . You should see the tracking in action. Even though there is no virtual scene yet, the gizmos will show you the image being tracked. Image tracking in action! The code I have just presented is, in essence, what you need to start a session. I'm going to move it to a new function called startARSession for convenience: ar-demo.js window . onload = async function () { try { const session = await startARSession (); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); const viewport = AR . Viewport ({ container : document . getElementById ( 'ar-viewport' ) }); const video = document . getElementById ( 'my-video' ); const source = AR . Source . Video ( video ); const session = await AR . startSession ({ mode : 'immersive' , viewport : viewport , trackers : [ tracker ], sources : [ source ], stats : true , gizmos : true , }); return session ; } Now all you have to do to start a new session is call startARSession() !","title":"Start the session"},{"location":"getting-started/set-up-the-session/#start-the-animation-loop","text":"The animation loop repeatedly calls the user callback. The user callback is a function responsible for updating and rendering the virtual scene. We have no virtual scene at the moment, but we'll already set up that function. Let's call session.requestAnimationFrame() and pass the user callback as an argument: ar-demo.js window . onload = async function () { try { const session = await startARSession (); function animate ( time , frame ) { session . requestAnimationFrame ( animate ); } session . requestAnimationFrame ( animate ); } catch ( error ) { alert ( error . message ); } }; async function startARSession () { // ... } Calling session.requestAnimationFrame() inside the user callback, animate() in this example, makes it loop until the session ends. Making that call outside the user callback starts the loop. requestAnimationFrame Note that session.requestAnimationFrame() is different from window.requestAnimationFrame() . The former is a call to the WebAR engine, whereas the latter is a standard call to the web browser.","title":"Start the animation loop"},{"location":"getting-started/set-up-the-tracker/","text":"Set up the tracker In this section we'll learn how to set up the tracker. Later on we'll see how to use the tracker to track an image in a video, with its position and orientation in 3D. Add a reference image The first thing we need to do is add the image we want to track to our web page. We'll be calling that a reference image . We simply pick a suitable image and add an <img> tag to the page. Not all images are suitable for tracking. Images should be distinct, detailed and asymmetrical. I discuss this in detail in Guidelines for Images . For now we'll just use the following image: Reference Image Download the image to the ar-demo/ folder. Save it as my-reference-image.webp . Next, let's add the reference image to our web page. Add an <img> tag to the <body> of the page as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" > </ body > </ html > Reload the page. You should see the reference image: Reference image in a web page If you don't see the image, make sure that there are no errors in the filename. Once you see that the image is being properly loaded, there is no need to keep it visible. Let's add the hidden attribute to the <img> tag: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > </ body > </ html > Add a test video We're going to be tracking that reference image in a test video. Please save the following video as my-video.webm and my-video.mp4 in ar-demo/ . Later on I'll tell you how to use your webcam instead. This is the expected directory structure at this point: ar-demo/ \u251c\u2500\u2500 index.html \u251c\u2500\u2500 encantar.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm Let's include the test video in our page. Add a <video> tag as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > Instantiate an Image Tracker In order to track the reference image in our video, we need an Image Tracker. Remember that a tracker is a subsystem of the WebAR engine that analyzes input data in some way. An Image Tracker is a tracker that finds and tracks reference images in a video stream. Before we track anything with an Image Tracker, we must tell it what to track. There are two steps to this: first, we instantiate an Image Tracker. Next, we link our reference image to it. We'll be writing a little bit of JavaScript code now. In order to keep our code clean, we'll be writing the JavaScript code to a new file. Let's add a <script> tag below encantar.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > Create a new file called ar-demo.js and store it in the ar-demo/ folder. Write the following contents to it: ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); } catch ( error ) { alert ( error . message ); } }; The AR namespace holds the various elements featured by the engine. We'll be using it extensively. encantar.js only requires standard web technologies that have been around for a while. Still, it's a good practice to check if those technologies are supported by the target system. If they are not, we display a message and quit. If they are, we instantiate an Image Tracker. Before moving on, make sure that you have the following directory structure at this point: ar-demo/ \u251c\u2500\u2500 ar-demo.js \u251c\u2500\u2500 index.html \u251c\u2500\u2500 encantar.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm Link the image to the tracker Our Image Tracker has an internal database of reference images that it's capable of tracking. We call it a reference image database . To link a reference image to the tracker means to add that image to the database. When linking a reference image to the tracker, the appropriate HTMLImageElement must be provided to the database. You may optionally assign a name to the image, so that you can identify it later on. If you don't, an automatically generated name will be assigned for you. Let's link the image to the tracker. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); } catch ( error ) { alert ( error . message ); } }; Reload the page. If you see no errors popping up, it means that the image is linked to the tracker. You're ready to proceed!","title":"Set up the tracker"},{"location":"getting-started/set-up-the-tracker/#set-up-the-tracker","text":"In this section we'll learn how to set up the tracker. Later on we'll see how to use the tracker to track an image in a video, with its position and orientation in 3D.","title":"Set up the tracker"},{"location":"getting-started/set-up-the-tracker/#add-a-reference-image","text":"The first thing we need to do is add the image we want to track to our web page. We'll be calling that a reference image . We simply pick a suitable image and add an <img> tag to the page. Not all images are suitable for tracking. Images should be distinct, detailed and asymmetrical. I discuss this in detail in Guidelines for Images . For now we'll just use the following image: Reference Image Download the image to the ar-demo/ folder. Save it as my-reference-image.webp . Next, let's add the reference image to our web page. Add an <img> tag to the <body> of the page as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" > </ body > </ html > Reload the page. You should see the reference image: Reference image in a web page If you don't see the image, make sure that there are no errors in the filename. Once you see that the image is being properly loaded, there is no need to keep it visible. Let's add the hidden attribute to the <img> tag: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > </ body > </ html >","title":"Add a reference image"},{"location":"getting-started/set-up-the-tracker/#add-a-test-video","text":"We're going to be tracking that reference image in a test video. Please save the following video as my-video.webm and my-video.mp4 in ar-demo/ . Later on I'll tell you how to use your webcam instead. This is the expected directory structure at this point: ar-demo/ \u251c\u2500\u2500 index.html \u251c\u2500\u2500 encantar.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm Let's include the test video in our page. Add a <video> tag as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html >","title":"Add a test video"},{"location":"getting-started/set-up-the-tracker/#instantiate-an-image-tracker","text":"In order to track the reference image in our video, we need an Image Tracker. Remember that a tracker is a subsystem of the WebAR engine that analyzes input data in some way. An Image Tracker is a tracker that finds and tracks reference images in a video stream. Before we track anything with an Image Tracker, we must tell it what to track. There are two steps to this: first, we instantiate an Image Tracker. Next, we link our reference image to it. We'll be writing a little bit of JavaScript code now. In order to keep our code clean, we'll be writing the JavaScript code to a new file. Let's add a <script> tag below encantar.js as follows: index.html <!doctype html> < html > < head > < meta charset = \"utf-8\" > < meta name = \"viewport\" content = \"width=device-width,initial-scale=1\" > < title > encantar.js WebAR demo </ title > < script src = \"encantar.js\" ></ script > < script src = \"ar-demo.js\" ></ script > < style > body { background-color : #3d5afe ; }</ style > </ head > < body > < img id = \"my-reference-image\" src = \"my-reference-image.webp\" hidden > < video id = \"my-video\" hidden muted loop playsinline autoplay > < source src = \"my-video.webm\" type = \"video/webm\" /> < source src = \"my-video.mp4\" type = \"video/mp4\" /> </ video > </ body > </ html > Create a new file called ar-demo.js and store it in the ar-demo/ folder. Write the following contents to it: ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); } catch ( error ) { alert ( error . message ); } }; The AR namespace holds the various elements featured by the engine. We'll be using it extensively. encantar.js only requires standard web technologies that have been around for a while. Still, it's a good practice to check if those technologies are supported by the target system. If they are not, we display a message and quit. If they are, we instantiate an Image Tracker. Before moving on, make sure that you have the following directory structure at this point: ar-demo/ \u251c\u2500\u2500 ar-demo.js \u251c\u2500\u2500 index.html \u251c\u2500\u2500 encantar.js \u251c\u2500\u2500 my-reference-image.webp \u251c\u2500\u2500 my-video.mp4 \u2514\u2500\u2500 my-video.webm","title":"Instantiate an Image Tracker"},{"location":"getting-started/set-up-the-tracker/#link-the-image-to-the-tracker","text":"Our Image Tracker has an internal database of reference images that it's capable of tracking. We call it a reference image database . To link a reference image to the tracker means to add that image to the database. When linking a reference image to the tracker, the appropriate HTMLImageElement must be provided to the database. You may optionally assign a name to the image, so that you can identify it later on. If you don't, an automatically generated name will be assigned for you. Let's link the image to the tracker. Add the following code to ar-demo.js : ar-demo.js window . onload = async function () { try { if ( ! AR . isSupported ()) { throw new Error ( 'This device is not compatible with this AR experience.\\n\\n' + 'User agent: ' + navigator . userAgent ); } const tracker = AR . Tracker . ImageTracker (); await tracker . database . add ([{ name : 'my-reference-image' , image : document . getElementById ( 'my-reference-image' ) }]); } catch ( error ) { alert ( error . message ); } }; Reload the page. If you see no errors popping up, it means that the image is linked to the tracker. You're ready to proceed!","title":"Link the image to the tracker"}]}